\section{Continual Learning in Neural Networks}
\label{sec:continual_learning}

A common assumption in machine learning is that the data comes from stationary distributions such that the properties of the data are the same over time. However, this assumption may complicate deployments in real-world applications where variations in the data is common and the data seen during deployment can be very different from the data used for training. Such models has to be capable of learning from experiences online and update its knowledge about new concepts that are encountered during the lifespan. Furthermore, as the model is operating in an online manner, the model has to adapt fast when learning new knowledge to minimize the time that the system has to be down during training. These requirements has lead to research in Continual Learning (CL)~\cite{delange2021continual, parisi2019continual} where the goal is to push neural networks closer to how humans and animals are learning various tasks during their lifespan.

    

  