\section{Deep Learning} 
\label{sec:deep_learning}

Neural networks is a class of machine learning models which popularity have grown immensely due to their ability to learn from large and high-dimensional datasets. Moreover, neural networks have been successfully applied in various number of fields in computer vision~\cite{he2016deep, krizhevsky2012imagenet}, natural language processing~\cite{devlin2018bert}, and reinforcement learning~\cite{mnih2015human, silver2016mastering}. These models are constructed by stacking layers of parameters that extract representations of the input data until reaching the final layer that outputs the target answer from the queried input. For example, the operation for passing the input data $\vx$ through the first layer can be denoted as the matrix multiplication $\vh = \mW_1\vx$ where $\mW_1$ are the weights of the first layer. An essential part for enabling neural networks to learn more interesting non-linear functions is to add an activation function right after the matrix multiplication of each layer, otherwise the neural network would only be capable of learning linear functions. A common activation function is the $a(\vx) = \max(0, \vx)$, or the so called Rectified Linear Unit (ReLU) activation, which outputs $\vx$ when $\vx > 0$ or otherwise zero. Stacking two layers together in a neural network with such activation between then becomes $\vh = \mW_2\max(0,  \mW_1\vx)$. In classification problems, the output layer outputs the score for which class the input $\vx$ could belong to. The weight parameters $\mW_1, \mW_2$ are learned with SGD, and their gradients are derived using the chain rule and computed using backpropagation (see \cite{goodfellow2016deep} for an introduction to backpropagation).  

\MK{Feel like I would like to include some brief info on CNNs and RNNs, but just a paragraph long for each. Maybe also describe the Autoencoders a bit. }

\subsection{Variational Autoencoders}
\label{sec:variational_autoencoders}

Generative models are used for approximating a data distribution $p_{data}$ from a given set of samples $\gD$. In most cases, we parameterize the distribution with $\vtheta$ and learn the parameters from the given data by minimizing some distance metric between the estimated distribution $p_{\vtheta}$ and $p_{data}$. In deep generative models, the distribution $p_{\theta}$ is parameterized by a neural network where the parameters $\vtheta$ represent the weights and biases in the network. These models can be broadly divided into three classes of models, namely Variational Autoencoders (VAEs)~\cite{kingma2013auto}, Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}, and Normalizing Flows~\cite{rezende2015variational}. Their commonality is that they are based on latent variable models where it is assumed that the observed data is generated from some hidden process from a simpler distribution than $p_{data}$. However, which class of models to select depends on the application and the goals with the task. In this thesis, we focus on VAEs because of their capability of learning lower-dimensional representations.

A key ingredient in learning generative models is to introduce a latent variable $\vz$ where we assume that $\vz$ is related to the observed variable $\vx$ through the data generation process. We can still estimate the parameterized distribution when introducing the latent variables since
\begin{align}\label{eq:marginal_density}
	p_{\vtheta}(\vx) = \int p_{\vtheta}(\vx, \vz) \,d\vz = \int p_{\vtheta}(\vx | \vz) p(\vz) \,d\vz,
\end{align}
where we incorporate $\vz$ to obtain the joint distribution $p_{\vtheta}(\vx, \vz)$ and use the chain rule for probabilities to obtain the likelihood $p_{\vtheta}(\vx | \vz)$ and the prior distribution $p(\vz)$. The prior $p(\vz)$ is where we can define our assumption of how the underlying hidden reasons are distributed by selecting a simple and well-known distribution for this space, e.g., a Gaussian distribution. The goal with latent variable models is often to compute the posterior $p_{\vtheta}(\vz | \vx)$ over the latent variables given the data $\vx$ which can be done using Bayes' rule 
\begin{align}
	p_{\vtheta}(\vz | \vx) = \frac{p_{\vtheta}(\vx | \vz) p(\vz)}{p_{\vtheta}(\vx)}. 
\end{align}
Unfortunately, the evidence $p_{\vtheta}(\vx)$ is very hard to compute as it requires calculating the integral $\int p_{\vtheta}(\vx, \vz) \,d\vz$ over all dimensions of the latent variable space. To overcome this issue, we will propose a distribution $q_{\vz}$ that should approximate the true posterior $p_{\vtheta}(\vz | \vx)$. Variational Inference (VI)~\cite{blei2017variational, zhang2018advances} is a method for approximating probability densities through optimization which is used in VAEs for approximating the posterior. Next, we give a description of how the posterior is approximated.
%The approximation can either be done using Markov Chain Monte Carlo (MCMC)~\cite{andrieu2003introduction} or Variational Inference (VI)~\cite{blei2017variational, zhang2018advances}. In MCMC, the idea is to use an easy-to-sample proposal distribution to draw samples that are used approximating the target distribution, while VI is a method for approximating probabilty densities through optimization. 

The learning objective in VAEs is based on the derivation of the marginal density, or evidence, in Equation \ref{eq:marginal_density}. As the evidence is intractable to compute exactly, we will instead maximize a lower bound on the evidence that is called the Evidence Lower BOund (ELBO). The ELBO is derived in log-space such that we can apply Jensen's inequality to obtain the lower bound when we have introduced the approximate posterior $q_{\vphi}(\vz | \vx)$ parameterized by $\vphi$. We begin be deriving a general expression for the ELBO: 
\begin{align}\label{eq:elbo}
	\begin{split}
		\log p_{\vtheta}(\vx) & = \log \int p_{\vtheta}(\vx, \vz) \,d\vz \\ 
		& = \log \int \frac{q_{\vphi}(\vz | \vx) p_{\vtheta}(\vx , \vz)}{q_{\vphi}(\vz | \vx)} \,d\vz \\
		& = \log \E_{q_{\vphi}(\vz | \vx)}\left[ \frac{p_{\vtheta}(\vx , \vz)}{q_{\vphi}(\vz | \vx)}\right] \\
		& \geq \E_{q_{\vphi}(\vz | \vx)}\left[ \log \frac{p_{\vtheta}(\vx , \vz)}{q_{\vphi}(\vz | \vx)}\right] ,
	\end{split}
\end{align}
where we applied Jensen's inequality between the third and fourth line to obtain the ELBO. This expression can be derived further to obtain the common VAE objective
\begin{align}\label{eq:vae}
	\begin{split}
		\log p_{\vtheta}(\vx) & \geq \E_{q_{\vphi}(\vz | \vx)}\left[ \log \frac{p_{\vtheta}(\vx , \vz)}{q_{\vphi}(\vz | \vx)}\right] \\
		& = \E_{q_{\vphi}(\vz | \vx)}\left[ \log p_{\vtheta}(\vx | \vz) \right] - \KL[q_{\vphi}(\vz | \vx) \,||\, p(\vz)], 
	\end{split}
\end{align}
where the second term is the Kullback-Leibler (KL) divergence between the approximate posterior and the prior. The KL divergence can be computed exactly when selecting both $q_{\vphi}(\vz | \vx)$ and $p(\vz)$ to be Gaussian densities, while the expectation of the log-likelihood can be estimated with Monte Carlo approximation. Furthermore, the reparameterization trick~\cite{kingma2013auto, rezende2014stochastic} is used to enable computing the gradients through the sampling step when estimating the log-likelihood that can be used for backpropagation. 


\subsection{Multi-view Learning}


