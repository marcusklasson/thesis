\section{Deep Learning} 
\label{sec:deep_learning}

Neural networks is a class of machine learning models which popularity have grown immensely due to their ability to learn from large and high-dimensional datasets. Moreover, neural networks have been successfully applied in various number of fields in computer vision~\cite{he2016deep, krizhevsky2012imagenet}, natural language processing~\cite{devlin2018bert}, and reinforcement learning~\cite{mnih2015human, silver2016mastering}. These models are constructed by stacking layers of parameters that extract representations of the input data until reaching the final layer that outputs the target answer from the queried input. For example, the operation for passing the input data $\vx$ through the first layer can be denoted as the matrix multiplication $\vh = \mW_1\vx$ where $\mW_1$ are the weights of the first layer. An essential part for enabling neural networks to learn more interesting non-linear functions is to add an activation function right after the matrix multiplication of each layer, otherwise the neural network would only be capable of learning linear functions. A common activation function is the $a(\vx) = \max(0, \vx)$, or the so called Rectified Linear Unit (ReLU) activation, which outputs $\vx$ when $\vx > 0$ or otherwise zero. Stacking two layers together in a neural network with such activation between then becomes $\vh = \mW_2\max(0,  \mW_1\vx)$. In classification problems, the output layer outputs the score for which class the input $\vx$ could belong to. The weight parameters $\mW_1, \mW_2$ are learned with SGD, and their gradients are derived using the chain rule and computed using backpropagation (see \cite{goodfellow2016deep} for an introduction to backpropagation).  

\MK{Feel like I would like to include some brief info on CNNs and RNNs, but just a paragraph long for each. Maybe also describe the Autoencoders a bit. }

\subsection{Deep Generative Models}
\label{sec:deep_generative_models}



\subsection{Multi-view Learning}