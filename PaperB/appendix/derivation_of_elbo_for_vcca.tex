
\subsection{Derivation of the ELBO for VCCA}
\label{paperB:app:derivation_of_elbo_for_vcca}

Let $x_{1:M}$ denote the all observed data, i.e., $x_{1:M} = x_1, \dots, x_M$, for $M$ different views. We derive the ELBO for VCCA by introducing the approximate posterior $q_{\phi}(z | x_m)$, where $x_m$ is the only view that we use to infer the latent variable $z$. We now derive the ELBO from the marginal log-likelihood $ \log p_{{\theta}}(x_{1:M})$ as 
\begin{align*}
    \begin{split}
        \log p_{{\theta}}(x_{1:M}) = & \log p_{{\theta}}(x_{1:M}) \int q_{\phi}(z | x_m) \, dz \\
        = & \int q_{\phi}(z | x_m) \log p_{{\theta}}(x_{1:M}) \, dz \\
        = & \int q_{\phi}(z | x_m) \log \frac{p_{{\theta}}(x_{1:M}, z)}{p_{{\theta}}(z | x_{1:M})}  \, dz \\
        = & \int q_{\phi}(z | x_m) \log \frac{p_{{\theta}}(x_{1:M}, z)}{p_{{\theta}}(z | x_{1:M})} \frac{q_{\phi}(z | x_m)}{q_{\phi}(z | x_m)}  \, dz \\
        = & \int q_{\phi}(z | x_m) \left( \log \frac{q_{\phi}(z | x_m)}{p_{{\theta}}(z | x_{1:M})} + \log \frac{p_{{\theta}}(x_{1:M}, z)}{q_{\phi}(z | x_m)} \right) dz \\
        = & \, \KL(q_{\phi}(z | x_m) \,||\, p_{{\theta}}(z | x_{1:M})) + \E_{q_{\phi}(z | x_m)}\left[ \log \frac{p_{{\theta}}(x_{1:M}, z)}{q_{\phi}(z | x_m)} \right] \\
        \geq & \, \E_{q_{\phi}(z | x_m)}\left[ \log \frac{p_{{\theta}}(x_{1:M}, z)}{q_{\phi}(z | x_m)} \right] = \mathcal{L}(x_{1:M}; \theta, \phi)
    \end{split}
\end{align*}
We use the factorization property of the joint distribution $p_{\theta}(x_{1:M})$ to further derive the ELBO $\mathcal{L}(x_{1:M}; \theta, \phi)$:
\begin{align*}
    \begin{split}
        \mathcal{L}(\theta, \phi; x_{1:M}) = & \E_{q_{\phi}(z | x_m)}\left[ \log \frac{p_{{\theta}_1}(x_1 |  z) \cdots p_{{\theta}_M}(x_M |  z) p(z)}{q_{\phi}(z | x_m)} \right] \\
        = & \E_{q_{\phi}(z | x_m)}\left[ \log p_{{\theta}_1}(x_1 |  z) + ... + \log p_{{\theta}_M}(x_M |  z) + \log \frac{p(z)}{q_{\phi}(z | x_m)} \right] \\
        = & \E_{q_{\phi}(z | x_m)}\left[ \log p_{{\theta}_1}(x_1 |  z) \right] + ... + \E_{q_{\phi}(z | x_m)}\left[ \log p_{{\theta}_M}(x_M |  z) \right] \\ 
        & -\KL(q_{\phi}(z | x_m) \,||\, p(z))
    \end{split}
\end{align*}
It may be necessary to balance the terms in the ELBO with some constant for every term, especially when the dimensions and magnitudes differ between the modalities. Thus, we introduce the likelihood weights $\lambda_1, ..., \lambda_M$, such that the ELBO will be written as 
\begin{align*}
    \begin{split}
        \mathcal{L}(\theta, \phi; x_{1:M}) = & \lambda_1 \E_{q_{\phi}(z | x_m)}\left[ \log p_{{\theta}_1}(x_1 |  z) \right] + ... + \lambda_M \E_{q_{\phi}(z | x_m)}\left[ \log p_{{\theta}_M}(x_M |  z) \right] \\
        & - \KL(q_{\phi}(z | x_m) \,||\, p(z)) 
    \end{split}
\end{align*}
The likelihood weights $\lambda_1, ..., \lambda_M$ that are optimal for the task at hand can be found with some hyperparameter search.