
\subsection{Details on Experimental Setup}
\label{paperB:app:details_on_experimental_setup}

In this section, we provide the full details on the experimental setups.

\vspace{-3mm}
\paragraph{Training DenseNet169 from Scratch} We train a DenseNet169 on the dataset from scratch as a baseline. We train the network using stochastic gradient descent for 300 epochs and follow the learning rate schedule of Huang \etal~\citeB{B:huang2017densely}, i.e., using an initial learning rate of 0.1 and dividing it by 10 after 150 and 225 epochs. We use a weight decay of $10^{-4}$ and Nesterov momentum of 0.9 without dampening. We denote this network as DenseNet-scratch in the experiments.

\vspace{-3mm}
\paragraph{Training the Softmax Classifier} We use a Softmax classifier trained on off-the-shelf features as another baseline. We use a DenseNet169~\citeB{B:huang2017densely} pre-trained on ImageNet 1K as the feature extractor, where we extract 1664-dimensional from the average pooling layer before the classification layer in the architecture. The Softmax classifier is trained for 100 epochs with batch size 64 to minimize the cross-entropy loss. We use the Adam optimizer~\citeB{B:kingma2015adam} with initial learning rate $10^{-4}$ and hyperparameters $\beta_1 = 0.5$ and $\beta_2 = 0.999$. Note that we used no regularization when training the Softmax classifier. We denote this classifier as Softmax in the experiments. This training setup is also used when training the Softmax classifiers for SplitAE, VCCA, and VCCA-private. 

\vspace{-3mm}
\paragraph{Architectures for Single-View Autoencoders} We use a vanilla autoencoder and a VAE as baselines that learn latent representations of the natural image features only. These models are denoted as AE$_{x}$ and VAE$_{x}$ respectively in the experiments. Their latent representations have dimension $d_{z} = 200$ throughout all experiments. The encoder and decoder networks consist of one hidden layer of $512$ hidden units with Leaky ReLU activation. The decoder aims to reconstruct the natural image features and we use the sum-of-squares as the reconstruction loss during training. For the VAE, we use the mean outputs $\mu_{z}(x)$ from the encoder as the latent representations of the image features to train a Softmax classifier, which we denote as VAE$_{x}$+Softmax.

\vspace{-3mm}
\paragraph{Architectures for SplitAE and VCCA} We use the same encoder and decoder architecture for the natural image features as in the single-view autoencoders for all SplitAE, VCCA, and VCCA-private models, i.e., one hidden layer of $512$ hidden units with Leaky ReLU activation. We set the latent dimension to $d_{z} = 200$ in all experiments. The class label decoders use the same architecture as the image feature decoder and predict the class label by optimizing the cross-entropy loss. The iconic image decoders use the generator architecture of the DCGAN~\citeB{B:radford2015unsupervised}. This decoder reconstructs the iconic images $i \in \R^{64 \times 64 \times 3}$ by minimizing the sum of squares loss. The text descriptions are assumed to be a sequence of words $w = (w_1, \dots, w_T)$, where $T$ is the length of the description. We create a vocabulary $V \in \mathbb{R}^{658}$ of the total number of unique words from all text descriptions in the dataset. The text description decoder is an LSTM~\citeB{B:hochreiter1997long} followed by a linear layer that predicts the next word in the description. More specifically, at time step $t$, the decoder yields a multinomial probability distribution $p_{{\theta_{w}}}(w_t | w_{t-1}, z)$ defined over $w_t \in V$. The LSTM is trained using teacher forcing, meaning that we use the previous ground truth word as input at every time step during the training phase. We project each word into an embedding space of dimensionality $d_{emb} = 200$ by using a lookup table before the word is input to the LSTM. The hidden state $h$ and memory state $c$ of the LSTM is initialized with a linear projection of the latent representation $z$ to provide the LSTM with some context about the natural image. We use the same dimension for $h$ and $c$ as for $z$, i.e., $d_{z} = d_{h} = d_{c} = 200$. We minimize the cross-entropy loss at every time step by comparing the predicted word with the true word in the description. We apply dropout~\citeB{B:srivastava2014dropout} with a keep rate of 0.5 on the output hidden state $h_t$ before we input it through the linear layer that predicts the word at time step $t$. 

\vspace{-3mm}
\paragraph{Architectures for VCCA-private} In the VCCA-private models, the decoders has the same architectures as the VCCA models. The encoder for the shared latent variable $z$ is also the same as the encoder for the previous described models. The encoder for the private latent variable $u_{x}$ uses an identical architecture as encoder for $z$. We use a convolutional encoder for the iconic image private latent variable, which is a reversed DCGAN generator outputting the mean $\mu_{u_{i}}(i)$ and variance ${\sigma}_{u_{i}}^2(i)$. The text description encoder is an LSTM with the same architectural details as the LSTM decoder. We obtain an embedding for the description by averaging all of the hidden states $h_t$ generated from the LSTM, i.e., $\frac{1}{T} \sum_{t=1}^{T} h_t$, and input it to a linear layer outputting the mean $\mu_{u_{w}}(w)$ and the variance ${\sigma}_{u_{w}}^2(w)$ for the private latent variable for the text description. We use the same latent dimensions for the shared and private latent variables, i.e., $d_{z} = d_{u_{x}} = d_{u_{i}} = d_{u_{w}} = 200$. 

\vspace{-3mm}
\paragraph{Training the Single- and Multi-View Autoencoders} The single- and multi-view autoencoding models are trained for 200 epochs with batch size 64 and aims to minimize either their reconstruction losses or their corresponding ELBOs. We use the Adam optimizer with initial learning rate $10^{-4}$ and hyperparameters $\beta_1 = 0.9$ and $\beta_2 = 0.999$ in all experiments. The mean outputs $\mu_{z}(x)$ from the encoder are used as the latent representations of the image features to train a Softmax classifier. For the class label decoder, we use $K=1$ posterior samples for predicting the class label during training, while we set $K=5$ in the validation and test stages. 

\vspace{-3mm}
\paragraph{Grid Search} We run a hyperparameter search for the scaling weights $\lambda$ for the reconstruction losses of the views using a grid search with grid points $\{0.1, 1, 10, 100, 1000\}$. The grid search is performed for all VCCA and VCCA-private models. The weight for the natural image feature loss $\lambda_{x}$ is used as a reference by setting it to $\lambda_{x}=1$ throughout all experiments. This means that we only vary the scaling weights $\lambda_{i}$, $\lambda_{w}$, and $\lambda_{y}$. We run the grid searches using three different random seeds and average the resulting validation accuracies to select the best hyperparameter setting. Table \ref{paperB:tab:classification_results_on_test_set_with_hyperparameters}
shows the best hyperparameter settings for the VCCA and VCCA-private models with their test %validation 
accuracies. Note that we use the same scaling weights for SplitAE as the ones we found for its corresponding VCCA model. To summarize the grid search results, it is always beneficial to use scaling weights $\lambda > 1$ for the models to enhance the classification accuracy. This will make the models add some semantically meaningful information to the latent space from the additional views, which makes the models more suitable for downstream tasks such as classification. 
