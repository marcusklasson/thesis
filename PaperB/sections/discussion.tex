\section{Discussion}\label{sec:discussion}

In this section, we summarize the experimental results and discuss our findings.

\paragraph{Classification Results} In the first experiments (see subsection Classification Results in Results),
%(Section \ref{sec:classification_results}), 
we showed that utilizing all four views with SplitAE$_{x i w y}$ and VCCA$_{x i w y}$ resulted in the best classification performance on the test set. This indicates that these models take advantage of each view to learn representations that enhance their generalization ability compared to the single-view models. Moreover, using either the iconic images, the text descriptions or both views yields better representations for classification compared to using the natural image features alone. Note that it was necessary to properly set the scaling weights $\lambda$ on the reconstruction losses of the additional views to achieve good classification performance (see Table S2).
Whenever the weight values are increased, the model tries to structure the representations according to variations between the items in the upweighted view rather than structuring the items based on visual features in the natural images, e.g., shape, color, and pose of items, image backgrounds, etc. 
Thus, the latent representations are structured based on semantic information that describes the item itself, which is important for constructing robust representations that generalize well in new environments. Furthermore, the class label decoders performed on par with the separate Softmax classifiers in most cases. The upside with training a separate classifier is that we only would have re-train the classifier if we receive a new class in the dataset, while we would have to train the whole model from scratch when the model uses a class label decoder. Note that the encoder for any of the multi-view models can be used for extracting latent representations for new tasks, whether the model utilizes the label or not, since the encoder only uses natural images as input. 


\paragraph{Iconic Images vs. Text Descriptions} 
In Table \ref{tab:classification_results_on_test_set}, the iconic images yielded higher classification accuracies compared to using the text descriptions. This was also evident in Figure \ref{fig:varying_t} where the classification performance remains more or less the same regardless of the text description length $T$ when the models utilize iconic images. We believe that the main reasons for the advantages with iconic images lie in the clear visual features of the items in these images, e.g., their color and shape, which carry lots of information that is important for image classification tasks. However, we also observed that iconic images and text descriptions can yield different benefits for constructing good representations. In Figure \ref{fig:2d_visualizations_pca}, we see that iconic images and text descriptions make the model construct different latent representations of the grocery items. Iconic images structures the representations with respect to color and shape of the items (see Figure \ref{fig:2d_visualizations_pca_apples}), while the descriptions groups items based on their ingredients and flavor (see Figure \ref{fig:2d_visualizations_pca_juice_yoghurt}). Therefore, the latent representations benefit differently from utilizing the additional views and a combination of all of them yields the best classification performance as shown in Table \ref{tab:classification_results_on_test_set}. We want to highlight the results in Figure \ref{fig:2d_visualizations_pca_juice_yoghurt}, where the model manages to separate juice and yoghurt packages based on their text description. 
Refrigerated items, e.g., milk and juice packages, have in general very similar shapes and the same color if they come from the same brand.
There are minor visual differences between items of the same brand that makes it possible to differentiate between them, e.g., the picture of the main ingredient on the package and ingredient description. Additionally, these items can be almost identical depending on which side of the package that is present on the natural image. When utilizing the text descriptions, we add useful information on how to distinguish between visually similar items that have different ingredients and contents. This is highly important for using computer vision models to distinguish between packaged items without having to use other kinds of information, e.g., barcodes. 

\paragraph{Text Description Length} We showed 
%in Section \ref{sec:classification_results} 
that the text descriptions are useful for the classification task, and that careful selection of the description length $T$ is important for achieving the best possible performance (see subsection Classification Results in Results). In Figure \ref{fig:varying_t}, we observed that most models achieve significantly better classification performance when the text description length $T$ increases up until $T=32$. The reason for this increase is due to the similarities between the descriptions of items from the same kind or brand, such as milk and juice packages. 
For instance, in Table S1, the first sentence in the descriptions for the milk packages only differ by the ninth word, which is \textit{organic} for the ecological milk package. 
This means that their descriptions will be identical when $T=8$. Therefore, the descriptions will become more different from each other as we increase $T$, which helps the model to distinguish between items with similar descriptions. However, the classification accuracies have more or less saturated when setting $T > 32$, which is also due to the similarity between the descriptions. For example, the bell pepper descriptions in Table S1 only differ by the second word that describes the color of the bell pepper, i.e., the words \textit{yellow} and \textit{orange}. We also see that the third and fourth sentences in the descriptions of the milk packages are identical. The text descriptions typically have words that separate the items in the first or second sentence, whereas the following sentences provide general information on ingredients and how the item can be used in cooking. For items of the same kind but of different colors or brand, e.g., bell peppers or milk packages respectively, the useful textual information for constructing good representations of grocery items typically comes from a few words in the description that describes features of the item. Therefore, the models yield better classification performance when $T$ is set to include at least the whole first sentence of the description. We could select $T$ more cleverly, e.g., by using different $T$ for different descriptions to make sure that we utilize the words that describe the item or filter out non-informative words for the classification task. 

\paragraph{VCCA vs. VCCA-private} The main motivation for using VCCA-private is to use private latent variables for modeling view-specific variations, e.g., image backgrounds and writing styles of text descriptions. This could allow the model to build shared representations that more efficiently combine salient information shared between the views for training better classifiers. This would then remove noise from the shared representation since the private latent variables are responsible for modeling the view-specific variations. For VCCA-private$_{x w}$, we observed that the private latent spaces managed to group different image backgrounds and grocery items with similar text descriptions respectively in Figure \ref{fig:pca_vcca_private_xw_ux} and \ref{fig:pca_vcca_private_xw_uw} respectively. This model also performed on par with VCCA$_{x w}$ regarding classification performance in Table \ref{tab:classification_results_on_test_set}. However, we also saw in the same table that the VCCA-private models using the iconic image perform poorly on the classification task compared to their VCCA counterpart. The reason for why this model fails is because of a form of \textit{posterior collapse}~\cite{bowman2015generating} in the encoder for the iconic image, where the encoder starts outputting random noise. We noticed this as the KL divergence term for the private latent variable converged to zero when we trained the models for 500 epochs (see Figure S2 and S3), which means that the encoder outputs a distribution which equals a Gaussian prior. The same phenomenon occurs for VCCA-private with the text description as well. We have also experimented with other models with encoders, such as Deep CCA and DCCAE, which also suffered from the collapsing encoder problem for the additional views. Therefore, we believe that the collapsing effect is a consequence of only having access to a single iconic image and text description for every grocery item. Therefore, a potential solution would be to extend the dataset with multiple web-scraped iconic images and text descriptions for every grocery item, which would then establish some variability within the view for each item class. Another possible solution would be to use data augmentation techniques to create some variability in the web-scraped views. For example, we could take a denoising approach and add noise to the iconic images which would force the decoder to reconstruct the real iconic images~\cite{vincent2010stacked}. For the text descriptions, we could mask words at random in the encoder and let the decoder predict the whole description, which would work as a form of \textit{word dropout}~\cite{bowman2015generating, devlin2018bert}. We leave this for future work if such augmentation techniques can create the needed variability for learning more robust representations as well as discovering the structures of the private latent spaces that this approach would bring. 

\paragraph{Decoded Iconic Images} %In Section \ref{sec:decoding_iconic_images}, we 
We observed with image similarity metrics that the quality of decoded iconic images coheres to some extent with the classification performance for VCCA models using the iconic images (see subsection Decoding Iconic Images from Unseen Natural Images in Results). We also showed that the decoded images are visually plausible decoded images with respect to colors, shapes, and identities of the grocery items in the dataset. The values for the metrics PSNR, SSIM, and KL divergence are similar across the different VCCA models. Since we used RGB values for estimating KL, we believe that including spatial information of pixels or using other color spaces, e.g., Lab, could provide more information about the dissimilarities between the decoded and true iconic images. To thoroughly assess the relationship between good classification performance and accurately decoding the iconic images, we also suggest evaluating the image quality on other image similarity metrics, e.g., perceptual similarity~\cite{zhang2018unreasonable}. Finally, we see the decoding of iconic images as a promising method to evaluate the quality of the latent representations as well as enhancing the interpretability of the classification. For example, we could inspect decoded iconic images qualitatively or by using image similarity metrics to determine how certain the model was about the present items in the natural images, which could then be used as a tool for explaining misclassifications.

\subsection{Conclusions}
\label{sec:conclusions}

In this paper, we introduce a dataset with natural images of grocery items taken in real grocery store environments. Each item class is accompanied by web-scraped information in the form of an iconic image and a text description of the item. The main application for this dataset is for training image classifiers that can assist visually impaired people when shopping for groceries but is not limited to this use case only. 

We selected the multi-view generative model VCCA that can utilize all of the available data views for image classification. 
To evaluate the contribution to the classification performance for each view, we conducted an ablation study comparing classification accuracies between VCCA models with different combinations of the available data types. We showed that utilizing the additional views with VCCA yields higher accuracies on classifying grocery items over models only using the natural images.
The iconic images and text descriptions impose different structures of the shared latent space, where we observed that iconic images help to group the items based on their color and shape while text descriptions separate the items based on differences in ingredients and flavor.
These types of semantics that VCCA has learned can be useful for generalizing to new grocery items and other object recognition tasks. We also investigated VCCA-private, which introduces private latent variables for view-specific variations, that separates the latent space into shared and private spaces for each view to provide high-quality representations. However, we observed that the private latent variables for the web-scraped views became uninformative by modeling noise due to the lack of variations in the additional web-scraped views. This encourages to explore new methods for extracting salient information from such data views that can be beneficial for downstream tasks. 

An evident direction of future work would be to investigate other methods for utilizing the web-scraped views more efficiently. For instance, we could apply pre-trained word representations for the text description, e.g., BERT~\cite{devlin2018bert} or GloVe~\cite{pennington2014glove}, to see if they enable the construction of representations that can more easily distinguish between visually similar items. 
Another interesting direction would be to experiment with various data augmentation techniques in the web-scraped views to create view-specific variations without the need for collecting and annotating more data. It is also important to investigate how the model can be extended to recognize multiple items. Finally, we see zero- and few-shot learning~\cite{xian2018zero} of new grocery items and transfer learning~\cite{pan2010transferlearning} as potential applications where our dataset can be used for benchmarking of multi-view learning models on classification tasks. 