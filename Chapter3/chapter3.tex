%*******************************************************************************
%*********************************** Third Chapter *****************************
%*******************************************************************************

\chapter{Fine-Grained Image Recognition}\label{chap3}

In this chapter, we provide an overview for Paper \ref{paperA} and \ref{paperB} that focuses on fine-grained image recognition (FGIR)~\cite{wei2021fine} of grocery items. 
Object recognition in grocery shopping scenarios is a challenging task for visually impaired (VI) people~\cite{brady2013visual,cimarolli2012challenges,szpiro2016finding} due to the necessity of visual capabilities to navigate and identify the products. Regarding identifying products, groceries are often structured in fine-grained manners where details on appearance and shape help us to distinguish between different items. However, since distinguishing between packaged items often requires reading text and raw food items can have similar shapes, recognizing fine-grained details can be difficult without the ability to see. 
In recent years, there has been an emerging interest of development of computer vision-based assistive technologies for assisting VIs with grocery shopping~\cite{lanigan2006trinetra,winlock2010toward,sosa2017hands,boldu2020aisee,zientara2017third,george2015fine}. Nevertheless, fine-grained recognition of groceries still remain challenging due to the need for real-world training data for such systems to be robust in grocery stores. We aimed to enhance the robustness and predictive performance of image classifiers by combining natural images with external web information with the items. In Paper \ref{paperA}, we have collected a dataset with natural images of groceries with a mobile phone camera, and web-scraped images and text descriptions about the items, for evaluating image classifiers in real-world supermarket scenarios. We perform an ablation study over the different data types in the data set in Paper \ref{paperB}, where we show that utilizing all data types improves the fine-grained classification performance over image classifiers trained with only the natural images. We cover some related work (Section \ref{chap3:sec:related_work}), describe the dataset collection process (Section \ref{chap3:sec:dataset_collection}) and our used approach for utilizing multi-view data (Section \ref{chap3:sec:fgr_of_grocery_items}), as well as summarize the experimental results from the ablation study (Section \ref{chap3:sec:experiments}). 


%This chapter presents an approach for enhancing fine-grained classification performance of grocery items by using web-scraped information. We focus on classification of grocery items due to applicability in assistive vision and its potential to enhance the independence of visually impaired (VI) people [ADD groceries/shopping/object recognition for VI REFs]. Initially, we were interested in learning classifiers with natural images taken in the grocery stores combined with web-scraped information about the grocery items, such as iconic images and text descriptions from supermarket websites. Using iconic images have been used in grocery image classification earlier [ADD grocery paper REFs], however, utilizing text descriptions was as far we know absent for this application even if it has been successfully applied in other image classification problems~\cite{wah2011cub, nilsback2008automated, bujwid2021large}. Thus, we collected our own dataset of grocery items images using a mobile phone camera as well as web-scraped images and text descriptions to study whether this multi-view approaches would benefit training the classifiers (Section \ref{chap3:sec:dataset_collection}). We then select a multi-view learning framework based on the Variational Autoencoder (VAE) for investigating how the different data views affect the fine-grained classification performance (Section \ref{chap3:sec:fgr_of_grocery_items}). 


\section{Related Work}\label{chap3:sec:related_work}

In this section, we will briefly discuss the related work on FGIR, datasets for FGIR, and multi-view learning. 

%\subsection{Fine-Grained Image Recognition}
\vspace{-3mm}
\paragraph{Fine-Grained Image Recognition.} 
The goal with FGIR~\cite{wei2021fine} is to distinguish between a set of visually similar object classes that belong to some super-class. The main challenge is to recognize the fine-grained visual details that are required for discriminating between visually similar objects. In recent years, the successes with deep learning in computer vision have encouraged researchers to explore various approaches for FGIR in different domains with the goal of discriminating between sub-classes of different animals~\cite{van2018inaturalist}, cars~\cite{krause2013stanford_cars}, fruits~\cite{hou2017vegfru}, and retail products~\cite{wei2019rpc}. These approaches can broadly be divided into three directions where the deep networks utilize (i) localization-classification subnetworks, (ii) end-to-end feature encodings, and (iii) external information. 
In (i), the goal is to find parts that are shared across the sub-classes of some general object and then discover how these localized parts differ in appearance. 
This can be achieved by utilizing feature maps from convolutional layers as local descriptors~\cite{zhang2016picking, wang2018learning, ding2019selective}, employing detection and segmentation techniques for localizing object parts~\cite{branson2014bird,zhang2014part,zhang2016weakly}, or leveraging attention mechanisms in scenarios where common object parts are difficult to represent~\cite{fu2017look,zheng2017learning,sun2018multi}. 
With (ii), the goal has been to learn features that are better at capturing local differences by computing second-order feature interactions~\cite{lin2015bilinear,cui2017kernel}, as well as designing novel loss functions~\cite{dubey2018maximum,dubey2018pairwise,chang2020devil}.
In (iii), data from external domains, such as the web or different modalities (e.g., text or audio), is utilized to influence the learning of fine-grained details in the objects. These additional data types can be relatively cheap to collect and serves as extra guidance for learning useful representations to the FGIR task. The goal with learning from noisy web data is to reduce the need for human-annotated data~\cite{krause2016unreasonable,xu2016webly,sun2019learning}, while learning from multimodal data aims to obtain a joint representation that captures the correspondences between the images and the other modalities~\cite{he2017fine,zhang2018audio,chen2018knowledge}. In addition to FGIR, both web-scraped and multimodal data has been used for zero-shot learning to transfer knowledge from densely annotated categories to learn new fine-grained classes~\cite{reed2016learning,niu2018webly,akata2015evaluation}. Our work in Paper \ref{paperA} and \ref{paperB} is mostly related to (iii) FGIR with external information, where we use web-scraped images and text descriptions in combination with natural images to improve the classification performance of fine-grained grocery recognition.


%The goal with FGIR~\cite{wei2021fine} is to distinguish between images with multiple visually similar sub-categories that belong to a super-category. For example, various attempts have been made to discriminate between sub-categories of different animals~\cite{van2018inaturalist}, cars~\cite{krause2013stanford_cars}, fruits~\cite{hou2017vegfru}, retail products~\cite{wei2019rpc}, etc. The challenge is to recognize differences that are sufficient for discriminating between objects that are generally similar but differ in fine-grained visual details. In recent years, the successes with deep learning in computer vision have encouraged researchers to explore various approaches for FGIR that can broadly be divided into three directions for recognition by utilizing (i) localization-classification subnetworks, (ii) end-to-end feature encoding, and (iii) external information. In (i), the goal is to find object parts that are shared across the sub-categories for discovering details that make the part representations different. This can be achieved by utilizing feature maps from the activations of convolutional layers as local descriptors~\cite{zhang2016picking, wang2018learning, ding2019selective}, employing detection and segmentation techniques for localizing object parts[REFs], or leveraging attention mechanisms when common object parts are difficult to represent or even define [REFs]. With (ii), the goal has been to learn features that are better at capturing subtle and local differences by, for instance, performing high-order features interactions~\cite{lin2015bilinear} as well as designing novel loss functions [Add REFs]. In the third approach (iii), the goal is to leverage external information, for example, web data and multimodal data, in FGIR as additional supervision to the images. We will put more focus on the approach on FGIR with external information next, as we use this approach in Paper \ref{paperA} and \ref{paperB}. 

%\subsection{Recognition with External Information}
%\paragraph{Recognition with External Information.} 
%Learning fine-grained details about objects often requires large amounts of labeled data. To ease the need for large amounts of accurately labeled images, there have been several attempts to let either web-scraped or multimodal data influence learning the fine-grained features of the sub-categories to boost the FGIR performance. Web-scraped images may be noisy in the sense that retrieved images may have high-variations of the objects. For example, the objects of interest can look different in appearance, and there could also be other irrelevant objects in the images that potentially occlude the category to recognize. Hence, incorporating web-scraped data into the training set may establish a domain gap between the easily acquired web data and the original training set which we need to overcome by reducing the domain gap or reducing the negative effects of the noisy web data that can disturb the learning. Another direction than using web-scraped data is to utilize multimodal data, for example, images, text and knowledge bases, for boosting the classification performance. In FGIR, the goal is to establish a joint representation between the images and additional data sources, where the additional data should act like extra guidance for learning useful representations that capture the fine-grained details of objects. Text descriptions have been a popular data type to combine with images, which can be both easy and cheap to collect as they can be accurately generated by non-experts. High-level knowledge graphs of objects have also been used and can contain rich knowledge useful for fine-grained recognition. In addition to FGIR, both web-scraped and multimodal external information has been used for zero-shot learning to transfer knowledge from annotated categories to new fine-grained categories. In Paper \ref{paperA}, we collect web-scraped images and text descriptions of grocery items to accompany real-world images of groceries for FGIR. Then, in Paper \ref{paperB}, we perform a study using multi-view learning to investigate how the external information can enhance the classification performance. Next, we will cover the related work for the multi-view learning approach that we used. 

\vspace{-3mm}
\paragraph{Datasets for Fine-Grained Image Recognition}
There exist many image datasets for benchmarking computer vision models~\cite{deng2009imagenet, nilsback2008automated, wah2011cub, krizhevsky2009learning, lin2014microsoft, van2018inaturalist, massiceti2021orbit, barbu2019objectnet}. These datasets ranges from large-scale datasets annotated by groups of classes, such as~\cite{deng2009imagenet, openimages}, to smaller but densely-annotated datasets~\cite{everingham2010pascal, lin2014microsoft, krishna2017visual, gupta2019lvis}. There also exist domain-specific datasets for FGIR tasks of animals and species~\cite{van2018inaturalist, van2021benchmarking, khosla2011stanforddogs, van2015building, parkhi12a, lampert2009learning}, fashion~\cite{liu2016deepfashion, jia2020fashionpedia}, airplanes and cars~\cite{vedaldi2014understanding, maji2013fine, krause20133d, yang2015large, gebru2017fine}, faces~\cite{parkhi2015deep, huang2008labeled, cao2018vggface2, guo2016ms}, and foods~\cite{hou2017vegfru, bossard2014food}. Several of the mentioned datasets have also been extended to benchmarking in zero/few-shot image classification~\cite{lampert2013attribute, reed2016learning, triantafillou2019meta, bujwid2021large}. In computer vision for visually impaired people, we have recently seen an emergence of datasets collected by people who are blind/low-vision~\cite{kacorri2017teachable, gurari2018vizwiz, sosa2017hands, massiceti2021orbit, gurari2019vizwiz}. Datasets for FGIR of grocery items in their natural environments, such as grocery stores, shelves, and kitchens, have been addressed in plenty of previous works~\cite{jund2016freiburg, waltner2015mango, george2014recognizing, merler2007recognizing, geng2018fine, wei2019rpc}. Similarly, there exist other kinds of food datasets with images of various food dishes~\cite{bossard2014food, kawano2014automatic, min2019ingredient, rich2016towards}, cooking videos~\cite{damen2018scaling, damen2021rescaling}, recipes~\cite{marin2019learning, salvador2017learning, yagcioglu2018recipeqa}, and also restaurant-oriented information~\cite{beijbom2015menu, xu2015geolocalized}. Our grocery item dataset (see Section \ref{chap3:sec:dataset_collection}) shares several similarities with the mentioned works above. For instance, all images of raw and packaged groceries are taken in their natural environment, images are taken with a mobile phone camera from an egocentric view-point, grocery classes are hierarchically labeled, and each class have an additional web-scraped iconic image and text description.


\vspace{-3mm}
\paragraph{Multi-view Learning.} 
Machine learning with multiple types of data is an active research field~\cite{baltruvsaitis2018multimodal, xu2013survey}. In particular, the intersection of computer vision and natural language processing has been of great interest and has led to applications such as image captioning~\cite{xu2015show, lu2018neural, aneja2019sequential}, visual question answering~\cite{antol2015vqa, hudson2018compositional, hu2019language}.
In this thesis, we have focused on learning joint representations across different data types by applying the subspace learning approach from multi-view learning~\cite{xu2013survey}. A view can be defined as data that has been recorded from a specific sensor~\cite{salzmann2010factorized}. For example, we consider natural and web images to be from different views as the images have been recorded using different cameras. Subspace learning approaches aims to learn joint representations from multiple views by assuming that each view have been generated from a latent space that is shared among the views. Most methods originate from Canonical Correlation Analysis~\cite{hotelling1936relations} (CCA) where the goal is to linearly project pairs of different views into a lower-dimensional space where the correlation between the projections is maximized. 
There exist various extensions of CCA which uses deep neural networks for learning nonlinear mappings to extract more rich features of the views, such as Deep CCA~\cite{andrew2013deep} and Deep Canonically Correlated Autoencoders~\cite{wang2015deep}. Inspired from the Deep CCA variants, Variational CCA~\cite{wang2016deep} (VCCA) is a multi-view deep generative model that can both learn joint representations and generate new data by sampling from the shared latent space. 
An advantage of VCCA is the straight-forward extension to modeling both a shared latent space and private latent spaces which capture the view-specific variations to focus the learning of shared variations into the shared latent space~\cite{hyvarinen2000independent, salzmann2010factorized, tsai2018learning, wang2016deep, zhang2016inter, damianou2021multi}.
In Paper \ref{paperB}, we employ VCCA, and its extention to private latent spaces, for learning joint representations of the different data views in our grocery item dataset to obtain more robust image classifiers.   


%Especially, there has been many research efforts made in the intersection of computer vision and natural language processing for applications in image captioning~\cite{xu2015show, lu2018neural, aneja2019sequential}, visual question answering~\cite{antol2015vqa, hudson2018compositional, hu2019language}, and learning joint representations with visual and text data~\cite{vedantam2017generative, wu2018multimodal,shi2019variational}. In this thesis, we focus on the latter where we apply ideas from multi-view learning~\cite{xu2013survey} for learning joint representations across data views. A view can be defined as any signal or data measured by some appropriate sensor~\cite{salzmann2010factorized}. A common approach for learning joint representations from multiple views is to assume that each view have been generated from the same latent space shared between all views. An example method is Canonical Correlation Analysis~\cite{hotelling1936relations} (CCA) where the goal is to linearly project pairs of different views into a lower-dimensional space where the correlation between the projections is maximized. There exist various extensions of CCA which uses deep neural networks for learning nonlinear mappings to extract more rich features of the views, such as Deep CCA~\cite{andrew2013deep} and Deep Canonically Correlated Autoencoders~\cite{wang2015deep}. Inspired from the Deep CCA variants, Variational CCA~\cite{wang2016deep} (VCCA) is a multi-view deep generative model that can both learn joint representations and generate new data by sampling from the shared latent space. In the case of noisy views, an advantage of VCCA is that it can be extended to modeling both a shared latent space and private latent spaces for each view which capture view-specific variations to ease the learning of shared variations~\cite{hyvarinen2000independent, salzmann2010factorized, tsai2018learning, wang2016deep, zhang2016inter, damianou2021multi}. In Paper B, we investigate whether the classification performance of grocery items can be improved by extracting the view-specific variations in the web-scraped views by comparing the classification performance of the joint latent representations from standard VCCA and the VCCA-private.

%Learning from combinations of different data sources has been studied frequently in machine learning~\cite{baltruvsaitis2018multimodal, xu2013survey}. Especially, there has been many research efforts made in the intersection of computer vision and natural language processing for applications in image captioning~\cite{xu2015show, lu2018neural, aneja2019sequential}, visual question answering~\cite{antol2015vqa, hudson2018compositional, hu2019language}, and learning joint representations with visual and text data~\cite{vedantam2017generative, wu2018multimodal,shi2019variational}. In this thesis, we focus on the latter where we apply ideas from multi-view learning~\cite{xu2013survey} for learning joint representations across data views. A view can be defined as any signal or data measured by some appropriate sensor~\cite{salzmann2010factorized}. A common approach for learning joint representations from multiple views is to assume that each view have been generated from the same latent space shared between all views. An example method is Canonical Correlation Analysis~\cite{hotelling1936relations} (CCA) where the goal is to linearly project pairs of different views into a lower-dimensional space where the correlation between the projections is maximized. There exist various extensions of CCA which uses deep neural networks for learning nonlinear mappings to extract more rich features of the views, such as Deep CCA~\cite{andrew2013deep} and Deep Canonically Correlated Autoencoders~\cite{wang2015deep}. Inspired from the Deep CCA variants, Variational CCA~\cite{wang2016deep} (VCCA) is a multi-view deep generative model that can both learn joint representations and generate new data by sampling from the shared latent space. In the case of noisy views, an advantage of VCCA is that it can be extended to modeling both a shared latent space and private latent spaces for each view which capture view-specific variations to ease the learning of shared variations~\cite{hyvarinen2000independent, salzmann2010factorized, tsai2018learning, wang2016deep, zhang2016inter, damianou2021multi}. In Paper B, we investigate whether the classification performance of grocery items can be improved by extracting the view-specific variations in the web-scraped views by comparing the classification performance of the joint latent representations from standard VCCA and the VCCA-private.






\section{Dataset Collection}\label{chap3:sec:dataset_collection}


\begin{wrapfigure}{r}{0.37\textwidth}
	%\centering
	\setlength{\fboxsep}{0pt}%
	\setlength{\fboxrule}{1pt}%
	\vspace{-4mm}
	\fbox{\includegraphics[width=0.15\textwidth]{Chapter3/imgs/Kaiser_029}}%
	~
	\fbox{\includegraphics[width=0.15\textwidth]{Chapter3/imgs/Arla-Ecological-Medium-Fat-Milk_011}}%
	\\[4pt]
	\fbox{\includegraphics[width=0.15\textwidth]{Chapter3/imgs/Green-Bell-Pepper_020}}%
	~
	\fbox{\includegraphics[width=0.15\textwidth]{Chapter3/imgs/Floury-Potato_016}}%
	\vspace{-2mm}
	%\captionsetup{margin={-1mm}}
	\caption{Example images of \\ challenging scenarios.
	}
	\vspace{-3mm}
	\label{fig:example_scenarios_groceries}
\end{wrapfigure} 
In this section, we describe our procedure for collecting the image dataset of grocery items in Paper \ref{paperA}. We visited several supermarkets in Stockholm, Sweden, to collect natural images of groceries with a mobile phone camera to imitate a shopping scenario using an assistive vision device. The collected images captures situations that can be challenging for assistive vision devices, such as misplaced items, hand occlusions, multiple instances and classes present, and various lighting conditions (see Figure \ref{fig:example_scenarios_groceries}). All images were taken with a single targeted item in mind, such that each image is paired with a single label. For items which belong to a clear super-class, e.g., apples and milk packages, we also provided the general class of the items to establish a hierarchical labeling structure of the data. Furthermore, we complemented the image dataset with web-scraped information of each grocery item. More specifically, we visited the online shopping website of a Swedish supermarket chain and downloaded 1) an iconic image of the item on a white background, 2) a text description that describes the flavor and ingredients of the item, and 3) the nutrition values for items where it was applicable. We show four examples of grocery items and their web-scraped information in Table \ref{tab:grocery_store_dataset}. Since these data types are on a class-based level, we can use the web-scraped information as weak supervision to guide the classifier to learn fine-grained details that helps discriminating between visually similar items. Finally, we made the dataset publicly available under the MIT license\footnote{Grocery Store Dataset URL: \url{https://github.com/marcusklasson/GroceryStoreDataset}}.

%In this section, we describe our procedure for collecting the image dataset of grocery items in Paper \ref{paperA}. As the target use case is grocery shopping with an assistive vision device, we visited several supermarkets and collected natural images of the groceries with a mobile phone camera to imitate such scenarios. Hence, the collected images will capture situations that can be challenging for the assistive device, such as, various lighting conditions, multiple instances and classes present, hand occlusions, and misplaced items. All images were taken with a single targeted item in mind, such that each image is paired with a single label. For items which belong to a clear super-class, for example, various kinds of apples and milk packages, we also provided the general class of the items to establish a hierarchical labeling structure of the data. Collecting natural images of the grocery items is unfortunately a time-consuming process. Furthermore, as the surroundings in every grocery store varies, it may be difficult to build accurate classifiers that can recognize fine-grained details solely from natural images. Hence, we need some cheaper procedure that can complement the collection of real-world images for boosting the classification performance of the groceries. 

%We have complemented the image dataset with external information from the web of each grocery item that can be used for training classifiers. In the past years, most supermarket chains have the option for consumers to purchase groceries online from their websites. The website usually provides each grocery item with an iconic image of the item on a white background, a text description that describes the flavor and ingredients of the item, as well as nutrition values if applicable. We downloaded these information types of all grocery item classes by web-scraping the online shopping website of a supermarket chain. We show four examples of grocery items and their web-scraped information in Table \ref{tab:grocery_store_dataset}. Since these data types are on a class-based level, we can use the web-scraped information as weak supervision to guide the classifier to learn fine-grained details that helps discriminating between visually similar items.   

\begin{table}[t]
	\centering
	\caption{Examples of grocery item classes in the Grocery Store dataset. We display four different items (coarse-grained class in parenthesis), followed by two natural images taken with a mobile phone inside grocery stores. Next comes the web-scraped information of the items consisting of an iconic image and a text description. We have highlighted ingredients and flavors in the text description that are characteristic for the specific item. }
	\vspace{-10pt}
	\setlength{\fboxsep}{0pt} 
	\setlength{\fboxrule}{0.33pt}
	\input{Chapter1/tables/dataset_paperA}
	\label{tab:grocery_store_dataset}
	%\vspace{-7mm}
\end{table}

\section{Fine-Grained Image Recognition of Grocery Items}\label{chap3:sec:fgr_of_grocery_items}

In this section, we describe the problem setting of the grocery item classification task from the dataset collected in Paper \ref{paperA}. Moreover, we describe our approach from Paper \ref{paperB} for learning joint representations of the available data views to improve the classification performance. 

%This section describes the approaches we used for classification of grocery items from the available data types in the collected dataset. We begin by introducing the problem setting, followed by describing the methods for learning representations from the available data views. 

\subsection{Problem Setting}

We focus on the application of training an image recognition app in mobile phones for assisting VIs with grocery shopping. Training such image classifiers typically requires immense amounts of annotated images of the groceries. Additionally, the natural images should be collected in scenarios where the app would be used, such as grocery stores or home environments, which further complicates the collection procedure. To reduce the need for collecting and annotating natural images from mobile phones, we aim to use web-scraped information about the groceries as additional supervision when training the image classifiers. The web-scraped information should help the classifier to focus on learning the learn fine-grained details of the items to improve the classification performance and robustness in real environments. 

%The application we are focusing on is grocery shopping with an assistive vision device. The device could for instance be a mobile phone app where the groceries are recognized by an in-built image classifier from natural images taken with the camera. Training such image classifier to be robust in grocery store environments would typically require an immense amount of labeled training examples of all available groceries. To reduce the need for labeled training data, we aim to combine the collected natural images with web-scraped information about the groceries when training the classifier. The goal is that incorporating the web-scraped information should help the classifier to learn fine-grained details about the items to enhance the classification performance and robustness. 

The following data views are available in the dataset for training the image classifiers:
%The available data views that are available for training image classifiers is denoted as follows:
\begin{itemize}[noitemsep,topsep=1pt]
	\item $\vI$: Natural images of the grocery items taken inside grocery stores.
	\item $\vx$: Feature vectors of the natural images $\vI$ extracted from some CNN $f_{\vvarphi}$ parameterized by $\vvarphi$. 
	\item $\vy$: Class labels of the grocery items in the corresponding natural images.
	\item $\vi$: Iconic images of the grocery items scraped from a supermarket website.
	\item $\vw$: Text description of the grocery items scraped from the same supermarket website as $\vi$.
\end{itemize}
See Table \ref{tab:grocery_store_dataset} for examples of the data views. 
The straightforward approach is to train a CNN with pairs of natural images $\vI$ and class labels $\vy$ in a supervised setting. However, the number of samples may be insufficient for training a CNN from scratch, which can lead to overfitting and low generalization capability to new images. 
One alternative is to employ transfer learning~\cite{zhuang2020comprehensive} where parameters from a CNN pretrained on some large dataset, such as Imagenet~\cite{deng2009imagenet}, are used for initialization and we adapt the network to the grocery item recognition task by either 1) fine-tune some of the final layers, or 2) train a linear classifier from extracted features~\cite{sharif2014cnn}. We take the latter approach and use extracted features for training to mitigate overfitting. Furthermore, we use the web-scraped data views for learning more rich representations of the grocery items that can be used for training more accurate and robust image classifiers, which we present in the next section.


%The simplest approach is to take a standard supervised approach and train a CNN from the natural image and class label pairs. An alternative is leverage from CNNs pre-trained on a large dataset, such as Imagenet~\cite{deng2009imagenet}, and fine-tune the final classification layer to the grocery item recognition task~\cite{sharif2014cnn}. We use ideas from multi-view learning~\cite{xu2013survey} and VAEs~\cite{kingma2013auto} for learning joint representations from the available data views that can be used for training the image classifiers, which we present in the next section.  


\subsection{Multi-View Representation Learning of Grocery Items}

This section describes the approach we took for learning representations of grocery items that are shared across the available data types. We employ a deep latent variable model called Variational Canonical Correlation Analysis~\cite{wang2016deep} (VCCA) for learning the shared representation. The main assumption in VCCA is that each data view have been generated from the same latent space. The goal then is to learn this latent space that captures the correspondences between all views into representations shared across the views for the grocery items. This representation can then be utilized for enhance the learning more accurate classifiers as well as for performing tasks such as synthesis and prediction of novel images. Next, we describe how to enable learning the shared latent space. 

Capturing variations from each view in the learned representation is performed by predicting the original views from the latent space. To obtain the latent representation, we extract the representation by encoding the natural images with neural network. The extracted representation is then used for predicting each view individually by inputting the representation through separate neural networks. Note that we only use the natural images for extracting the latent representation here since it is the only view that is available at test time when we want to use the learned classifier in the grocery store. We have two options for exploiting the new representation to train classifiers. The first option is to train the classifier with the latent representations after we have learned the latent space as described above. The second option is to train the classifier and learning the latent space simultaneously by adding an additional classifier network predicting the class label with the latent representation as input. 

\begin{figure}[t]
	\centering
	\resizebox{0.95\textwidth}{!}{
		\input{Chapter3/tikz/vcca}
	}
	\caption{VCCA architecture used in Paper \ref{paperB}}
	\label{fig:vcca_architecture}
\end{figure}



%we take an encoder-decoder approach as these have been successful in applications with data with images and class-specific additional data types. this is in contrast to webly supervised methods where the web data usually has many instances per class. 

%we use shared subspace learning approach from multi-view learning because it is a simple approach that gives us a joint representation across the views that we can use for training classifiers. This also opens up for using generative modeling approaches 


\section{Experiments}\label{chap3:sec:experiments}

In this section, we summarize the main results from the experimental study in Paper B. We performed an ablation study with VCCA over the combinations of available data views to investigate how each data view contributes to the classification performance. First, we present results on fine-grained classification performance between the compared methods. Then, we provide insights in how the web-scraped icnoc images and text descriptions contribute to the boosting the classification performance by visualizing their the joint latent spaces. Finally, we demonstrate how the iconic image decoder in VCCA can be used for explaining the misclassifications by generating iconic images from natural images at test time. 

We compare the VCCA models against two CNN baselines that uses the DenseNet~\cite{huang2017densely} architecture. The first baseline is a DenseNet trained from scratch on the dataset, and the second baseline is a Softmax classifier trained from image features extracted from a DenseNet pre-trained on ImageNet. We denote which data views that are used by VCCA using subscripts. For instance, VCCA$_{\vx \vi \vw}$ means that the natural images $\vx$, iconic images $\vi$, and text descriptions $\vw$ are utilized for learning the joint latent representation. These VCCA models uses the two-stage classifier setup with steps 1) train VCCA on the data views, and 2) train 1-layer MLP classifier using the extracted latent representations from VCCA and the corresponding class labels. We also compare against VAE$_{\vx}$ only using the natural images $\vx$ trained in this setting. The VCCA models with class label decoders are denoted by using $\vy$ in the subscript, such as VCCA$_{\vx \vi \vw \vy}$. 

\begin{figure}[t]
	\centering
	\setlength{\figwidth}{0.62\textwidth}
	\setlength{\figheight}{.22\textheight}
	\input{Chapter3/figures/accuracy}
	\vspace{-2mm}
	\caption{Fine-grained accuracies on the Grocery Store dataset for all classification methods. We show the means and standard deviations averaged over 5 seeds. Adding the iconic image $\vi$ and text description $\vw$ for learning joint representations with VCCA improves the classification performance over approaches that only utilize the natural images and class labels.}
	\label{fig:fine_grained_classification_results}
	\vspace{-4mm}
\end{figure}

\paragraph{Fine-Grained Classification Results.} Adding the web-scraped views in VCCA improves the classification performance over approaches that only utilize the natural images and class labels. Figure \ref{fig:fine_grained_classification_results} shows a bar plot over the fine-grained accuracies achieved by all classification methods. Among the methods only using natural images and class labels, we see that the Softmax baseline performs best, which could be due to some information loss when compressing the images into as low-dimensional latent representations with VAE$_{\vx}$ and VCCA$_{\vx \vy}$. The performance of VCCA significantly improves over Softmax when the iconic image $\vi$ and text description $\vw$ are used, which shows that both data views are useful for enhancing the fine-grained classification performance. Comparing VCCA$_{\vx \vi}$ and VCCA$_{\vx \vw}$, we see that utilizing the iconic image has an advantage over using the text description for improving the performance. This could be due to the fact that the text descriptions are providing information on ingredients and flavors rather than visual appearance. Combining both $\vi$ and $\vw$ achieves on par performance as only utilizing the iconic image, which could potentially be improved by filtering the text descriptions for obtaining words relevant for describing the fine-grained details of the items. Finally, we observe that both classification options for VCCA performs similar, which could potentially be since $\vi$ and $\vw$ acts as labels since there is only a single instance of these views for all classes.  



 \paragraph{Visualization of Latent Space.} The web-scraped iconic images and text descriptions structures the grocery items based on view-specific similarities in the latent space that are beneficial for fine-grained classification. In Figure \ref{fig:latent_space_visualizations}, we illustrate how adding either the iconic image $\vi$ or the text description $\vw$ changes the structure of the latent space, where we have used PCA to project the latent representations into a 2-dimensional space. In Figure \ref{fig:latent_space_visualizations}(a-c), we have plotted the corresponding iconic image for all latent representations of models VAE$_{\vx}$, VCCA$_{\vx \vi}$, VCCA$_{\vx \vw}$ for visualization purposes. The latent space for VAE$_{\vx}$ separates raw and packaged grocery items into two separate clusters. When adding the iconic image in VCCA$_{\vx \vi}$, we observe that the latent space becomes structured according to the color of the items. For VCCA$_{\vx \vw}$, the latent space becomes structured according to the ingredients of the items, where we see in the raw food cluster that bell peppers are placed in the upper region while apples are in the lower region. 

Next, we focus on a certain set of classes to inspect to gain more insights in how the additional data views affect the latent space. First, we focus on the green and red apples classes to inspect how the views handle visually different items in color. In Figure \ref{fig:latent_space_visualizations}(d-f), we plot the latent representations for the three models but highlight the green and red apple classes by plotting them as green and red dots respectively. All other classes are plotted as smaller blue dots. We see that both VCCA$_{\vx \vi}$ and VCCA$_{\vx \vw}$ manages to separate the apple classes better than VAE$_{\vx}$, where adding the iconic images yields the most clear separation. Next, we want to study the benefits of the text descriptions. We focus on some yoghurt and juice package classes that are visually similar but have very different ingredients and flavors. In Figure \ref{fig:latent_space_visualizations}(g-i), we plot the latent representations for the three models again where the yoghurt and juice classes are plotted in green and yellow colored dots respectively. Here, we see that VCCA$_{\vx \vw}$ manages to separate these items better than VCCA$_{\vx \vw}$ due to the differences in the text descriptions between the selected package classes. 

\begin{figure}[t]
	\centering
	\input{Chapter3/figures/latent_space_visualizations}
	\vspace{-2mm}
	\caption{ Visualizations of the latent representations from VAE$_\vx$, VCCA$_{\vx\vi}$, and VCCA$_{\vx\vw}$ projected in 2-dimensional space with PCA. In (a-c), we show the latent representations plotted using the iconic images of the corresponding object class. In (d-f), we illustrate how the iconic images structures the items based on visual similarities by focusing on the \textcolor{ForestGreen}{green} and \textcolor{RedOrange}{red} apple classes in the dataset plotted in their corresponding colors. Similarly, in (g-i), we show how the text descriptions structure items based on ingredients and flavor by focusing on visually similar yoghurt (\textcolor{ForestGreen}{green}) and juice (\textcolor{GreenYellow}{yellow}) packages. The \textcolor{blue}{blue} dots correspond to all other grocery item classes. } 
	\label{fig:latent_space_visualizations}
	\vspace{-3mm}
\end{figure}

%Conclusions on visualizations: ​

%Iconic image structures items based on visual similarities​

%Text description structures items based on ingredients and flavor

\begin{wrapfigure}{r}{0.44\textwidth}
	\centering
	\vspace{-5mm}
	\begin{tabular}{c c c c}
		\hline \\[-5mm]
		\thead{\scriptsize Natural \\[-2mm] \scriptsize Image} & \thead{\scriptsize Iconic \\[-2mm] \scriptsize Image} & \thead{\scriptsize Decoded \\[-2mm] \scriptsize Image} \\[-1mm]
		%\scriptsize{{\bf Natural Image}} & \scriptsize{{\bf Iconic Image}} & \scriptsize{{\bf Decoded Image}}  \\
		\hline \\[-3mm]
		\includegraphics[width=13mm, height=13mm]{Chapter3/figures/decoded_iconic_images/Orange-Bell-Pepper_008.jpg} & \includegraphics[width=13mm, height=13mm]{Chapter3/figures/iconic_image_figures/Orange-Bell-Pepper_Iconic.jpg} & \includegraphics[width=13mm, height=13mm]{Chapter3/figures/decoded_iconic_images/vcca_xiwy/orange_bell_pepper_image2191.png}  \\[-0.7mm] 
		\hline \\[-3mm]
		\includegraphics[width=13mm, height=13mm]{Chapter3/figures/decoded_iconic_images/Anjou_015.jpg} & \includegraphics[width=13mm, height=13mm]{Chapter3/figures/iconic_image_figures/Anjou-Pear_Clean.jpg} & \includegraphics[width=13mm, height=13mm]{Chapter3/figures/decoded_iconic_images/vcca_xiwy/anjou_pear_image849.png} \\[-0.7mm]
		\hline
	\end{tabular}
	\vspace{-3mm}
	\captionsetup{width=.9\linewidth}
	\caption{Examples of decoded iconic images from VCCA$_{\vx \vi \vw \vy}$ with their corresponding natural image and true iconic image.}
	\vspace{-3mm}
	\label{fig:decoded_iconic_images}
\end{wrapfigure} 
\paragraph{Iconic Image Generation.} The iconic image decoder can provide explanations for the predicted classes. Figure \ref{fig:decoded_iconic_images} shows two examples of decoded iconic images from two natural images where the class labels are \textit{Orange Bell Pepper} and \textit{Anjou Pear}. On the first row, we see that VCCA$_{\vx \vi \vw \vy}$ has recognized the green bell peppers in the natural image and generated a mixed orange and green bell pepper in the iconic image. On the second row, we see that the decoded iconic image is a \textit{Granny Smith} apple instead of a pear which was the true class. The classifier consequently predicts the natural image to be a \textit{Granny Smith} apple. Hence, the iconic image decoder can be used as a tool for providing an intuition of why the classifier made an error. 



\section{Discussion}

In the experiments, we showed that utilizing the web-scraped information with VCCA can enhance the performance of grocery item classifiers. This shows that the cheaper web-scraped views can serve as a good alternative for improving classification performance over collecting more natural images in the grocery stores. Furthermore, we illustrated how the iconic images and text descriptions affects the structure of the latent space based on view-specific information. More specifically, the iconic images structures the latent space after visual similarities such as colors and shapes, while the text descriptions pushes items with similar ingredients and flavors closer to each other in the latent space. Finally, we demonstrated how the iconic images can be used for providing potential explanations for misclassifications, which could help us detect hard classes and give us indications of how robust the latent representations are to classifying images with different classes. 

We observed that utilizing the iconic images in VCCA affects the classification performance significantly better than the text descriptions. Potentially, this is due to the text description view to be more noisy than the iconic images as there are few words that are relevant to the recognition task. However, the text descriptions could be utilized more efficiently, for example by pre-processing the text to keep words describing fine-grained details about the items as well as removing stop words ('the', 'it', 'and', etc.) and other irrelevant words. A second option would be to use attention mechanisms~\cite{luong2015effective, vaswani2017attention} that helps the model to learn which words to emphasize on when learning the joint latent representations. We could also encode the text into a single-vector embedding with various methods~\cite{mikolov2013distributed, pennington2014glove, lan2019albert} and replace the RNN with an MLP predicting in the embedding space which the text embedding the natural image is closes to. 

%% Paper on Text augmentation: https://arxiv.org/abs/1901.11196

Regarding the dataset collection, we have suggested extensions on how to provide more useful information about the items to improve the recognition task. Firstly, it would be valuable to download instances of iconic images and text descriptions from more supermarket websites to potentially allow the VCCA model to capture more view-specific variations into the learned representations. Secondly, the model should be extended to handle video data rather than still images. This would require record videos with the mobile phone camera in the grocery stores to properly evaluate the classifiers. Nevertheless, the extension to video would be beneficial for user experience of the recognition app since the classifier would receive more chances to classify the items correctly by utilizing multiple frames. 

Finally, the Grocery store dataset could be extended to zero/few-shot learning~\cite{xian2018zero, wang2020generalizing} and continual learning~\cite{delange2021continual, parisi2019continual} settings. Such applications are important for assistive vision devices to build data-efficient and adaptable systems that improves their usability in real-world scenarios. 



