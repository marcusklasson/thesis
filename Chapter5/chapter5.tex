
\chapter{Conclusions and Future Directions}\label{chap5}

In this chapter, we summarize and provide conclusions to each included paper in the thesis as well as discuss future work (Section \ref{chap5:sec:conclusions}). Finally, we discuss potential long-term future directions for this project that could be interesting for future researchers to dive into for developing computer vision methods for assisting the visually impaired (Section \ref{chap5:sec:future_directions}).

\section{Conclusions}\label{chap5:sec:conclusions}

We divide the conclusions into two parts where Paper \ref{paperA} and \ref{paperB} are discussed in the first part on fine-grained classification followed by the the second part on continual learning where the conclusions for Paper \ref{paperC} and \ref{paperD} are discussed.

\subsection{Fine-Grained Image Classification of Groceries}


In Paper \ref{paperA}, we presented a challenging fine-grained and hierarchically labeled image classification dataset of grocery items\footnote{Publicly available at \url{https://github.com/marcusklasson/GroceryStoreDataset}.}. All images are taken with a mobile phone camera to simulate the scenario of shopping with an assistive vision app for recognizing groceries. Therefore, the images includes natural variations common in grocery stores such as misplaced items, varying lighting conditions and backgrounds, as well as images with multiple instances of the target class and items from different classes. In addition to the natural images, we also collect external information by web-scraping a supermarket website for an iconic image and a text description of each grocery class. There are several lessons learned from collecting this dataset. First of all, recording videos would have enhanced the number of images in the dataset and potentially eased the collection process. The dataset could then be used for benchmarking on both image and video recognition tasks, where the latter would also be more user-friendly as this increases the chance of capturing frames containing the item. Furthermore, we could have provided more annotations for the items in the natural images. For example, we could have placed bounding boxes over the location of items of the target class as well as provided labels for if the whole item is captured in the image, and whether there are multiple instances of the target class or if there are several different classes present. Moreover, collecting multiple instances of the web-scraped information could be beneficial to increase the chances of extracting the fine-grained details of the grocery items. Finally, it would have been valuable to have blind/low-vision collectors of the images to reduce the gap between the collected data and how the data is later encountered in application use~\cite{theodorou2021disability}. 

In Paper \ref{paperB}, we showed that the web-scraped iconic images and text descriptions in the dataset from Paper \ref{paperA} are useful for enhancing the fine-grained classification performance. We employ the multi-view autoencoder VCCA~\cite{wang2016deep} for learning joint representations between the data views to capture the fine-grained details of the groceries items better by utilizing the web-scraped views. We perform an ablation study over the available data views and find that VCCA structures the latent space differently depending on which views that are used in the model. More specifically, the iconic images structures the items based on visual similarities, such as color and shape, in the latent space, while the text descriptions structures the items based on ingredients and flavors. Using the more web-scraped views yields better fine-grained classification performance compared to approaches that only use the natural images for training the classifier. Hence, the more easily available views can potentially reduce the need for large amounts of natural images to obtain reasonable classification performance of groceries in assistive vision devices. One limitation of VCCA is its inability of learning the view-specific variations when learning from single instances of the web-scraped views, which would help the model to separate the shared information across the utilized views from the private information for each view. A different direction than using autoencoder-based approaches could be to study attention-based methods~\cite{fu2017look, zheng2019looking, luong2015effective} in combination with the natural images and the web-scraped images and text. Furthermore, it would also be interesting to test VCCA on images of the items in different environments than the gropcery stores to check its robustness. Finally, we believe that the dataset would be suitable for zero/few-shot learning and continual learning settings where the model should adapt fast to new classes from potentially few available samples, which would be interesting to investigate for testing the VCCA model in other realistic scenarios. 


\subsection{Replay Scheduling in Continual Learning}

In Paper \ref{paperC} and \ref{paperD}, we focused on a slightly new setting for continual learning (CL) with the aim to bring CL research closer to real-world needs. The main contrast to the traditional CL setting is that historical data is stored rather than deleted which stems well with how major companies handle their incoming data~\cite{bronson2015open, asta2016observability}. However, the model has a budget on how many historical samples that can be used for mitigating catastrophic forgetting when learning from new data due to constraints on the processing and data transmission times. We present this setting in Paper \ref{paperC} and propose scheduling over which tasks to replay at different times to be capable of selecting subsets of historical data from the huge storage. To demonstrate the replay scheduling procedure, we construct a discrete action space of options on how to compose a replay memory with different proportions of the seen tasks. We then select MCTS~\cite{coulom2006efficient, browne2012survey} to enable searching for replay schedules that efficiently reduce catastrophic forgetting for a CL model in large action spaces. The experiments showed that selecting a good replay schedule over the tasks to remember yields better CL performance compared to replaying all tasks equally and could also outperform a heuristic scheduling rule. Furthermore, we showed that recent replay methods~\cite{chaudhry2021using, riemer2018learning, buzzega2020dark} also benefits from the replay schedules selected by MCTS. Finally, we illustrated by letting MCTS select fewer samples for replay than the number of seen classes that replay scheduling can be as efficient as always replaying 1 sample per class. We emphasize that MCTS was used as an exemplar method for demonstrating the importance of learning the memory scheduling in our new CL setting, since MCTS requires multiple rollouts in the action space which is prohibited in CL. Hence, we need a method that learns a general policy for replay scheduling that can be applied across CL domains, which we address in Paper \ref{paperD}.

To the best of our knowledge, there is a lack of efficient methods for applying replay scheduling policies in real-world scenarios. To this end, in Paper \ref{paperD}, we propose a framework based on reinforcement learning~\cite{sutton2018reinforcement} (RL) for learning policies that can mitigate catastrophic forgetting in new CL scenarios. Our framework learns a single policy for selecting which tasks to replay for a CL classifier based on how well the classifier has performed on previous tasks. To foster generalization to new CL scenarios, such as new task orders and datasets, we train the policy with experiences from several CL environments where the policy schedules which tasks to be replayed at different times. We show through experiments on common CL benchmark datasets that the learned policies with our framework can be applied to new CL scenarios and successfully mitigate catastrophic forgetting in the classifiers without any additional computational cost. As we employed Deep Q-Networks~\cite{mnih2013playing}, we suggest for future work to test more RL algorithms, such as policy-gradient methods~\cite{mnih2016asynchronous, schulman2017proximal}, for learning the replay scheduling policy. This also opens up for learning policies with continuous action spaces that could ease the scalability issues from using discrete action spaces in CL datasets with long task horizons. Scaling up the policy to large number of tasks opens up for utilizing replay scheduling approaches in real-world scenarios where a classifier must adapt fast to recognize new objects of interest. As an example, assistive vision devices that are user-personalized could make use of replay scheduling policies learned from other users for mitigating catastrophic forgetting when learning new tasks. However, training the policy to generalize to unseen domains currently requires lots of training from CL environments where each agent-environment query is expensive. Furthermore, RL approaches typically need highly diverse training data for the policy to generalize~\cite{zhang2018dissection, kirk2021survey} as well as hyperparameter tuning that can be costly. Hence, it will be important to enhance the sample-efficiency and adaptation ability of the policy in the future. 

%\begin{itemize}
	%\item Paper A: Presented a challenging fine-grained classification dataset of grocery items
	%\item Paper B: Learning with web-scraped information can reduce the need for real-world images
	%\item Paper C: Replay scheduling is important in our new CL setting that aligns well with real-world needs
	%\item Paper D: Our RL-based method for replay scheduling can be applied to new CL scenarios for mitigating catastrophic forgetting
%\end{itemize}



\section{Future Directions}\label{chap5:sec:future_directions}

In this section, we discuss two future directions that we believe are important to advance assistive vision technologies. First, we suggest future researchers in machine learning and computer vision to work with federated learning~\cite{li2020federated} applied to assistive vision devices to enhance the robustness in their image recognition performance. Secondly, we highlight the importance of involving visually impaired (VI) people throughout research processes to enhance the usefulness of the next generation of assistive vision technologies.  

%\begin{itemize}
%	\item Video data for object recognition instead of images for making systems easier to use. And use a disability-first approach when collecting the data
%	\item Federated Learning for decentralizing model updates 
	%\item Uncertainty Quantification - How to make the classifiers trustworthy?
%\end{itemize}

\subsection{Federated Learning}

In recent years, there has been a growing interest for federated learning~\cite{li2020federated} in the machine learning community. Federated learning involves leveraging the storage and computational capabilities of remote devices, such as mobile phones, for training local models while keeping the data on the device to preserve privacy. 

We give an example scenario in the context of assistive vision where Alice and Bob are grocery shopping in a supermarket using an assistive vision app installed on their personal smartphones. Alice needs to buy milk and wants to recognize a milk package held in her hand. She capture several frames of the milk package by recording a video with the camera that is processed in the app. However, the app responds that this milk package is an unknown item since this type of milk brand is new in the supermarket. Alice asks Charles the clerk what item she is holding, gets help with updating her app with the new milk class from Charles, and purchases the milk. The next day, Bob does his grocery shopping in the same supermarket as Alice. Bob is curious about a new milk package he has never seen before, which is of the same class as the milk Alice bought the day before. Charles app can successfully recognize the correct class of the milk package although this is the first time Bob takes photos of this milk with the app. This is enabled through a central server communicating with all mobile phones with the assistive app installed for learning a global model from local updates from the connected devices in the network. Thus, after the local update of the new milk package in Alice's app has been incorporated, the server transmits the new global model to Bob's app. This is the standard procedure for federated learning which has the potential to utilize the computational resources for predictions on smartphones while maintaining the user experience and preserving privacy. 

There are many interesting challenges that remains to tackle in federated learning. Li \etal~\cite{li2020federated} mentions challenges with statistical heterogeneity in the data from different users, variability of the systems on each device, privacy concerns, and expensive communication rounds between the server and the devices. From the perspective of this thesis, it would be valuable to study the statistical heterogeneity as the generated data can vary immensely across different users in terms of sample frequency, data quality, and context of where the data has been recorded. Here, it may be important to consider issues regarding fairness in addition to accuracy, since the learned model may become biased towards devices with large amounts of data or to commonly occurring groups of devices. Another relevant extension to this thesis would be on systems heterogeneity among different devices, since it will be important to push research and development towards image recognition methods that are accurate, robust, and performs in real-time regardless of the hardware, network connectivity, and power of the mobile devices. This could enable assistive vision apps to be less reliant on the mobile phone type, which in return may yield assistive vision apps that work equally well across different geographical locations and income levels. 



\subsection{Disability-First Approach in Development of Assistive Vision Technologies}

This section is for encouraging future researchers in computer vision and machine learning to have their target user in mind for projects motivated by real-world applications such as assistive technologies. In general, there seems to be consensus between computer vision researchers and the visually impaired (VI) that combining computer vision with assistive technologies have several benefits to ease everyday life for VI people. The past years, there has been an increasing interest of deploying computer vision and machine learning models on mobile devices. Additionally, it is well-known that VI people use mobile phones, including the camera to record events, and want help with object recognition and navigation. Even if the combination is evidently a good fit, researchers should keep VI people involved throughout the process to ensure that the developed technology is requested and, in fact, useful for blind/low-vision people. An example of close collaboration between the two communities is the collection of the recent ORBIT dataset~\cite{massiceti2021orbit, theodorou2021disability} where the data collection was performed by blind/low-vision participants. They take on a \textit{disability-first approach} in the process to produce good quality dataset that should primarily serve the VI community by continuously engaging with and supporting the participants to balance the usefulness versus the effort of data to be collected~\cite{theodorou2021disability}. Involving VI users in the research process should provide benefits on for both the VI users as well as the vision community by bringing insights on how to build more robust image recognizers. 

Aspects on usefulness, fairness, and privacy needs to be considered when developing assistive vision technologies. Regarding usefulness, it will be important to engage with VI people and people from technical disciplines, such as human-computer interaction and user experience designers, to understand what VI people want their app to assist them with and how to build functionality to achieve those goals. For computer vision researchers, we have seen that the models suffer from biases that can disadvantage users based on their gender and geographical location. Such challenges on fairness and various types of distribution shifts will be crucial to tackle for establishing trust in image recognition systems for practitioners. Furthermore, we should strive for developing privacy-preserving systems to avoid unintended leakages of user-sensitive information. However, we also need to take into account the point-of-view of bystanders and whether they are comfortable with being photographed by mobile and wearable devices. This aspect has been studied recently, however, it is still non-trivial how to approach users and ask them to stop using their device. This matter of privacy of assistive vision users and surrounding people should be further discussed with experts on AI ethics in addition to computer vision researchers and VI people. 




%preserve user-sensitive information 
%Potentially, we may be able to mitigate such fairness issues by taking greater care in dataset collections by involving VI people

%there are several challenges on biased datasets and fairness in models that need to be tackled to achieve object recognition that works equally well for everyone. 
 


%has raised the question whether computer vision works equally for everyone.   

%Are models biased, fair, do they work equally well everywhere?

%Privacy concerns for the VI users, privacy for bystanders?

%What is it that the target users want exactly? Collaborate with VI people, experts on HCI and other disciplines 


%but secondly be possible to generalize to applications for serve all people~\cite{theodorou2021disability}. 

%firstly to serve as first serving the  Taking such a disability-first approach

%There exist several recent works great examples of 

%Researchers should take disability-first approaches to conduct research that can be useful for the end user. 

%There are many open problems that are interesting for the computer vision community and can benefit technologies for assisting VI people. 

  

%when collecting datasets and performing model evaluations

%Target users, in our case VI people, should be part of the dataset collection process and test phase to evaluate the developed methods. 

%It will be important in the future to include 

%There are several studies with VI participants

%There are many studies with visually impaired (VI) participants showing that they need help with object recognition in their everyday life~\cite{jayant2011supporting, gurari2018vizwiz}. It is also well-known that VI people use mobile phones cameras to take pictures~\cite{jayant2011supporting} and positive responses from VI participants in experimental studies with mobile apps for navigation and image recognition [Add refs]. Furthermore, the recent advances in computer vision in addition to the interest of deploying machine learning models on mobile devices~\cite{zhang2019deep} makes the combination of assistive technologies and computer vision a great fit. In this section, we want to highlight previous studies regarding the development of computer vision-based assistive technologies and give advice on how research could be conducted in this field. 

%Applying computer vision and machine learning in situations for assisting VI people should be taken with great care. Machine learning works very well for tasks with large amounts of labeled data and where the data distribution is static, i.e., the training and test data are generated from identical distributions. However, in the real world, our surroundings are constantly changing over time and objects can appear in highly varying ways which makes it challenging to deploy machine learning models for daily usage. Several works have shown that deep learning models for computer vision tasks often suffer from biases in the data which 









