
\chapter{Conclusions and Future Directions}\label{chap5}

In this chapter, we provide our conclusions from the works described in this thesis (Section \ref{chap5:sec:conclusions}). Thereafter, we discuss potential future directions for work towards better computer vision methods for assisting visually impaired (VI) people (Section \ref{chap5:sec:future_directions}). 

%In this chapter, we summarize and provide conclusions to each included paper in the thesis as well as discuss future work (Section \ref{chap5:sec:conclusions}). Finally, we discuss potential long-term future directions for this project that could be interesting for future researchers to dive into for developing computer vision methods for assisting the visually impaired (Section \ref{chap5:sec:future_directions}).

\section{Conclusions}\label{chap5:sec:conclusions}

We have divided our conclusions into two parts, where we first provide our insights on the work on fine-grained image recognition (FGIR) of grocery items presented in Chapter \ref{chap3}, and thereafter our insights on our proposed replay scheduling method for continual learning (CL) presented in Chapter \ref{chap4}. 

%We divide the conclusions into two parts where Paper \ref{paperA} and \ref{paperB} are discussed in the first part on fine-grained classification followed by the the second part on continual learning where the conclusions for Paper \ref{paperC} and \ref{paperD} are discussed.

\subsection{Fine-Grained Image Recognition of Groceries}

Here, we provide our insights around the work presented in Chapter \ref{chap3}.

\vspace{-3mm}
\paragraph{Dataset Collection.} In Paper \ref{paperA}, we presented a realistic dataset of images of groceries with a mobile phone to simulate a grocery shopping scenario using an assistive vision app. Collecting these images in the grocery stores was indeed time consuming and required effort. However, as there is a gap between benchmark datasets and reality, we argue that evaluating computer vision methods on real-world data and scenarios is critical for assistive vision. One major learned lesson is that we should have collected the images by recording videos rather than taking snapshot images. First of all, recording videos to classify the groceries would be more user-friendly and probably more accurate than processing static images one at a time. The dataset size would probably be enhanced as well, and the dataset could be used for evaluating both image and video models on supervised classification. In addition to data quality, the selection of network architecture can also have high influence on the final performance. Therefore, we should have performed a broader study over different network types, especially those specialized for mobile computing~\cite{howard2017mobilenets,tan2019efficientnet}, while measuring memory consumption and compute requirements as computation should be on-device to preserve user privacy~\cite{hayes2022online}. It is also important to collect meta-data, such as  hardware information, image resolutions, and geographical location, about the mobile phone images to develop image recognition models that are device-agnostic and robust in new environments. Finally, it would have been valuable to include images from blind/low-vision collectors to measure the FGIR performance with data from the actual users of assistive vision devices~\cite{theodorou2021disability}. 

%\begin{itemize}
	%\item Insight: Collecting real-world data is hard and time consuming (video could have helped here). Run evaluation across several methods and backbones (Mobilenet, Efficientnet) that are suitable for the end-user case, architectures with different memory consumption and inference times matter.  
	%\item Does the collected data represent scenarios that could happen for the end user? Is the dataset challenging enough? How is evaluation performed?
	%\item What annotations or external information can be used to build more robust methods? Meta-data like location of image is relevant since methods shouldn't overfit to specific stores.   
%\end{itemize}

%In Paper \ref{paperA}, we presented a challenging fine-grained and hierarchically labeled image classification dataset of grocery items\footnote{Publicly available at \url{https://github.com/marcusklasson/GroceryStoreDataset}.}. All images are taken with a mobile phone camera to simulate the scenario of shopping with an assistive vision app for recognizing groceries. Therefore, the images includes natural variations common in grocery stores such as misplaced items, varying lighting conditions and backgrounds, as well as images with multiple instances of the target class and items from different classes. In addition to the natural images, we also collect external information by web-scraping a supermarket website for an iconic image and a text description of each grocery class. There are several lessons learned from collecting this dataset. First of all, recording videos would have enhanced the number of images in the dataset and potentially eased the collection process. The dataset could then be used for benchmarking on both image and video recognition tasks, where the latter would also be more user-friendly as this increases the chance of capturing frames containing the item. Furthermore, we could have provided more annotations for the items in the natural images. For example, we could have placed bounding boxes over the location of items of the target class as well as provided labels for if the whole item is captured in the image, and whether there are multiple instances of the target class or if there are several different classes present. Moreover, collecting multiple instances of the web-scraped information could be beneficial to increase the chances of extracting the fine-grained details of the grocery items. Finally, it would have been valuable to have blind/low-vision collectors of the images to reduce the gap between the collected data and how the data is later encountered in application use~\cite{theodorou2021disability}. 

\vspace{-3mm}
\paragraph{Utilizing Web-Scraped Views for FGIR.} In Paper \ref{paperB}, we showed that the web-scraped iconic images and text descriptions in the dataset from Paper \ref{paperA} are useful for enhancing the FGIR performance. Our ablation study showed that the learned latent representations improves the classification performance as the iconic images structures the items based on visual similarities (e.g. color and shape) in the latent space, while the text descriptions structures the items based on ingredients and flavors. To provide more insights on how the web-scraped views boost the performance, we could analyze the natural images with visual explanation methods, e.g., with class activation maps~\cite{zhou2016learning, selvaraju2017grad}, to investigate whether the classifier recognizes the fine-grained details in the items. However, performing such evaluation automatically probably requires human annotations on the location of the details. Adding more examples of iconic images and text descriptions may enable better modeling of the view-specific variations to more accurately capture the shared information about the views into the learned latent representations. In combination with more advanced text processing~\cite{lan2019albert,wei2019eda}, this could potentially enhance the separation between visually similar packaged items with different ingredients to further improve the FGIR performance. Finally, it would be interesting to study how the web-scraped views can be utilized to enhance the data-efficiency in few-shot learning and CL scenarios, which are important applications for assistive vision systems.



%\begin{itemize}
	%\item Insight: web-scraped views can help in boosting FGIR. 
	%\item Better methods for handling the web-scraped views?
	%\item How can we evaluate that the model has learned the fine-grained details in the items? Grad-CAM to evaluate this?
	%\item Study how methods from other applications, such as few-shot learning and online CL, perform on this dataset in other settings than FGIR. Like can the web-scraped views be used somehow to imrpove data-efficiency
%\end{itemize}

%In Paper \ref{paperB}, we showed that the web-scraped iconic images and text descriptions in the dataset from Paper \ref{paperA} are useful for enhancing the fine-grained classification performance. We employ the multi-view autoencoder VCCA~\cite{wang2016deep} for learning joint representations between the data views to capture the fine-grained details of the groceries items better by utilizing the web-scraped views. We perform an ablation study over the available data views and find that VCCA structures the latent space differently depending on which views that are used in the model. More specifically, the iconic images structures the items based on visual similarities, such as color and shape, in the latent space, while the text descriptions structures the items based on ingredients and flavors. Using the more web-scraped views yields better fine-grained classification performance compared to approaches that only use the natural images for training the classifier. Hence, the more easily available views can potentially reduce the need for large amounts of natural images to obtain reasonable classification performance of groceries in assistive vision devices. One limitation of VCCA is its inability of learning the view-specific variations when learning from single instances of the web-scraped views, which would help the model to separate the shared information across the utilized views from the private information for each view. A different direction than using autoencoder-based approaches could be to study attention-based methods~\cite{fu2017look, zheng2019looking, luong2015effective} in combination with the natural images and the web-scraped images and text. Furthermore, it would also be interesting to test VCCA on images of the items in different environments than the gropcery stores to check its robustness. Finally, we believe that the dataset would be suitable for zero/few-shot learning and continual learning settings where the model should adapt fast to new classes from potentially few available samples, which would be interesting to investigate for testing the VCCA model in other realistic scenarios. 


\subsection{Replay Scheduling in Continual Learning}

In this section, we provide our insights around the work presented in Chapter \ref{chap4}.

\vspace{-3mm}
\paragraph{Benefits of Replay Scheduling.} In Paper \ref{paperC}, we proposed a slightly new CL setting where 1) the historical is stored rather than deleted and 2) the replay memory size is limited by processing time constraints instead of storage capacity. This setting stems well with real-world needs as many major companies stores all their recorded data~\cite{bronson2015open,asta2016observability,bailis2017macrobase}. We then showed in an ideal CL environment that scheduling over which tasks to replay can significantly improve the final CL performance compared to using fixed policies, e.g., replaying all tasks equally, especially with small replay memory sizes. Consider a CL system where the number of seen classes is much larger than the number of samples that the system has time to replay. In such scenario, it becomes crucial for the system to have a policy scheduling over which tasks to replay for reducing catastrophic forgetting efficiently. Furthermore, advances in CL are critical for assistive vision devices to maintain recognition of new objects in ever-changing environments without the need for re-training the systems from scratch. As these systems could require on-device learning to maintain privacy, replay scheduling could potentially retain the previous knowledge better than fixed scheduling policies and reduce idle times when learning new abilities. 


%To this end, we proposed learning the time to learn where we select which tasks to replay at different times for mitigating catastrophic forgetting. We then showed in an ideal CL setting that scheduling over which tasks to replay can significantly improve the final CL performance compared to using fixed policies, e.g., replaying all tasks equally. 


%\begin{itemize}
	%\item Insight: Replay scheduling can be critical for the CL performance. 
	%\item advances in CL are critical for assistive vision devices as our surroundings are changing constantly and we don't want to retrain models from scratch
	%\item Focus on the fact that data is cheap to store and that many ML applications has lots of access to it. Mention the limitations on privacy concerns here
	%\item Mention the application of user personalization for VI people perhaps. 
	%\item We also show that fixed policies may not be optimal in certain scenarios, especially when the replay memory size is small. What if we have lots of classes to replay and the memory size is small?
%\end{itemize}

%In Paper \ref{paperC} and \ref{paperD}, we focused on a slightly new setting for continual learning (CL) with the aim to bring CL research closer to real-world needs. The main contrast to the traditional CL setting is that historical data is stored rather than deleted which stems well with how major companies handle their incoming data~\cite{bronson2015open, asta2016observability}. However, the model has a budget on how many historical samples that can be used for mitigating catastrophic forgetting when learning from new data due to constraints on the processing and data transmission times. We present this setting in Paper \ref{paperC} and propose scheduling over which tasks to replay at different times to be capable of selecting subsets of historical data from the huge storage. To demonstrate the replay scheduling procedure, we construct a discrete action space of options on how to compose a replay memory with different proportions of the seen tasks. We then select MCTS~\cite{coulom2006efficient, browne2012survey} to enable searching for replay schedules that efficiently reduce catastrophic forgetting for a CL model in large action spaces. The experiments showed that selecting a good replay schedule over the tasks to remember yields better CL performance compared to replaying all tasks equally and could also outperform a heuristic scheduling rule. Furthermore, we showed that recent replay methods~\cite{chaudhry2021using, riemer2018learning, buzzega2020dark} also benefits from the replay schedules selected by MCTS. Finally, we illustrated by letting MCTS select fewer samples for replay than the number of seen classes that replay scheduling can be as efficient as always replaying 1 sample per class. We emphasize that MCTS was used as an exemplar method for demonstrating the importance of learning the memory scheduling in our new CL setting, since MCTS requires multiple rollouts in the action space which is prohibited in CL. Hence, we need a method that learns a general policy for replay scheduling that can be applied across CL domains, which we address in Paper \ref{paperD}.

\vspace{-3mm}
\paragraph{Learning Replay Scheduling Policies.}

\begin{itemize}
	\item Insight: Our proposed RL-based framework learns replay scheduling policies that can generalize to new CL scenarios.  
	\item How can we make it scale to more tasks? Continuous action space. Improve the sample efficiency with more advanced generalization methods in RL. 
	\item Mention the application of user personalization for VI people again, where we can let new users use the replay scheduling policy to generalize in new environments. 
\end{itemize}

To the best of our knowledge, there is a lack of efficient methods for applying replay scheduling policies in real-world scenarios. To this end, in Paper \ref{paperD}, we propose a framework based on reinforcement learning~\cite{sutton2018reinforcement} (RL) for learning policies that can mitigate catastrophic forgetting in new CL scenarios. Our framework learns a single policy for selecting which tasks to replay for a CL classifier based on how well the classifier has performed on previous tasks. To foster generalization to new CL scenarios, such as new task orders and datasets, we train the policy with experiences from several CL environments where the policy schedules which tasks to be replayed at different times. We show through experiments on common CL benchmark datasets that the learned policies with our framework can be applied to new CL scenarios and successfully mitigate catastrophic forgetting in the classifiers without any additional computational cost. As we employed Deep Q-Networks~\cite{mnih2013playing}, we suggest for future work to test more RL algorithms, such as policy-gradient methods~\cite{mnih2016asynchronous, schulman2017proximal}, for learning the replay scheduling policy. This also opens up for learning policies with continuous action spaces that could ease the scalability issues from using discrete action spaces in CL datasets with long task horizons. Scaling up the policy to large number of tasks opens up for utilizing replay scheduling approaches in real-world scenarios where a classifier must adapt fast to recognize new objects of interest. As an example, assistive vision devices that are user-personalized could make use of replay scheduling policies learned from other users for mitigating catastrophic forgetting when learning new tasks. However, training the policy to generalize to unseen domains currently requires lots of training from CL environments where each agent-environment query is expensive. Furthermore, RL approaches typically need highly diverse training data for the policy to generalize~\cite{zhang2018dissection, kirk2021survey} as well as hyperparameter tuning that can be costly. Hence, it will be important to enhance the sample-efficiency and adaptation ability of the policy in the future. 

%\begin{itemize}
	%\item Paper A: Presented a challenging fine-grained classification dataset of grocery items
	%\item Paper B: Learning with web-scraped information can reduce the need for real-world images
	%\item Paper C: Replay scheduling is important in our new CL setting that aligns well with real-world needs
	%\item Paper D: Our RL-based method for replay scheduling can be applied to new CL scenarios for mitigating catastrophic forgetting
%\end{itemize}



\section{Future Directions}\label{chap5:sec:future_directions}

In this section, we discuss two future directions that we believe are important to advance assistive vision technologies. First, we suggest future researchers in machine learning and computer vision to work with federated learning~\cite{li2020federated} applied to assistive vision devices to enhance the robustness in their image recognition performance. Secondly, we highlight the importance of involving visually impaired (VI) people throughout research processes to enhance the usefulness of the next generation of assistive vision technologies.  

%\begin{itemize}
%	\item Video data for object recognition instead of images for making systems easier to use. And use a disability-first approach when collecting the data
%	\item Federated Learning for decentralizing model updates 
	%\item Uncertainty Quantification - How to make the classifiers trustworthy?
%\end{itemize}

\subsection{Federated Learning}

In recent years, there has been a growing interest for federated learning~\cite{konevcny2015federated, mcmahan2017communication, li2020federated} in the machine learning community. Federated learning involves leveraging the storage and computational capabilities of remote devices, such as mobile phones, for training local models while keeping the data on the device to preserve privacy. 

\vspace{-3mm}
\paragraph{Example Scenario.} We give an example scenario in the context of assistive vision where Alice and Bob are grocery shopping in a supermarket using an assistive vision app installed on their personal smartphones. Alice needs to buy milk and wants to recognize a milk package held in her hand. She capture several frames of the milk package by recording a video with the camera that is processed in the app. However, the app responds that this milk package is an unknown item since this type of milk brand is new in the supermarket. Alice asks Charles the clerk what item she is holding, gets help with updating her app with the new milk class from Charles, and purchases the milk. The next day, Bob does his grocery shopping in the same supermarket as Alice. Bob is curious about a new milk package he has never seen before, which is of the same class as the milk Alice bought the day before. Charles app can successfully recognize the correct class of the milk package although this is the first time Bob takes photos of this milk with the app. This is enabled through a central server communicating with all mobile phones with the assistive app installed for learning a global model from local updates from the connected devices in the network. Thus, after the local update of the new milk package in Alice's app has been incorporated, the server transmits the new global model to Bob's app. This is the standard procedure for federated learning which has the potential to utilize the computational resources for predictions on smartphones while maintaining the user experience and preserving privacy. 

There are many interesting challenges that remains to tackle in federated learning. Li \etal~\cite{li2020federated} mentions challenges with statistical heterogeneity in the data from different users~\cite{smith2017federated, zhao2018federated, li2020federated}, variability of the systems on each device~\cite{bonawitz2019towards}, privacy concerns~\cite{geyer2017differentially, bhowmick2018protection}, and expensive communication rounds between the server and the devices~\cite{konevcny2016federated}. From the perspective of this thesis, it would be valuable to study the statistical heterogeneity as the generated data can vary immensely across different users in terms of sample frequency, data quality, and context of where the data has been recorded. Here, it may be important to consider issues regarding fairness in addition to accuracy, since the learned model may become biased towards devices with large amounts of data or to commonly occurring groups of devices. Another relevant extension to this thesis would be on systems heterogeneity among different devices, since it will be important to push research and development towards image recognition methods that are accurate, robust, and performs in real-time regardless of the hardware, network connectivity, and power of the mobile devices. This could enable assistive vision apps to be less reliant on the mobile phone type, which in return may yield assistive vision apps that work equally well across different geographical locations and income levels. 



\subsection{Disability-First Approach in Development of Assistive Vision Technologies}

This section is for encouraging future researchers in computer vision and machine learning to have their target user in mind for projects motivated by real-world applications such as assistive technologies.
There exist many studies on computer vision-based assistive technologies describing the benefits and challenges to ease everyday life for VI people~\cite{jayant2011supporting, tian2013toward, terven2013new, leo2017computer, tapu2020wearable, wang2019implications}.  
%In general, there seems to be consensus between computer vision researchers and the visually impaired (VI) that combining computer vision with assistive technologies have several benefits to ease everyday life for VI people~\cite{bibid}. 
The past years, there has been an increasing interest of deploying computer vision and machine learning models on mobile devices~\cite{zhang2019deep, lim2020federated}. Additionally, it is well-known that VI people use mobile phones, including the camera to record events, and want help with object recognition and navigation~\cite{jayant2011supporting, vazquez2012helping, szpiro2016finding, gurari2018vizwiz}. 
Even if the combination is evidently a good fit, many of the mentioned works stress that researchers should keep VI people involved throughout the process to ensure that the developed technology is requested and, in fact, useful for blind/low-vision people. 
An example of close collaboration between the two communities is the collection of the recent ORBIT dataset~\cite{massiceti2021orbit, theodorou2021disability} where the data collection was performed by blind/low-vision participants. They take on a \textit{disability-first approach} in the process to produce good quality dataset that should primarily serve the VI community by continuously engaging with and supporting the participants to balance the usefulness versus the effort of data to be collected~\cite{theodorou2021disability}. Involving VI users in the research process should provide benefits on for both the VI users as well as the vision community by bringing insights on how to build more robust image recognizers. 

Aspects on usefulness, fairness, and privacy needs to be considered when developing assistive vision technologies. Regarding usefulness, it will be important to engage with VI people and people from technical disciplines, such as human-computer interaction and user experience designers, to understand what VI people want their app to assist them with and how to build functionality to achieve those goals. For computer vision researchers, we have seen that the models suffer from biases that can disadvantage users based on their gender~\cite{bolukbasi2016man, buolamwini2018gender, burns2018women}, geographical location~\cite{de2019does}, and co-occurrences between objects and surrounding contexts~\cite{singh2020don}. Such challenges on fairness~\cite{barocas2019fairml, holstein2019improving, yang2020towards, goyal2022fairness} and various types of distribution shifts~\cite{quionero2009dataset, wiles2021fine} will be crucial to tackle for establishing trust in image recognition systems for practitioners. Furthermore, we should strive for developing privacy-preserving systems to avoid unintended leakages of user-sensitive information~\cite{ahmed2015privacy, ahmed2017understanding, gurari2019vizwiz, akter2020uncomfortable}. However, we also need to take into account the point-of-view of bystanders and whether they are comfortable with being photographed by mobile and wearable devices. This aspect has been studied recently~\cite{lee2020pedestrian, ahmed2018up, akter2021shared}, however, it is still non-trivial how to approach users and ask them to stop using their device. This matter of privacy and social acceptance between assistive vision users and surrounding people needs to be further studied in realistic environments for the purpose of understanding the needs of those who will be involved in the usage of the technology. 






%preserve user-sensitive information 
%Potentially, we may be able to mitigate such fairness issues by taking greater care in dataset collections by involving VI people

%there are several challenges on biased datasets and fairness in models that need to be tackled to achieve object recognition that works equally well for everyone. 
 


%has raised the question whether computer vision works equally for everyone.   

%Are models biased, fair, do they work equally well everywhere?

%Privacy concerns for the VI users, privacy for bystanders?

%What is it that the target users want exactly? Collaborate with VI people, experts on HCI and other disciplines 


%but secondly be possible to generalize to applications for serve all people~\cite{theodorou2021disability}. 

%firstly to serve as first serving the  Taking such a disability-first approach

%There exist several recent works great examples of 

%Researchers should take disability-first approaches to conduct research that can be useful for the end user. 

%There are many open problems that are interesting for the computer vision community and can benefit technologies for assisting VI people. 

  

%when collecting datasets and performing model evaluations

%Target users, in our case VI people, should be part of the dataset collection process and test phase to evaluate the developed methods. 

%It will be important in the future to include 

%There are several studies with VI participants

%There are many studies with visually impaired (VI) participants showing that they need help with object recognition in their everyday life~\cite{jayant2011supporting, gurari2018vizwiz}. It is also well-known that VI people use mobile phones cameras to take pictures~\cite{jayant2011supporting} and positive responses from VI participants in experimental studies with mobile apps for navigation and image recognition [Add refs]. Furthermore, the recent advances in computer vision in addition to the interest of deploying machine learning models on mobile devices~\cite{zhang2019deep} makes the combination of assistive technologies and computer vision a great fit. In this section, we want to highlight previous studies regarding the development of computer vision-based assistive technologies and give advice on how research could be conducted in this field. 

%Applying computer vision and machine learning in situations for assisting VI people should be taken with great care. Machine learning works very well for tasks with large amounts of labeled data and where the data distribution is static, i.e., the training and test data are generated from identical distributions. However, in the real world, our surroundings are constantly changing over time and objects can appear in highly varying ways which makes it challenging to deploy machine learning models for daily usage. Several works have shown that deep learning models for computer vision tasks often suffer from biases in the data which 









