
\chapter{Conclusions and Future Directions}\label{chap5}

In this chapter, we summarize and provide conclusions to each included paper in the thesis as well as discuss future work (Section \ref{chap5:sec:conclusions}). Finally, we discuss potential long-term future directions for this project that could be interesting for future researchers to dive into for developing computer vision methods for assisting the visually impaired (Section \ref{chap5:sec:future_directions}).

\section{Conclusions}\label{chap5:sec:conclusions}

We divide the conclusions into two parts where Paper \ref{paperA} and \ref{paperB} are discussed in the first part on fine-grained classification followed by the the second part on continual learning where the conclusions for Paper \ref{paperC} and \ref{paperD} are discussed.

\subsection{Fine-Grained Image Classification of Groceries}


In Paper \ref{paperA}, we presented a challenging fine-grained and hierarchically labeled image classification dataset of grocery items\footnote{Publicly available at \url{https://github.com/marcusklasson/GroceryStoreDataset}.}. All images are taken with a mobile phone camera to simulate the scenario of shopping with an assistive vision app for recognizing groceries. Therefore, the images includes natural variations common in grocery stores such as misplaced items, varying lighting conditions and backgrounds, as well as images with multiple instances of the target class and items from different classes. In addition to the natural images, we also collect external information by web-scraping a supermarket website for an iconic image and a text description of each grocery class. There are several lessons learned from collecting this dataset. First of all, recording videos would have enhanced the number of images in the dataset and potentially eased the collection process. The dataset could then be used for benchmarking on both image and video recognition tasks, where the latter would also be more user-friendly as this increases the chance of capturing frames containing the item. Furthermore, we could have provided more annotations for the items in the natural images. For example, we could have placed bounding boxes over the location of items of the target class as well as provided labels for if the whole item is captured in the image, and whether there are multiple instances of the target class or if there are several different classes present. Moreover, collecting multiple instances of the web-scraped information could be beneficial to increase the chances of extracting the fine-grained details of the grocery items. Finally, it would have been valuable to have blind/low-vision collectors of the images to reduce the gap between the collected data and how the data is later encountered in application use~\cite{theodorou2021disability}. 

In Paper \ref{paperB}, we showed that the web-scraped iconic images and text descriptions in the dataset from Paper \ref{paperA} are useful for enhancing the fine-grained classification performance. We employ the multi-view autoencoder VCCA~\cite{wang2016deep} for learning joint representations between the data views to capture the fine-grained details of the groceries items better by utilizing the web-scraped views. We perform an ablation study over the available data views and find that VCCA structures the latent space differently depending on which views that are used in the model. More specifically, the iconic images structures the items based on visual similarities, such as color and shape, in the latent space, while the text descriptions structures the items based on ingredients and flavors. Using the more web-scraped views yields better fine-grained classification performance compared to approaches that only use the natural images for training the classifier. Hence, the more easily available views can potentially reduce the need for large amounts of natural images to obtain reasonable classification performance of groceries in assistive vision devices. One limitation of VCCA is its inability of learning the view-specific variations when learning from single instances of the web-scraped views, which would help the model to separate the shared information across the utilized views from the private information for each view. A different direction than using autoencoder-based approaches could be to study attention-based methods~\cite{fu2017look, zheng2019looking, luong2015effective} in combination with the natural images and the web-scraped images and text. Furthermore, it would also be interesting to test VCCA on images of the items in different environments than the gropcery stores to check its robustness. Finally, we believe that the dataset would be suitable for zero/few-shot learning and continual learning settings where the model should adapt fast to new classes from potentially few available samples, which would be interesting to investigate for testing the VCCA model in other realistic scenarios. 


\subsection{Replay Scheduling in Continual Learning}

In Paper \ref{paperC} and \ref{paperD}, we focused on a slightly new setting for continual learning (CL) with the aim to bring CL research closer to real-world needs. The main contrast to the traditional CL setting is that historical data is stored rather than deleted which stems well with how major companies handle their incoming data~\cite{bronson2015open, asta2016observability}. However, the model has a budget on how many historical samples that can be used for mitigating catastrophic forgetting when learning from new data due to constraints on the processing and data transmission times. We present this setting in Paper \ref{paperC} and propose scheduling over which tasks to replay at different times to be capable of selecting subsets of historical data from the huge storage. To demonstrate the replay scheduling procedure, we construct a discrete action space of options on how to compose a replay memory with different proportions of the seen tasks. We then select MCTS~\cite{coulom2006efficient, browne2012survey} to enable searching for replay schedules that efficiently reduce catastrophic forgetting for a CL model in large action spaces. The experiments showed that selecting a good replay schedule over the tasks to remember yields better CL performance compared to replaying all tasks equally and could also outperform a heuristic scheduling rule. Furthermore, we showed that recent replay methods~\cite{chaudhry2021using, riemer2018learning, buzzega2020dark} also benefits from the replay schedules selected by MCTS. Finally, we illustrated by letting MCTS select fewer samples for replay than the number of seen classes that replay scheduling can be as efficient as always replaying 1 sample per class. We emphasize that MCTS was used as an exemplar method for demonstrating the importance of learning the memory scheduling in our new CL setting, since MCTS requires multiple rollouts in the action space which is prohibited in CL. Hence, we need a method that learns a general policy for replay scheduling that can be applied across CL domains, which we address in Paper \ref{paperD}.

To the best of our knowledge, there is a lack of efficient methods for applying replay scheduling policies in real-world scenarios. To this end, in Paper \ref{paperD}, we propose a framework based on reinforcement learning~\cite{sutton2018reinforcement} (RL) for learning policies that can mitigate catastrophic forgetting in new CL scenarios. Our framework learns a single policy for selecting which tasks to replay for a CL classifier based on how well the classifier has performed on previous tasks. To foster generalization to new CL scenarios, such as new task orders and datasets, we train the policy with experiences from several CL environments where the policy schedules which tasks to be replayed at different times. We show through experiments on common CL benchmark datasets that the learned policies with our framework can be applied to new CL scenarios and successfully mitigate catastrophic forgetting in the classifiers without any additional computational cost. As we employed Deep Q-Networks~\cite{mnih2013playing}, we suggest for future work to test more RL algorithms, such as policy-gradient methods~\cite{mnih2016asynchronous, schulman2017proximal}, for learning the replay scheduling policy. This also opens up for learning policies with continuous action spaces that could ease the scalability issues from using discrete action spaces in CL datasets with long task horizons. Scaling up the policy to large number of tasks opens up for utilizing replay scheduling approaches in real-world scenarios where a classifier must adapt fast to recognize new objects of interest. As an example, assistive vision devices that are user-personalized could make use of replay scheduling policies learned from other users for mitigating catastrophic forgetting when learning new tasks. However, training the policy to generalize to unseen domains currently requires lots of training from CL environments where each agent-environment query is expensive. Furthermore, RL approaches typically need highly diverse training data for the policy to generalize~\cite{zhang2018dissection, kirk2021survey} as well as hyperparameter tuning that can be costly. Hence, it will be important to enhance the sample-efficiency and adaptation ability of the policy in the future. 

%\begin{itemize}
	%\item Paper A: Presented a challenging fine-grained classification dataset of grocery items
	%\item Paper B: Learning with web-scraped information can reduce the need for real-world images
	%\item Paper C: Replay scheduling is important in our new CL setting that aligns well with real-world needs
	%\item Paper D: Our RL-based method for replay scheduling can be applied to new CL scenarios for mitigating catastrophic forgetting
%\end{itemize}



\section{Future Directions}\label{chap5:sec:future_directions}

In this section, we discuss two future directions that we believe are important to advance assistive vision technologies. First, we discuss the importance of having a disability-first mindset when conducting research towards useful technology for assisting the visually impaired~\cite{theodorou2021disability}. Secondly, we suggest future researchers in machine learning and computer vision to work with federated learning~\cite{li2020federated} applied to assistive vision devices to enhance the robustness in their image recognition performance. 

%\begin{itemize}
%	\item Video data for object recognition instead of images for making systems easier to use. And use a disability-first approach when collecting the data
%	\item Federated Learning for decentralizing model updates 
	%\item Uncertainty Quantification - How to make the classifiers trustworthy?
%\end{itemize}


\subsection{Disability-First Approach in Development of Assistive Vision Technologies}

There are many studies with visually impaired (VI) participants showing that they need help with object recognition in their everyday life~\cite{jayant2011supporting, gurari2018vizwiz}. It is also well-known that VI people use mobile phones cameras to take pictures~\cite{jayant2011supporting} and positive responses from VI participants in experimental studies with mobile apps for navigation and image recognition [Add refs]. Furthermore, the recent advances in computer vision in addition to the interest of deploying machine learning models on mobile devices [Add Refs] makes the combination of assistive technologies and computer vision a great fit. In this section, we want to highlight previous studies regarding the development of computer vision-based assistive technologies and give advice on how research could be conducted in this field. 



Are models biased, fair, do they work equally well everywhere?

Privacy concerns for the VI users, privacy for bystanders?


\subsection{Federated Learning}




