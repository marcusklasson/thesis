


%\renewcommand{\thechapter}{A}  % use A, B, C for chapter numbers
%\renewcommand{\chaptername}{Paper} % A chapter is now called Paper
%\renewcommand\thesection{\arabic{section}}
%\renewcommand*{\thepage}{A\arabic{page}}
%\setcounter{page}{1}
%\setcounter{section}{0}

%%% Paper content

\chapter{
	\centering{A Hierarchical Grocery Store Image Dataset with Visual and Semantic Labels}
}\label{chap:paperA}
\chaptermark{Hierarchical Grocery Store Image Dataset}
\begin{comment}
\vspace{-1cm}
\large{\textbf{Marcus Klasson}} \\
\large{\textit{Robotics, Perception, and Learning, EECS}} \\
\large{\textit{KTH Royal Institute of Technology, Stockholm, Sweden}} \\

\noindent\large{\textbf{Cheng Zhang}} \\
\large{\textit{Microsoft Research}} \\
\large{\textit{Cambridge, United Kingdom}} \\

\noindent\large{\textbf{Hedvig Kjellstr\"{o}m}} \\
\large{\textit{Robotics, Perception, and Learning, EECS}} \\
\large{\textit{KTH Royal Institute of Technology, Stockholm, Sweden}} 
\end{comment}
\vspace{-5mm}
\begin{center}
\large{\textbf{Marcus Klasson$^{*}$, Cheng Zhang$^{\dagger}$, Hedvig Kjellstr√∂m$^{*}$}} \\[2mm]
\small{$^{*}$KTH Royal Institute of Technology, Stockholm, Sweden} \\
\small{$^{\dagger}$Microsoft Research, Cambridge, United Kingdom} \\
\end{center}
%\vspace{5pt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
	\noindent Image classification models built into visual support systems and other assistive devices need to provide accurate predictions about their environment. We focus on an application of assistive technology for people with visual impairments, for daily activities such as shopping or cooking. In this paper, we provide a new benchmark dataset for a challenging task in this application -- classification of fruits, vegetables, and refrigerated products, e.g. milk packages and juice cartons, in grocery stores. To enable the learning process to utilize multiple sources of structured information, this dataset not only contains a large volume of natural images but also includes the corresponding information of the product from an online shopping website. Such information encompasses the hierarchical structure of the object classes, as well as an iconic image of each type of object. This dataset can be used to train and evaluate image classification models for helping visually impaired people in natural environments. Additionally, we provide benchmark results evaluated on pretrained convolutional neural networks often used for image understanding purposes, and also a multi-view variational autoencoder, which is capable of utilizing the rich product information in the dataset.
\end{abstract}

%%% Contents
\input{PaperA/sections_test/introduction}
\input{PaperA/sections_test/related-work}
\input{PaperA/sections_test/our-dataset}
\input{PaperA/sections_test/classification-methods}
\input{PaperA/sections_test/experimental-results}
\input{PaperA/sections_test/conclusions}

\renewcommand*{\bibname}{References}
\bibliographystyleA{unsrt}
\bibliographyA{References/paperA_test}
