
\section{Conclusions}\label{paperD:sec:conclusions}

In this paper, we introduce an an RL-based framework for learning the time to learn in a CL context. Building on the replay scheduling idea from~\citeD{D:klasson2021learn}, we are the first to propose a method for learning policies that schedules which tasks to replay at different times to improve memory retention of historical tasks. We showed that the framework learns replay scheduling policies that can generalize across different CL environments with unseen task orders and datasets without additional computational cost or training in the test environment, which would be useful in user personalization applications. Moreover, the learned policies are capable of considering replaying forgotten tasks which can mitigate catastrophic forgetting more efficiently than fixed scheduling policies. Our replay scheduling policy approach brings current research closer to tackling real-world CL challenges where the number of tasks exceeds the memory size.


\vspace{-3mm}
\paragraph{Limitations and Future Work.} 
Generalization in RL is a challenging research topic by itself. With the current method, large amounts of diverse data and training time is required to enable the learned policy to generalize well. This can be costly due to generating the CL environments is expensive since each state transition involves training the classifier on a CL task. 
Moreover, we mainly focused on a discrete action space which is hard to construct especially when the number of tasks is large. Thus, in future work, we would explore more advanced RL methods which can handle continuous action spaces and generalize well.  

