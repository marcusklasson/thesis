
\section{Conclusions}\label{paperD:sec:conclusions}

We proposed a novel method for learning policies for replay scheduling in CL which previously have been unavailable. The goal of our method is to learn a general policy that can be applied to any CL scenario for replay scheduling without requiring additional training costs at test time. We used RL for learning the policy where we employed DQNs for selecting which tasks to replay at different times. In the experiments, we showed that the replay schedules from the learned policy perform similarly to carefully tuned heuristic scheduling baselines. Furthermore, we gained more insights by visualizing the learned replay schedules and observed that the policy is capable of learning scheduling behaviors similar to human education techniques such as spaced repetition. In future work, we will investigate how alternative RL algorithms than DQN are capable of learning policies in this setting as well as experimenting with continuous action spaces to enable scaling to longer task-horizons. We hope that our work can encourage more research within this CL setting where the limitations are on compute rather than memory storage which stems well with the needs in real-world CL scenarios.  

