
\section{Conclusions}\label{paperD:sec:conclusions}

In this paper, we introduce an an RL-based framework for learning the time to learn in a CL context. Building on the replay scheduling idea from~\citeD{D:klasson2021learn}, we are the first to propose a method for learning policies that schedules which tasks to replay at different times to improve memory retention of historical tasks. We showed that the framework learns replay scheduling policies that can generalize across different CL environments with unseen task orders and datasets without additional computational cost or training in the test environment. Moreover, the learned policies are capable of considering replaying forgotten tasks which can mitigate catastrophic forgetting more efficiently than fixed scheduling policies. The proposed policy learning framework opens up for new research directions in replay-based CL and memory scheduling, especially in real-world scenarios where the amount of available data can exceed the replay memory capacity and processing time constraints.   

\vspace{-3mm}
\paragraph{Limitations and Future Work.} 
Generalization in RL is a challenging research topic by itself. With the current method, large amounts of diverse data and training time is required to enable the learned policy to generalize well. This can be costly due to generating the CL environments is expensive since each state transition involves training the classifier on a CL task. 
Moreover, we are currently considering a discrete action space which is hard to construct especially when the number of tasks is large. Thus, in future work, we would explore more advanced RL methods which can handle continuous action spaces and generalize well.  

