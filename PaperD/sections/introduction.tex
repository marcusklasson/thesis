
\section{Introduction}\label{paperD:sec:introduction}

Scheduling over which historical tasks to replay has been shown to mitigate catastrophic forgetting efficiently in continual learning (CL)~\citeD{D:aljundi2019online, D:klasson2021learn}. 
Such replay strategies are important in real-world applications with machine learning systems that needs to learn continuously from streaming data, wherein the incoming data is stored rather than deleted.
In such scenarios, re-training on all stored data is often prohibited due to limitations on computational resources and data transmission times. Therefore, we need a policy that schedules from immense amounts of recorded data which previously learned abilities to replay when learning new tasks. 

Although memory scheduling is essential for real-world CL, an efficient policy that schedules the time to replay which tasks is currently missing from the literature. For example, the policy in~\citeD{D:klasson2021learn} is searched after using multiple CL episodes which is prohibited in the CL setting. Moreover, the method in~\citeD{D:aljundi2019online} performs forward passes on all stored data for selecting the tasks to replay, which becomes challenging in scenarios where the amounts of historical data is huge. 
If we could learn the scheduling policy, we would want the possibility to transfer the policy to new CL scenarios without the need for additional training in the target domain. Ideally, the policy should be domain-agnostic such that it is capable of generalizing to any CL dataset unseen during training. Such policy could potentially enable replay methods to efficiently mitigate catastrophic forgetting in real-world CL applications where the number of seen classes exceeds the replay memory budget. 

In this paper, we propose a framework based on reinforcement learning (RL)~\citeD{D:sutton2018reinforcement} for learning a general replay scheduling policy that selects which tasks to replay at different times. We focus on the CL setting introduced in~\citeD{D:klasson2021learn} which simulates a scenario where a replay memory is sampled from a large historical data storage for mitigating catastrophic forgetting. The policy is learned from multiple episodes in a CL environment by selecting which tasks to replay to efficiently mitigate catastrophic forgetting in the CL network. 
Our goal is to learn a replay scheduling policy that generalizes to new CL scenarios without added computational cost. 
We enable the possibility for generalizing to new domains by using the intuition that the policy should replay tasks that are to be forgotten by the classifier. Hence, we provide the policy with the evaluated task performances of the classifier in the CL environment, such that the policy can select the tasks to efficiently retain overall CL performance.  
In summary, our contributions are:
\begin{itemize}[topsep=1pt,] %noitemsep]
	\item We take the first steps to enable replay scheduling in real-world CL scenarios by proposing an RL-based framework for learning policies that generalize across different CL environments (Section \ref{paperD:sec:methodology}). 
	\item We show that the learned policies can efficiently mitigate catastrophic forgetting in CL scenarios with new task orders and datasets unseen during training without added computational cost (Section \ref{paperD:sec:results_on_policy_generalization}).
\end{itemize}
