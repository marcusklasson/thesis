
\section{Introduction}\label{paperD:sec:introduction}

The timing when to rehearse on previously learned tasks is important for memory retention in human continual learning (CL)~\citeD{D:dempster1989spacing, D:ebbinghaus2013memory, D:landauer1978optimum}, however, there currently exist no such method for CL in neural networks. Memory-based CL~\citeD{D:delange2021continual, D:parisi2019continual} have been shown to efficiently retain knowledge of previously learned tasks by rehearsing the network on samples from historical tasks stored in a limited replay memory. Most methods simply apply random sampling for retrieving mini-batches from the replay memory since more elaborate retrieval strategies have been shown to yield small performance improvements~\citeD{D:chaudhry2019tiny, D:hayes2020remind}. Recently, Klasson \etal~\citeD{D:klasson2021learn} showed that replay scheduling over which tasks to replay at different time steps can be critical for the final CL performance. However, a general replay scheduling policy that can be applied in any CL scenario is currently missing, which makes it infeasible to apply in real-world scenarios.

%The timing of rehearsing on various tasks is important for learning in human continual learning. Such a learning technique that considers time is spaced repetition where increasing the time interval between rehearsal has been shown to improve memory retention~\citep{ebbinghaus2013memory, dempster1989spacing}. In continual learning in neural networks~\citep{delange2021continual, parisi2019continual}, replay methods have been shown to efficiently retain knowledge of previously learned tasks by rehearsing the network on samples from historical tasks stored in a limited memory. Using scheduling methods over when to replay is ignored in most replay methods, even if it is known that it can be critical for the final continual learning performance~\citep{klasson2021learn}. However, a general replay scheduling policy that can be applied in any continual learning scenario is absent at this time, which makes it infeasible in real-world scenarios.       

Assuming we have a policy for replay scheduling, we would like the policy to have the property of being transferable to any CL scenario. Ideally, the policy should be capable of selecting the tasks to replay in new CL environments without additional training with experiences from the environment. This would be useful in real-world applications where a machine learning system is continuously updated from streaming data and there are vast amounts of historical data accessible, e.g., from the cloud, that can be used for mitigating catastrophic forgetting.
In such situations, the system would likely have constraints on the processing time due to operational costs prohibiting training on all the available historical data. Therefore, replay scheduling is important for selecting what historical data to replay for obtaining the best overall performance when new tasks are learned. In this paper, we present a novel method for learning a general replay scheduling policy that satisfies the above mentioned properties to move CL research closer to real-world needs.


%Assuming we have a replay scheduling policy, a desired property of such policy would be that it transferable to any continual learning scenario. Our intuition of the timing to replay is that known tasks need to be rehearsed when they have been or are about to be forgotten. Building a scheduling system with these intuitions would yield a general policy for replay scheduling that could be deployed in any continual learning scenario. Foreseeing which tasks that will eventually be forgotten is unfortunately difficult in continual learning settings, especially without data from future tasks. However, measuring the current performance for the model on every seen task is possible if we are allowed to store old data for validation. In this paper, we focus on using the current task performances for learning a general policy for selecting which tasks the model should replay at different time steps.      
%The desired property of a method for learning the time to learn is a policy that can be transferable.

%Knowing the optimal time to update a machine learning model for improving its performance is a challenging task. In many real world scenarios, the model operates on streaming data and has to continually adapt to unseen data points through its lifetime. Rehearsing the model on old tasks to mitigate catastrophic forgetting~\citep{mccloskey1989catastrophic} of already learned knowledge then becomes vital for the model. However, it is often very costly or impossible to retrain the model on all seen data due to processing time constraints. Replay methods in continual learning~\citep{delange2021continual, parisi2019continual} is an efficient method for retaining knowledge in neural networks with the assumption that the replay memory is limited due to storage. Furthermore, the replay memory must have capacity to store at least one example per task, which can be very challenging in applications where the number of tasks exceeds the replay memory capacity. In such scenarios, learning the time to replay different tasks becomes important, but currently there exists no method or answer on how to achieve this.
%Learning the time to learn is important, however there exists no method for doing this.  
%A previous work showed that learning the time to learn which task to replay is important in continual learning settings.

%The desired property of such policy would be that the policy can generalize to new continual learning scenarios. We can guess that the policy decides to replay the tasks that the classifier has difficulties predicting accurately. The data arrives in a stream all the time so the idea of replaying tasks that are about to be forgotten should apply in all kinds of scenarios, which should make it unnecessary to retrain the policy for every new scenario. The goal is therefore to have a general policy that can be deployed in any continual learning scenario without the need for training samples for finetuning to the new environment. Hence, we need to define the state of how well the classifier is performing on every task in a general way that can be used in any continual learning scenario.    
%The desired property of a method for learning the time to learn is a policy that can be transferable.
%In the memory-based CL literature, learning the time to learn has been overlooked.

We propose using reinforcement learning (RL)~\citeD{D:sutton2018reinforcement} for learning a general policy for replay scheduling that can be applied to any new CL scenario. We focus on the setting for memory-based CL proposed in Klasson \etal~\citeD{D:klasson2021learn} where limitations are on the processing time of memory samples rather than the memory storage capacity. We take inspiration from meta-learning~\citeD{D:hospedales2020meta} and learn the policy from experiences from multiple episodes of CL procedures. The RL agent take actions within the CL procedures where each action represents which tasks to replay for the CL model, where the actions are taken based on the current performance on the seen tasks. Thus, the goal for the agent is to learn the time when the CL model should replay different tasks and by what proportion to use of memory samples to fill the replay memory with.
To foster generalization between new CL scenarios, we let the agent interact with several different CL models with various variations on the task datasets they should learn from. Finally, we evaluate the obtained policy by applying it to new CL scenarios without adding any computational by training on experiences from the test environments. Our contributions are the following:
\begin{itemize}[topsep=1pt,] %noitemsep]
    \item We propose a novel method based on RL for learning a policy for replay scheduling in CL settings. The learned policy can then be transferred to any new CL scenario without any additional training.
    \item We evaluate the generalization capabilities of the learned policy on unseen CL scenarios where we have varied the task splits in the datasets as well as introducing unseen datasets for testing the policy. 
    %\item We propose a novel method for learning the time to learn in a continual learning setting that is transferable to new continual learning environments.
    %\item We evaluate the generalization capabilities of the learned policies on environments with different task splits as well as unseen datasets during training. 
\end{itemize} 


%We propose using reinforcement learning (RL) for learning a general policy for selecting which tasks to replay at different time steps. We let the RL agent take actions representing which tasks to replay for the continual learning model based on the current performance on the seen tasks. The goal for the agent is thus to learn the time when the model should learn previously knowledge. To foster generalization between unseen continual learning scenarios, we let the agent interact with several different continual learning models with various variations of the task datasets they should learn from. Our goal is to obtain a general policy that can be applied in any continual learning scenario without adding any computational cost when it is deployed in new environments.       


%We propose using reinforcement learning~\citep{sutton2018reinforcement} for learning a general policy that suggests which tasks to replay based on the performance of the classifier. We target the continual learning setting from~\citep{klasson2021learn} where all seen datasets are stored and a replay memory is filled at every time step by selecting which tasks to replay to mitigate catastrophic forgetting. In this setting, the reinforcement learning agent acts as a scheduler over which old tasks that should be replayed when learning new tasks. The scheduling agent bases its decision on how well the classifier is performing on the seen tasks to determine whether a task should be replayed. The agent selects a proportion of how much each task should be replayed such that the replay memory can be filled samples from the selected tasks. The goal for the agent is to be capable generalizing to new continual learning environments and decide which tasks that should be replayed there.  
%We propose using RL to learn a general policy that suggests which tasks to replay based on the performance of the continual learner. We target the CL setting from [ICML WS paper].  

%Our contributions are the following:
%\begin{itemize}[topsep=1pt, noitemsep]
%    \item We propose a novel method that enables learning the time to learn in a continual learning settings. The learned policy can then be transferred to any new continual learning scenario without any additional training.
%    \item We evaluate the generalization capabilities of the learned policy on unseen continual learning environments where we vary the task splits as well as introducing unseen datasets to evaluate. 
    %%\item We propose a novel method for learning the time to learn in a continual learning setting that is transferable to new continual learning environments.
    %%\item We evaluate the generalization capabilities of the learned policies on environments with different task splits as well as unseen datasets during training. 
%\end{itemize}
%Our contributions are the following:
%*Propose a novel method for learning the time to learn that is transferable to new CL environments.
%* We evaluate the policy's generalization capabilities on various settings and datasets. 


%Machine learning systems in the real-world often receive streaming data for making decisions. These systems can also be required to adapt fast to new and unseen data points. Furthermore, data is often stored in the cloud for updating the system through its lifetime to prevent the system from forgetting relevant knowledge. Memory-based continual learning aims to tackle the challenges of adapting fast to new data and avoid forgetting previously learned tasks by keeping a limited memory of seen samples that is replayed when new data is received. However, the limitation of storage space in memory-based continual learning can be rather hard restriction that is not necessarily a restriction in many real-world scenarios. 

%Assuming that memory storage space is more or less unlimited, it is preferable to avoid re-training the system on all stored samples due to the computational costs and time. Therefore, replaying a limited amount of old samples to mitigate catastrophic forgetting is still relevant many machine learning models. However, determining how to select these old samples in an optimal way for the models to retain the old knowledge can be very challenging especially when there are huge amounts of data to select from. 

%Research in memory-based continual learning have mainly focused on memory population methods for sample selection or compression techniques for increasing storage capacity. However, these works ignore the time to learn from old tasks again and often simply use random sampling for creating batches of replay data. Learning when to replay is challenging and non-trivial but comes with several potential advantages, e.g., 1) using replay when the replay memory size is smaller than the total number of classes, and 2) allows for focusing on replaying harder tasks more. Additionally, taking into account the time to learn replay tasks also stems well with human learning techniques such as spaced repetition which has been shown to superior for knowledge retention compared to rehearsing at all times steps. 

%In this paper, we aim to learn a policy for deciding which tasks to replay at different time steps. This can be done using search methods e.g. random searches or tree searches by building a search tree over which tasks that should be replayed. However, using such search methods to learn this policy could have difficulties generalizing to new datasets and domains. Reinforcement learning helps to learn a general policy if we define the states and actions in general ways such that they can be reused in other domains. 

%We propose to learn a policy for deciding which tasks to replay based on the current performance of the seen tasks. Our goal is to learn a general policy that we can transfer to different datasets learned in continual learning scenarios to avoid retraining the policy for every new domain of interest. The learned policy is evaluated on zero-shot policy transfer scenarios where the policy is deployed in unseen domains to maximize its reward without finetuning on the environment. We evaluate the policies in environments of different degrees of difficulty including unseen label sequences and new datasets. To the best of our knowledge, we are the first to propose using reinforcement learning for learning retrieval policy of which task to replay in continual learning settings. 

%Research question(?): Can we learn replay scheduling policies that can be reused in new CL environments? No requirement for retraining 

%\MK{Question, how should I motivate this work? "Automatically select which tasks to replay based on the task performance" ... "learn a general policy that can be transferred to new environments ... hard to construct such heuristic so learning is needed"}


