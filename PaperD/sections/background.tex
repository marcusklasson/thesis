
\section{Preliminaries}\label{paperD:sec:background}

In this section, we recall the CL setting in~\citeD{D:klasson2021learn} that we consider as well as some background on RL and the algorithms Deep Q-Networks (DQNs)~\citeD{D:mnih2013playing, D:mnih2015human}, Advantage Actor-Critic (A2C)~\citeD{D:mnih2016asynchronous}, and Soft Actor-Critic~~\citeD{D:haarnoja2018soft,D:haarnoja2018sac_algo}. 


\subsection{Continual Learning Setting}
%\vspace{-3mm}
%\paragraph{Continual Learning Setting.} 
We consider the CL setting presented in Klasson \etal~\citeD{D:klasson2021learn}. In contrast to the traditional CL setting~\citeD{D:delange2021continual, D:parisi2019continual}, the historical data is assumed to be accessible at any time for replay to mitigate catastrophic forgetting. However, retraining on all seen data is prohibited due to processing time constraints, such as limitations on data transmission times or the allowed time for learning new tasks. The learning setup is the same as for CL in image classification, where a network parameterized by $\vphi$ should learn $T$ tasks arriving sequentially in the form of $T$ datasets $\gD_{1:T} = \{\gD_1, \dots, \gD_T\}$. The dataset for the $t$-th task $\gD_t = \{(\vx_t^{(i)}, y_t^{(i)})\}_{i=1}^{N_t}$ where $\vx_t^{(i)}$ and $y_t^{(i)}$ are the $i$-th data point and corresponding class label among a total of $N_t$ examples in the dataset. Furthermore, each dataset is split into a training, validation, and test set, i.e., $\gD_t = \{\gD_t^{train}, \gD_t^{val}, \gD_t^{test}\}$. The objective at task $t$ is to minimize the loss $\ell(f_{\vphi}(\vx_t), y_{t})$ where $\ell(\cdot)$ is the cross-entropy loss in our case. 

The challenge is for the network $f_{\vphi}$ to retain its performance on the previous tasks. However, as the historical data can be huge, we are only allowed to fill a tiny replay memory $\gM$ of size $M$ with historical data due to limitations on processing time. Hence, we need a method for selecting which tasks to replay and the amount of samples to add to the replay memory from each selected task. We use the same approach as in~\citeD{D:klasson2021learn} to enable the task selection, 
where a discrete action space of task proportions is created. At each task $t$, there is a finite set possible task proportions $\vp_t = (p_1, \dots, p_{T-1})$, where $\sum_j p_j = 1$ with $p_j \geq 0$ if $j < t$ otherwise $p_j = 0$, that can be used for constructing the replay memory for mitigating catastrophic forgetting. 
%See Appendix \ref{paperD:app:action_space} for details on how to construct the action space. 
Replay scheduling involves finding a policy for selecting task proportions that are efficient in mitigating catastrophic forgetting in the CL network. In Section \ref{paperD:sec:methodology}, we describe our RL-based framework for learning such policies. 


\subsection{Background on Reinforcement Learning}

The RL setup considers an agent interacting with an environment $E$ over a number of discrete time steps~\citeD{D:sutton2018reinforcement}. The environment is modeled with a Markov Decision Process (MDP)~\citeD{D:bellman1957markovian} represented as a tuple $E = (\gS, \gA, P, R, \mu, \gamma)$ consisting of the state space $\gS$, action space $\gA$, state transition probability $P(s' | s, a)$, reward function $R(s, a)$, initial state distribution $\mu(s_1)$, and discount factor $\gamma$. At each time step $t$, the agent receives a state $s_t$ from the environment, selects an action $a_t \in \gA$ using a policy $\pi(a | s)$, and enters the next state $s_{t+1}$ with transition probability $P(s_{t+1} | s_t, a_t)$ and receives a numerical reward following $r_t$ from the environment. This procedure is repeated until the agent reaches a terminal state in which the procedure can be restarted. The return $G_t = \sum_{k=0}^{\infty} \gamma^{k} r_{t+k}$ is the discounted accumulated reward from time step $t$. The goal for the agent is to learns  policy that maximizes the expected return.

The action value $Q^{\pi}(s, a) = \E[G_t | s_t=s, a]$  is the expected return for selecting action $a$ in state $s$ and following policy $\pi$. The optimal action value $Q^{*}(s, a) = \max_{\pi} Q^{\pi}(s, a)$ is defined as the maximum action value for state $s$ and action $a$ for any given policy $\pi$. Deep Q-Networks (DQNs)~\citeD{D:mnih2013playing, D:mnih2015human} is a value-based RL algorithm which aims to approximate the optimal action value function as $Q^{*}(s, a) \approx Q_{\vtheta}(s, a)$ with a neural network $Q_{\vtheta}$ parameterized by $\vtheta$. For learning the function $Q_{\vtheta}$, we collect data from the environment with by using an epsilon-greedy policy~\citeD{D:sutton2018reinforcement} to estimate $Q_{\vtheta}$ by minimizing the loss
\begin{align}\label{eq:dqn_loss}
	\gL_{\text{DQN}}(\vtheta) = (y_t - Q_{\vtheta}(s_t, a_t))^2
\end{align}
with $y_t = r_t + \gamma \max_{a'} Q_{\vtheta^{-}}(s_{t+1}, a')$, where $\vtheta^{-}$ is a previous copy of the network $\vtheta$ referred to as the target network. In addition to the introduction of target networks, several methods have been proposed for stabilizing the learning process, such as using experience replay~\citeD{D:lin1992self} to sample training data stored in a replay buffer, applying L1-smoothing to the loss, and correcting the action value estimates~\citeD{D:van2016deep}. 

An alternative to value-based RL is policy gradient methods where the optimal policy is estimated directly with a parameterized form $\pi_{\vtheta}(a | s)$ where $\vtheta$ represents the parameters of a neural network. 
Similar to the action value function, the value function $V^{\pi}(s) = \E[G_t | s_t=s]$ defines the expected return following policy $\pi$ from state $s$. In actor critic methods, such as Advantage Actor-Critic (A2C)~\citeD{D:mnih2016asynchronous} and Proximal Policy Optimization (PPO)~\citeD{D:schulman2017proximal}, we estimate the value function with a neural network $V_{\vtheta_v}$ to reduce variance of the gradients. 
The policy network takes the state $s_t$ as input and outputs a distribution over the possible actions $a_t$. The agent collects experiences from the environment with the current policy to use for updating the parameters $\vtheta$ by minimizing the loss 
\begin{align}\label{eq:pg_loss}
	\gL_{\text{PG}}(\vtheta) = \E[\log \pi_{\vtheta}(a | s) \hat{A}(s_t, a_t)], 
\end{align}
where $\hat{A}_{\vtheta_v}(s_t, a_t)$ is an estimate of the advantage function given by $\sum_{i=0}^{k-1} = \gamma^{i} r_i + \gamma^{k} V_{\vtheta_v}(s_{t+k}) - V_{\vtheta_v}(s_{t})$. The policy and value function can be updated after $t_{max}$ actions or when a terminal state is reached~\citeD{D:mnih2016asynchronous}.

Model-free deep RL algorithms usually suffer from high sample complexity or requires careful hyperparameter tuning. Soft Actor-Critic (SAC)~\citeD{D:haarnoja2018soft,D:haarnoja2018sac_algo} is a maximum entropy RL algorithm that aims to maximize the expected reward and the entropy of the policy, i.e., 
\begin{align}\label{eq:sac_loss}
	\gL_{\text{SAC}} = \E[R(s_t, a_t) + \alpha \gH(\pi(\cdot | s_t))],
\end{align}
where $\alpha$ is a temperature parameter determining the relative importance of the entropy against the reward. The goal for maximum entropy RL is to maximize the reward while maintaining high entropy to encourage exploration of promising actions. SAC has been shown to learn faster than on-policy methods, such as PPO, in tasks with high-dimensional continuous action spaces and being less sensitive to hyperparameter selection~\citeD{D:haarnoja2018soft}. 
