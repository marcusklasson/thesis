
\section{Background}\label{paperD:sec:background}

In this section, we recall the CL setting from Klasson \etal~\cite{D:klasson2021learn} and give a brief overview of RL and Deep Q-Networks (DQNs)~\citeD{D:mnih2013playing, D:mnih2015human}.

%\mk{We are using the CL setting from [our paper], we will recall the setting next}

\vspace{-3mm}
\paragraph{Continual Learning Setting.} We target the CL setting presented in Klasson \etal~\citeD{D:klasson2021learn} where historical data is assumed to be accessible at any time to connect the current CL research with real-world scenarios where historical data is accessible at any time. The CL model parameterized by $\vphi$ should learn $T$ tasks arriving sequentially in the form of $T$ datasets $\gD_{1:T} = \{\gD_1, \dots, \gD_T\}$. The dataset for the $t$-th task $\gD_t = \{(\vx_t^{(i)}, y_t^{(i)})\}_{i=1}^{N_t}$ where $\vx_t^{(i)}$ and $y_t^{(i)}$ are the $i$-th data point and corresponding class label among a total of $N_t$ examples in the dataset. In contrary to traditional CL, all seen data are available to use for replay here. However, we are only allowed to fill a tiny replay memory $\gM$ of size $M$ with historical data due to limitations on processing time. Hence, we need a method for selecting which tasks to replay and the amount of samples to add to the replay memory from each selected task. 


%We target the continual learning setting presented in ~\cite{klasson2021learn} where the method is constrained by processing time rather than storing historical data. As in many real-world settings, we assume that historical data has unlimited storage and is accessible at any time. However, since it is impossible to train the model on all historical data when new tasks have to be learned, we can use a tiny subset of historical data to replay for mitigating catastrophic forgetting of old tasks. The challenge is then how to select which tasks to replay at different times to perform well on new tasks while retaining previous learned knowledge.   

%Next, we lay out the notation for this continual learning setting. We let a classifier parametrized by $\vphi$ learn $T$ tasks that arrive sequentially in the form of $T$ datasets $\gD_{1:T} = \{\gD_1, \dots, \gD_T\}$. The dataset for the $t$-th task $\gD_t = \{(\vx_t^{(i)}, y_t^{(i)})\}_{i=1}^{N_t}$ where $\vx_t^{(i)}$ and $y_t^{(i)}$ are the $i$-th data point and corresponding class label among a total of $N_t$ examples in the dataset. In contrary to traditional continual learning, all seen data are available to use for replay here. However, we are only allowed to fill a tiny replay memory $\gM$ of size $M$ with historical data, so we need a method for select which tasks to replay and by what proportion to fill the memory with the selected tasks. \MK{These two paragraphs above could probably be merged?}


%\paragraph{Continual Learning.} Enabling autonomous agents to learn throughout their lifespan is a challenge yet to be solved. Continual learning (CL) in neural networks~\cite{delange2021continual, parisi2019continual} commonly involve a classifier parametrized by $\vphi$ that should update its knowledge about a sequence of tasks that arrives to the model in the form of $T$ datasets $\gD_{1:T} = \{\gD_1, \dots, \gD_T\}$. The dataset for the $t$-th task $\gD_t = \{(\vx_t^{(i)}, y_t^{(i)})\}_{i=1}^{N_t}$ where $\vx_t^{(i)}$ and $y_t^{(i)}$ are the $i$-th data point and corresponding class label among a total of $N_t$ examples in the dataset. One of the main challenges of CL is catastrophic forgetting of old tasks where already existing knowledge is overwritten with information about the recently learned tasks~\cite{mccloskey1989catastrophic}. A simple yet efficient approach to mitigate catastrophic forgetting is to replay samples from old tasks that are stored in an external memory $\gM$ when learning new tasks~\cite{chaudhry2019tiny, hayes2020remind, rebuffi2017icarl}. Replaying these samples are done by mixing them training samples from the current task dataset. After training on task $t$ is finished, we select samples from task $t$ to store in the memory to use for replay when learning new tasks on future time steps. Commonly, the memory is populated by randomly sampling from the seen datasets to gather replay samples. 


\vspace{-3mm}
\paragraph{Reinforcement Learning.} We treat the CL setting described above as an RL problem where an agent interacts with an environment with a discrete sequence of states, actions, and rewards of length $T$. The environment is formalized as a Markov Decision Process (MDP) represented as a tuple $E = (\gS, \gA, P, r, \gS_1, \gamma)$ consisting of state space $\gS$, action space $\gA$, transition distribution $P(s_{t+1}| s_{t}, a_{t})$, reward function $r(s_{t}, a_{t})$, initial state distribution $\gS_1$, and discount factor $\gamma \in [0, 1]$~\citeD{D:bellman1957markovian}. The action selection strategy is represented as the policy $\pi: \gS \rightarrow \gA$. The goal for the agent is to learn an optimal policy $\pi^{*}$ for selecting actions that maximize the expected sum of rewards. In this paper, the learned policy should select actions to maximize the CL performance for the model measured using classification accuracy. Therefore, we represent the states $s_{t} \in \gS$ as the classification performance of model $\vphi$ and the actions $a_{t} \in \gA$ represent task proportions over the tasks to fill the replay memory $\gM$ with.  

%We connect the continual learning setting described above with reinforcement learning where an agent interacts with an environment with a sequence of states, actions, and rewards. The environment is formalized as a Markov Decision Process (MDP) represented as a tuple $(\gS, \gA, P, R, \mu, \gamma)$ consisting of state space $\gS$, action space $\gA$, transition distribution $P(s'|s,a)$, reward function $R(s, a)$, initial state distribution $\mu(s_0)$, and discount factor $\gamma \in [0, 1]$~\cite{bellman1957markovian}. The goal for the agent is to learn a policy $\pi(a | s)$ for selecting actions that maximize the expected discounted return: $G_t =  \sum_{t=1}^T \gamma^t R(s_t, a_t)$. Furthermore, we consider state and action spaces that are expanding at every step $t$, such that $\gS \in (\gS_1, \dots, \gS_{T})$ and $\gA \in (\gA_1, \dots, \gA_{T-1})$ where we have omitted the action space at $T$ since the episodes are of fixed to length $T$. 

%\paragraph{Reinforcement Learning.} We examine settings where an agent is interacting with an environment $\gE$ with a sequence of actions, states, and rewards. The interactions between the agent and the environment are formalized as a Markov Decision Process (MDP) represented as a tuple $(\gS, \gA, P, R, \mu, \gamma)$ consisting of state-space $\gS$, action space $\gA$, transition distribution $P(s'|s,a)$, reward function $R(s, a)$, initial state distribution $\mu(s_0)$, and discount factor $\gamma \in [0, 1]$~\cite{bellman1957markovian}. The goal for the agent is to learn a policy $\pi(a | s)$ for selecting actions that maximize the expected discounted return: $G_t =  \sum_{t=0}^T \gamma^t R(s_t, a_t)$. The state-action value $Q_{\pi}(s, a) = \E_{\pi}[G_t | s, a]$ is defined as the expected sum of future rewards when taking action $a$ given state $s$ under a given policy $\pi$ and following that policy thereafter. The optimal state-action value is then $Q^{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)$ gives the expected return when selecting action $a$ given state $s$ and acting optimally thereafter. If $Q^{*}$ is known, then an optimal deterministic policy $\pi^{*}$ can be obtained by acting greedily with respect to $Q^{*}$, i.e., taking $\pi^{*}(s) = \argmax_{a} Q^{*}(s, a)$~\cite{sutton2018reinforcement}.  

\paragraph{Deep Q-Networks.} We employ DQNs for learning a policy for replay scheduling. For DQNs, the goal is to estimate the state-action value function recursively using the Bellman equation:
\begin{align}
    Q^{\pi}(s_{t}, a_{t}) = \E_{s_{t+1} \sim P} [r_t + \gamma \max_{a'} Q^{\pi}(s_{t+1}, a')].
\end{align}
In many applications, the transition distribution $P$ is unknown, so we collect data from the environment to learn a function $Q_{\vtheta}(s, a)$ parameterized by $\vtheta$ that estimates the Q-function by minimizing the loss $L(\vtheta) = (y_t - Q_{\vtheta}(s_t, a_t))^2$ where $y_t = r_t + \gamma \max_{a'} Q_{\vtheta}(s_{t+1}, a')$. The learned Q-function is parameterized by a neural network to ease the estimation in problems with high-dimensional state spaces. To stabilize the learning, DQNs use target networks from previous iterations to compute the target values $y_t$, incorporates experience replay~\citeD{D:lin1992self}, and can apply Double Q-learning~\citeD{D:van2016deep} to mitigate overestimation of the state-action values.  



