
\section{Related Work}\label{paperD:sec:related_work}

In this section, we place this paper into context by providing a brief overview of CL as well as generalization in RL. 

\vspace{-3mm}
\paragraph{Continual Learning.} The various CL approaches for mitigating catastrophic forgetting can broadly be divided into three categories, namely, regularization-based~\citeD{D:kirkpatrick2017overcoming, D:li2017learning, D:nguyen2017variational, D:schwarz2018progress, D:saha2021gradient, D:kao2021natural, D:adel2019continual}, architecture-based~\citeD{D:ebrahimi2020adversarial, D:rusu2016progressive, D:yoon2017lifelong, D:mallya2018packnet, D:serra2018overcoming, D:yoon2019scalable, D:xu2018reinforced}, and replay-based approaches~\citeD{D:chaudhry2019tiny, D:hayes2020remind, D:lopez2017gradient, D:aljundi2019online, D:van2018generative, D:van2020brain, D:verwimp2021rehearsal, D:yoon2021online, D:aljundi2019gradient, D:borsos2020coresets, D:chrysakis2020online, D:hayes2019memory, D:jin2020gradient, D:pellegrini2019latent, D:klasson2021learn, D:shin2017continual}. 
Regularization-based methods generally focuses on applying regularization techniques on parameters important for recognizing old tasks and fit the remaining parameters to new tasks~\citeD{D:kirkpatrick2017overcoming, D:zenke2017continual, D:nguyen2017variational, D:adel2019continual}. Furthermore, knowledge distillation~\citeD{D:hinton2015distilling} has been used for regularizing network output units of previous tasks~\citeD{D:li2017learning, D:schwarz2018progress}, as well as constraining the parameter updates to subspaces with gradient projections to avoid interference with previous tasks~\citeD{D:saha2021gradient, D:kao2021natural}. Architecture-based approaches focuses on adding task-specific network modules for every seen task~\citeD{D:rusu2016progressive, D:yoon2017lifelong, D:yoon2019scalable, D:ebrahimi2020adversarial, D:xu2018reinforced}, or isolating parameters for predicting specific task in fixed-size networks~\citeD{D:mallya2018packnet, D:serra2018overcoming, D:schwarz2021powerpropagation}. Replay-based methods re-trains the network on samples of old tasks that are either stored in an external memory~\citeD{D:aljundi2019online, D:chaudhry2019tiny, D:hayes2020remind, D:rolnick2018experience, D:jin2020gradient, D:chrysakis2020online, D:aljundi2019gradient, D:verwimp2021rehearsal, D:yoon2021online, D:borsos2020coresets, D:pellegrini2019latent, D:klasson2021learn}, or synthesized with a generative model~\citeD{D:van2018generative, D:van2020brain, D:shin2017continual}. 
Scheduling over which samples or tasks to replay has been studied in~\citeD{D:aljundi2019online, D:klasson2021learn}. This work builds on the replay scheduling idea in~\citeD{D:klasson2021learn} where we propose an RL-based framework for learning policies that selects which tasks to replay at different times. 

\vspace{-3mm}
\paragraph{Generalization in Reinforcement Learning.} Generalization is an active research topic in RL~\citeD{D:kirk2021survey} as RL agents tend to overfit to their training environments~\citeD{D:henderson2018deep, D:zhang2018dissection, D:zhao2019investigating, D:zhang2018study}. The goal is often to transfer learned policies to environments with new tasks~\citeD{D:finn2017model, D:kessler2021same, D:higgins2017darla, D:khetarpal2020towards} and action spaces~\citeD{D:chandak2019learning, D:jain2020generalization, D:chandak2020lifelong}. Some approaches aim to improve generalization capabilities by generating more diverse training data~\citeD{D:cobbe2019quantifying, D:tobin2017domain, D:zhang2018dissection}, using network regularization or inductive biases~\citeD{D:farebrother2018generalization, D:igl2019generalization, D:zambaldi2018relational}, or learning dynamics models~\citeD{D:ball2021augmented, D:nagabandi2018learning}. In this paper, we use RL for learning policies for selecting which tasks a CL network should replay. The goal is to learn policies that can be applied in new CL environments for replay scheduling on unseen task orders and datasets without additional computational cost. 


%\paragraph{Human Learning.} We draw inspiration from human learning techniques, especially spaced repetition~\citeDp{dempster1989spacing}, in our approach for learning replay scheduling policies. 

%\MK{
%Some links on human learning and ML with spaced repetition 
%\begin{itemize}
%    \item Enhancing human learning via spaced repetition optimization, Tabibian et al. \url{https://www.pnas.org/content/pnas/116/10/3988.full.pdf}, \url{http://learning.mpi-sws.org/memorize/}
%    \item Accelerating Human Learning with Deep Reinforcement Learning, Reddy et al.
%    \item Unbounded Human Learning: Optimal Scheduling for Spaced Repetition Reddy et al. \url{https://dl.acm.org/doi/abs/10.1145/2939672.2939850}
%\end{itemize}}