

%%% Paper content

\chapter{
	\centering{Meta Policy Learning for Replay Scheduling in Continual Learning}
}\label{paperD}
\chaptermark{Meta Policy Learning for Replay Scheduling}
\vspace{-5mm}
\begin{center}
	\large{\textbf{Marcus Klasson$^{*}$, Hedvig Kjellstr√∂m$^{*}$, Cheng Zhang$^{\dagger}$}} \\[2mm]
	\small{$^{*}$KTH Royal Institute of Technology, Stockholm, Sweden} \\
	\small{$^{\dagger}$Microsoft Research, Cambridge, United Kingdom} \\
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
	%\printinunitsof{in}\prntlen{\linewidth}
	\noindent Learning the time to replay different tasks have been demonstrated to be important in continual learning. 
	However, a replay scheduling policy that can be applied in any continual learning scenario is currently absent, which makes replay scheduling infeasible in real-world scenarios.  
	To this end, we propose using reinforcement learning to enable learning general policies that can generalize across different data domains. Thus, an efficient replay schedule can be used in any new real-world continual learning scenario to improve the continual learning performance without any additional computational cost.
	We compare the learned policies to several replay scheduling baselines and show that the learned policies can improve the continual learning performance given %any 
	unseen tasks and datasets. 
\end{abstract}


%%% Contents
\input{PaperD/sections/introduction}
\input{PaperD/sections/background}
\input{PaperD/sections/related_work}
\input{PaperD/sections/methodology}
\input{PaperD/sections/experiments}
\input{PaperD/sections/discussion}
\input{PaperD/sections/conclusions}

\begin{appendices}
\input{PaperD/appendix/appendix}
\end{appendices}
%\clearpage

%\renewcommand*{\bibname}{References}
\bibliographystyleD{unsrt}
{\small
	\bibliographyD{PaperD/refD}
}