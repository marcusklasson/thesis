
\section{Construction of the Action Space}\label{app:construction_of_the_action_space}

In this section, we describe the action space construction used in~\cite{klasson2021learn} that we use in our experiments. Each action $a_t = (p_1, \dots, p_{T-1}) \in \gA_t$ consists of a sequence of task proportions $p$ where the valid proportions sum to one, i.e., $\sum_{i=1}^{t} p_i = 1$, and invalid proportions are set to zero as $p_i = 0$ for $i>t$. The simplify the selection of task proportions, we create a discrete number of choices for the proportions of valid tasks. We use a binning method to construct the discrete action space. At task $t$, there are $t-1$ historical tasks that can be replayed. Thus, we create $t-1$ bins $\vb_t = [b_1, \dots, b_{t-1}]$ and choose a task index to sample for each bin $b_i \in \{1, \dots, t-1 \}$. The bins are treated as interchangeable and only the unique choices are kept. As an example, the unique choices of vectors at task 3 are $[1,1], [1,2], [2,2]$, where $[1,1]$ indicates that all memory samples are from task 1, $[1,2]$ indicates that half memory is from task 1 and the other half are from task etc. The task proportions for each choice are then computed by counting the number of occurrences of each task index in $\vb_t$ and dividing by $t-1$, such that $a_t = \texttt{bincount}(\vb_t) / (t-1)$. The task proportions are used to determine the number of samples to place in the replay memory from each task, such that $\texttt{round}(p_t \cdot M)$ are the number of samples from task $t$ where we round up or down accordingly while keeping the memory size $M$ fixed. From this specification, the whole action space over the task-horizon becomes a tree of different replay schedules that can be evaluated by the network.
