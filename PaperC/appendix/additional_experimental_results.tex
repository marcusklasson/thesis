
\section{Additional Experimental Results}
\label{paperC:app:additional_experimental_results}

In this section, we bring more insights to the benefits of replay scheduling in Section \ref{paperC:app:replay_schedule_visualization_for_split_mnist}, provide statistical significance tests for the experiments on memory selection, applying scheduling to recent replay methods, and efficiency of replay scheduling in Section \ref{paperC:app:statistical_significance_tests}, and show performance metrics and statistical significance tests for the varying memory size experiments in Section \ref{paperC:app:varying_memory_size}. 
%In this section, we bring more insights to the benefits of replay scheduling in Section \ref{paperC:app:replay_schedule_visualization_for_split_mnist} as well as provide metrics for catastrophic forgetting in Section \ref{paperC:app:analysis_of_catastrophic_forgetting}.

\subsection{Replay Schedule Visualization for Split MNIST}
\label{paperC:app:replay_schedule_visualization_for_split_mnist}

In Figure \ref{fig:split_mnist_task_accuracies_and_bubble_plot}, we show the progress in test classification performance for each task when using ETS and RS-MCTS with memory size $M=10$ on Split MNIST. For comparison, we also show the performance from a network that is fine-tuning on the current task without using replay. Both ETS and RS-MCTS overcome catastrophic forgetting to a large degree compared to the fine-tuning network. Our method RS-MCTS further improves the performance compared to ETS with the same memory, which indicates that learning the time to learn can be more efficient against catastrophic forgetting. Especially, Task 1 and 2 seems to be the most difficult task to remember since it has the lowest final performance using the fine-tuning network. Both ETS and RS-MCTS manage to retain their performance on Task 1 using replay, however, RS-MCTS remembers Task 2 better than ETS by around 5\%. 

To bring more insights to this behavior, we have visualized the task proportions of the replay examples using a bubble plot showing the corresponding replay schedule from RS-MCTS in Figure \ref{fig:split_mnist_task_accuracies_and_bubble_plot}(right). At Task 3 and 4, we see that the schedule fills the memory with data from Task 2 and discards replaying Task 1. This helps the network to retain knowledge about Task 2 better than ETS at the cost of forgetting Task 3 slightly when learning Task 4. This shows that the learned policy has considered the difficulty level of different tasks. At the next task, the RS-MCTS schedule has decided to rehearse Task 3 and reduces replaying Task 2 when learning Task 5. This behavior is similar to spaced repetition, where increasing the time interval between rehearsals helps memory retention.
We emphasize that even on datasets with few tasks, using learned replay schedules can overcome catastrophic forgetting better than standard ETS approaches.



\begin{figure}[t]
  \centering
  \setlength{\figwidth}{0.27\textwidth}
  \setlength{\figheight}{.15\textheight}
  \input{PaperC/appendix/figures/bubble_plots/mnist/seed3/mnist_accuracy_and_bubble_plot}
  \vspace{-3mm}
  \caption{ Comparison of test classification accuracies for Task 1-5 on Split MNIST from a network trained without replay (Finetune), ETS, and RS-MCTS (Ours). The ACC metric for each method is shown on top of each figure. We also visualize the replay schedule found by RS-MCTS as a bubble plot to the right. The memory size is set to $M=10$ with uniform memory selection for ETS and RS-MCTS. Results are shown for 1 seed. 
  }
  \vspace{-3mm}
  \label{fig:split_mnist_task_accuracies_and_bubble_plot}
\end{figure}



\subsection{Statistical Significance Tests}\label{paperC:app:statistical_significance_tests}

We provide p-values from Welch's t-tests to show the statistical significance between the methods in the experiments on different memory selections methods (Section \ref{paperC:sec:alternative_memory_selection_methods}), applying scheduling to recent replay methods (Section \ref{paperC:sec:applying_scheduling_to_recent_replay_methods}), and efficiency of replay scheduling (Section \ref{paperC:sec:efficiency_of_replay_scheduling}).

\vspace{-3mm}
\paragraph{Memory Selection Methods.} Table \ref{tab:t_test_memory_selection_appendix} presents the statistical significance tests between MCTS and the baselines that corresponds to the performance results in Table \ref{tab:results_memory_selection_methods}. We note that our method in general achieves significantly higher ACC comparing to the baselines showing that learning the time to learn is important.



\vspace{-3mm}
\paragraph{Recent Replay Methods.} Table \ref{tab:t_test_alternative_replay_methods} presents the statistical significance tests between MCTS and the baselines that corresponds to the performance results in Table \ref{tab:results_memory_selection_methods}. We note that our method (MCTS) in general achieves significantly higher ACC compared to the scheduling baselines, which shows that replay scheduling can improve the CL performance when combined with any replay method. 


\vspace{-3mm}
\paragraph{Efficiency of Scheduling.} Table \ref{tab:t_test_efficiency_replay_scheduling} presents the statistical significance tests between MCTS and the baselines that corresponds to the performance results in Table \ref{tab:efficiency_of_replay_scheduling_all_datasets}. Our method (MCTS) mostly performs on par with the baselines, but is significantly better than the baselines on Permuted MNIST, despite that MCTS replays fewer samples than the baselines.




\begin{table}[t]
	\centering
	\caption{Two-tailed Welch's $t$-test results for the various memory selection methods presented in Table \ref{tab:results_memory_selection_methods}. 
	}
	\vspace{-3mm}
	\resizebox{0.98\textwidth}{!}{
		\input{PaperC/appendix/tables/statistical_significance_tests/memory_selection}
	}
	\vspace{-3mm}
	\label{tab:t_test_memory_selection_appendix}
\end{table}


\begin{table}[t]
	\centering
	\caption{Two-tailed Welch's $t$-test results for the alternative replay methods results presented in Table \ref{tab:results_applying_scheduling_to_recent_replay_methods}. 
	}
	\vspace{-3mm}
	\resizebox{0.98\textwidth}{!}{
		\input{PaperC/appendix/tables/statistical_significance_tests/alternative_replay_methods}
	}
	\vspace{-3mm}
	\label{tab:t_test_alternative_replay_methods}
\end{table}


\begin{table}[t]
	\centering
	\caption{Two-tailed Welch's $t$-test results for efficiency of replay scheduling presented in Table \ref{tab:efficiency_of_replay_scheduling_all_datasets}. 
	}
	\vspace{-3mm}
	\resizebox{0.95\textwidth}{!}{
		\input{PaperC/appendix/tables/statistical_significance_tests/efficiency_replay_scheduling}
	}
	\vspace{-3mm}
	\label{tab:t_test_efficiency_replay_scheduling}
\end{table}


\clearpage


\subsection{Varying Memory Size in Different Continual Learning Setting}
\label{paperC:app:varying_memory_size}

Here, we provide the ACC and BWT metrics from the varying memory size experiments from figure \ref{fig:acc_over_replay_memory_size} in Section \ref{paperC:sec:results_with_varying_replay_memory_size}. We also provide the p-values from Welch's t-tests to show the statistical significance between the methods. 
\begin{itemize}[topsep=0pt, noitemsep]
	\item \textbf{Domain-IL, 10-task Permuted MNIST:} Metrics in Table \ref{tab:varying_memory_size_domain_il_permutedmnist}, t-tests in Table \ref{tab:t_test_varying_memory_size_domain_il_permutedmnist}.
	
	\item \textbf{Task-IL, 5-task Split MNIST/FashionMNIST/notMNIST:} Metrics in Table \ref{tab:varying_memory_size_task_il_5task_datasets}, t-tests in Table \ref{tab:t_test_varying_memory_size_task_il_5task_datasets}. 
	
	\item \textbf{Task-IL, 20-task Split CIFAR-100/miniImagenet:} Metrics in Table \ref{tab:varying_memory_size_task_il_20task_datasets}, t-tests in Table \ref{tab:t_test_varying_memory_size_task_il_20task_datasets}. 
	
	\item \textbf{Class-IL, 5-task Split MNIST/FashionMNIST/notMNIST:} Metrics in Table \ref{tab:varying_memory_size_class_il_5task_datasets}, t-tests in Table \ref{tab:t_test_varying_memory_size_class_il_5task_datasets}. 
	
	\item \textbf{Class-IL, 20-task Split CIFAR-100/miniImagenet:} Metrics in Table \ref{tab:varying_memory_size_class_il_20task_datasets}, t-tests in Table \ref{tab:t_test_varying_memory_size_class_il_20task_datasets}. 
	%\vspace{-1mm}
\end{itemize}
We note that MCTS mostly performs significantly better than the baselines on the 10-task Permuted MNIST and, interestingly, on the 20-task Split CIFAR-100 and Split miniImagenet in both Task-IL and Clas-IL settings, which shows the importance of replay scheduling for CL datasets with long task horizons.    

\begin{table}[h]
	%\caption{Global caption}
	
	\begin{minipage}{.5\textwidth}
	\centering
	\captionsetup{width=.9\linewidth}
	\caption{Performance comparison in the Domain Incremental Learning setting over various memory sizes for the methods on Permuted MNIST. 
	}
	\vspace{-3mm}
	\resizebox{0.95\textwidth}{!}{
	\input{PaperC/appendix/tables/varying_memory_size_domain_il_permutedmnist.tex}
	}
	%\vspace{-5mm}
	\label{tab:varying_memory_size_domain_il_permutedmnist}
	\end{minipage}% \quad
	\begin{minipage}{.5\textwidth}
	\centering
	\captionsetup{width=.9\linewidth}
	\caption{Two-tailed Welch's $t$-test results for the varying memory size experiments presented in Table \ref{tab:varying_memory_size_domain_il_permutedmnist} in the Domain Incremental Learning setting on Permuted MNIST. }
	\vspace{-3mm}
	\resizebox{0.95\textwidth}{!}{
	\input{PaperC/appendix/tables/statistical_significance_tests/varying_memory_size_domain_il_permutedmnist.tex}
	}
	\label{tab:t_test_varying_memory_size_domain_il_permutedmnist}
	\end{minipage} 
\end{table}

\begin{table}[h]

\end{table}

\begin{comment}

\begin{table}[h]
	\centering
	\caption{Two-tailed Welch's $t$-test results for the varying memory size experiments presented in Table \ref{tab:varying_memory_size_task_il_5task_datasets} in the Domain Incremental Learning setting on Permuted MNIST. 
	}
	\vspace{-3mm}
	\resizebox{0.5\textwidth}{!}{
		\input{PaperC/appendix/tables/statistical_significance_tests/varying_memory_size_domain_il_permutedmnist.tex}
	}
	\label{tab:t_test_varying_memory_size_domain_il_permutedmnist}
\end{table}
	content...
\end{comment}


\begin{table}[t]
	\centering
	\caption{Performance comparison in the Task Incremental Learning setting over various memory sizes for the methods on the 5-task datasets. 
	}
	\vspace{-3mm}
	\resizebox{0.9\textwidth}{!}{
		\input{PaperC/appendix/tables/varying_memory_size_task_il_5task_datasets.tex}
	}
	\label{tab:varying_memory_size_task_il_5task_datasets}
\end{table}



\begin{table}[t]
	\centering
	\caption{Two-tailed Welch's $t$-test results for the varying memory size experiments presented in Table \ref{tab:varying_memory_size_task_il_5task_datasets} in the Task Incremental Learning setting on the 5-task datasets. 
	}
	\vspace{-3mm}
	\resizebox{0.9\textwidth}{!}{
		\input{PaperC/appendix/tables/statistical_significance_tests/varying_memory_size_task_il_5task_datasets.tex}
	}
	\label{tab:t_test_varying_memory_size_task_il_5task_datasets}
\end{table}

\begin{table}[t]
	\centering
	\caption{Performance comparison in the Task Incremental Learning setting over various memory sizes for the methods on the 20-task datasets. 
	}
	\vspace{-3mm}
	\resizebox{0.9\textwidth}{!}{
		\input{PaperC/appendix/tables/varying_memory_size_task_il_20task_datasets.tex}
	}
	\label{tab:varying_memory_size_task_il_20task_datasets}
\end{table}


\begin{table}[t]
	\centering
	\caption{Two-tailed Welch's $t$-test results for the varying memory size experiments presented in Table \ref{tab:varying_memory_size_task_il_20task_datasets} in the Task Incremental Learning setting on the 20-task datasets. 
	}
	\vspace{-3mm}
	\resizebox{0.9\textwidth}{!}{
		\input{PaperC/appendix/tables/statistical_significance_tests/varying_memory_size_task_il_20task_datasets.tex}
	}
	\label{tab:t_test_varying_memory_size_task_il_20task_datasets}
\end{table}

\begin{table}[t]
	\centering
	\caption{Performance comparison in the Class Incremental Learning setting over various memory sizes for the methods on the 5-task datasets. 
	}
	\vspace{-3mm}
	\resizebox{0.95\textwidth}{!}{
		\input{PaperC/appendix/tables/varying_memory_size_class_il_5task_datasets.tex}
	}
	\label{tab:varying_memory_size_class_il_5task_datasets}
\end{table}


\begin{table}[t]
	\centering
	\caption{Two-tailed Welch's $t$-test results for the varying memory size experiments presented in Table \ref{tab:varying_memory_size_class_il_5task_datasets} in the Class Incremental Learning setting on the 5-task datasets. 
	}
	\vspace{-3mm}
	\resizebox{0.95\textwidth}{!}{
		\input{PaperC/appendix/tables/statistical_significance_tests/varying_memory_size_class_il_5task_datasets.tex}
	}
	\label{tab:t_test_varying_memory_size_class_il_5task_datasets}
\end{table}

\begin{table}[t]
	\centering
	\caption{Performance comparison in the Class Incremental Learning setting over various memory sizes for the methods on the 20-task datasets. 
	}
	\vspace{-3mm}
	\resizebox{0.9\textwidth}{!}{
		\input{PaperC/appendix/tables/varying_memory_size_class_il_20task_datasets.tex}
	}
	\label{tab:varying_memory_size_class_il_20task_datasets}
\end{table}


\begin{table}[t]
	\centering
	\caption{Two-tailed Welch's $t$-test results for the varying memory size experiments presented in Table \ref{tab:varying_memory_size_class_il_20task_datasets} in the Class Incremental Learning setting on the 20-task datasets. 
	}
	\vspace{-3mm}
	\resizebox{0.9\textwidth}{!}{
		\input{PaperC/appendix/tables/statistical_significance_tests/varying_memory_size_class_il_20task_datasets.tex}
	}
	\label{tab:t_test_varying_memory_size_class_il_20task_datasets}
\end{table}





\begin{comment}
\subsection{Analysis of Catastrophic Forgetting}\label{paperC:app:analysis_of_catastrophic_forgetting}

We have compared the degree of catastrophic forgetting for our method against the baselines by measuring the backward transfer (BWT) metric from \citeC{C:lopez2017gradient}, which is given by
\begin{align}
	\textnormal{BWT} = \frac{1}{T-1} \sum_{i=1}^{T-1} A_{T, i} - A_{i, i},
\end{align}
where $A_{t, i}$ is the test accuracy for task $t$ after learning task $i$. Table \ref{tab:bwt_alternative_memory_selection} shows the ACC and BWT metrics for the experiments in Section \ref{paperC:sec:alternative_memory_selection_methods}. In general, the BWT metric is consistently better when the corresponding ACC is better. We find an exception in Table \ref{tab:bwt_alternative_memory_selection} on Split CIFAR-100 and Split miniImagenet between Ours and Heuristic with uniform selection method, where Heuristic has better BWT while its mean of ACC is slightly lower than ACC for Ours. Table \ref{tab:bwt_efficiency_of_replay_scheduling} shows the ACC and BWT metrics for the experiments in Section \ref{paperC:sec:efficiency_of_replay_scheduling}, where we see a similar pattern that better ACC yields better BWT. The BWT of RS-MCTS is on par with the other baselines except on Split CIFAR-100 where the ACC on our method was a bit lower than the best baselines.



\subsection{Applying Scheduling to Recent Replay Methods}
\label{paperC:app:apply_scheduling_to_recent_replay_methods}

In Section \ref{paperC:sec:applying_scheduling_to_recent_replay_methods}, we showed that RS-MCTS can be applied to any replay method. We combined RS-MCTS together with four recent replay methods, namely Hindsight Anchor Learning (HAL)~\citeC{C:chaudhry2021using}, Meta Experience Replay (MER)~\citeC{C:riemer2018learning}, and Dark Experience Replay (DER)~\citeC{C:buzzega2020dark}. Table \ref{tab:bwt_sota_models_applied_to_rsmcts} shows the ACC and BWT for all methods combined with the scheduling from Random, ETS, Heuristic, and RS-MCTS. We observe that RS-MCTS can further improve the performance for each of the replay methods across the different datasets.  

We present the hyperparameters used for each method in Table \ref{tab:hyperparameters_applying_scheduling_to_recent_replay_methods}. The hyperparameters for each method are denoted as
\begin{itemize}[topsep=0pt, leftmargin=*, noitemsep]
	\item {\bf HAL.} $\eta$: learning rate, $\lambda$: regularization, $\gamma$: mean embedding strength, $\beta$: decay rate, $k$: gradient steps on anchors   
	\item {\bf MER.} $\gamma$: across batch meta-learning rate, $\beta$: within batch meta-learning rate 
	\item {\bf DER.} $\alpha$: loss coefficient for memory logits 
	\item {\bf DER++.} $\alpha$: loss coefficient for memory logits, $\beta$: loss coefficient for memory labels
\end{itemize}
We %For the experiments, we 
used the same architectures and hyperparameters as described in Appendix \ref{paperC:app:experimental_settings} for all datasets, except for the optimizer in HAL where we used the SGD optimizer since using Adam made the model diverge in our experiments. %if not mentioned otherwise. 
We used the Adam optimizer with learning rate $\eta=0.001$ for MER, DER, and DER++. 
%For HAL, we used the SGD optimizer since using Adam made the model diverge in our experiments. 

	content...



\section{Additional Figures}\label{paperC:app:additional_figures}

\paragraph{Memory usage.} We visualize the memory usage for Permuted MNIST and the 20-task datasets Split CIFAR-100 and Split miniImagenet in Figure \ref{fig:memory_usage_10_and_20task_datasets} for our method and the baselines used in the experiment in Section \ref{paperC:sec:efficiency_of_replay_scheduling}. Our method uses a fixed memory size of $M=50$ samples for replay on all three datasets. The memory size capacity for our method is reached after learning task 6 and task 11 on the Permuted MNIST and the 20-task datasets respectively, while the baselines continue incrementing their replay memory size.

\begin{figure}[h]
	\centering
	\vspace{-1mm}
	\setlength{\figwidth}{0.75\textwidth}
	\setlength{\figheight}{.20\textheight}
	\input{PaperC/appendix/figures/memory_usage/groupplot}
	\vspace{-3mm}
	\caption{Number of replayed samples per task for 10-task Permuted MNIST (top) and the 20-task datasets in the experiment in Section \ref{paperC:sec:efficiency_of_replay_scheduling}. The fixed memory size $M=50$ for our method is reached after learning task 6 and task 11 on the Permuted MNIST and the 20-task datasets respectively, while the baselines continue incrementing their number of replay samples per task.}
	\label{fig:memory_usage_10_and_20task_datasets}
\end{figure}

%%% TABLE
\input{PaperC/appendix/tables/table_bwt_alternative_memory_selection}

\begin{table}[t]
	%\scriptsize
	\centering
	\caption{
		Hyperparameters for replay-based methods HAL, MER, DER and DER++ used in experiments on applying RS-MCTS to recent replay-based methods in Section \ref{paperC:sec:applying_scheduling_to_recent_replay_methods}.
	}
	\vspace{-3mm}
	\resizebox{0.95\textwidth}{!}{
		\begin{tabular}{l c c c c c c c}
			\toprule
			& & \multicolumn{3}{c}{{\bf 5-task Datasets}} & \multicolumn{3}{c}{{\bf 10- and 20-task Datasets}} \\
			\cmidrule(lr){3-5} \cmidrule(lr){6-8}
			{\bf Method} & {\bf Hyperparam.}  & S-MNIST & S-FashionMNIST & S-notMNIST & P-MNIST & S-CIFAR-100 & S-miniImagenet \\
			\midrule
			\multirow{5}{*}{HAL} & $\eta$ & 0.1 & 0.1 & 0.1 & 0.1 & 0.03 & 0.03 \\
			& $\lambda$  & 0.1 & 0.1 & 0.1 & 0.1 & 1.0 & 0.03 \\
			& $\gamma$ & 0.5 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
			& $\beta$ & 0.7 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\
			& $k$ & 100 & 100 & 100 & 100 & 100 & 100 \\
			\midrule 
			\multirow{2}{*}{MER} & $\gamma$ & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0  \\ 
			& $\beta$  & 1.0 & 0.01 & 1.0 & 1.0 & 0.1 & 0.1 \\
			\midrule 
			\multirow{1}{*}{DER} & $\alpha$ & 0.2 & 0.2 & 0.1 & 1.0 & 1.0 & 0.1 \\ 
			\midrule 
			\multirow{2}{*}{DER++} & $\alpha$ & 0.2 & 0.2 & 0.1 & 1.0 & 1.0 & 0.1 \\ 
			& $\beta$ & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-2mm}
	\label{tab:hyperparameters_applying_scheduling_to_recent_replay_methods}
\end{table}




\input{PaperC/appendix/tables/table_apply_scheduling_to_recent_replay_methods}

\input{PaperC/appendix/tables/table_bwt_efficiency_replay_scheduling}


%\clearpage
%\clearpage
\end{comment}
