

\begin{algorithm}[ht!]
\caption{Replay Scheduling Monte Carlo tree search}
\label{alg:replay_scheduling_mcts}
\algrenewcommand\alglinenumber[1]{\tiny #1:}
\scriptsize
\begin{algorithmic}[1]
\Require Tree nodes $v_{1:T}$, Datasets $\gD_{1:T}$, Learning rate $\eta$
\Require Replay memory size $M$ %, Historical data sample size $H$
%\State Initialize historical data $\gH$ using $\gD_{1:T}$
%\State Initialize model $\vtheta_0$
\State $\textnormal{ACC}_{best} \leftarrow 0$, $S_{best} \leftarrow ()$
%\State $\vtheta_1 \leftarrow \textsc{TrainModel}(\gD_1, \vtheta_0)$
\While{within computational budget}
    \State $S \leftarrow ()$
    \State $v_{t}, S \leftarrow \textsc{TreePolicy}(v_1, S)$%\State $v_{t}, \vtheta_t \leftarrow \textsc{TreePolicy}(v_1, \vtheta_1, \gD_{1:T}, \gH, M)$
    \State $v_{T}, S \leftarrow \textsc{DefaultPolicy}(v_{t}, S)$%\State $v_{t}, \vtheta_T, \textnormal{acc} \leftarrow \textsc{DefaultPolicy}(v_{t}, \vtheta_t, \gD_{1:T}, \gH, M)$
    \State ACC $\leftarrow \textsc{EvaluateReplaySchedule}(\gD_{1:T}, S, M)$
    \State $\textsc{Backpropagate}(v_{t}, \textnormal{ACC})$
    \If{$\textnormal{ACC} > \textnormal{ACC}_{best}$}
        %\State $v_T^{best} \leftarrow v_T$
        %\State $\vtheta_T^{best} \leftarrow \vtheta_{T}$
        \State $\textnormal{ACC}_{best} \leftarrow \textnormal{ACC}$
        \State $S_{best} \leftarrow S$
    \EndIf
\EndWhile
\State \Return $\textnormal{ACC}_{best}, S_{best}$

\Statex 
\vspace{-3pt}
\Function{TreePolicy}{$v_t, S$}
\While{$v_t$ is non-terminal}  
\If{$v_t$ not fully expanded}
    \State \Return $\textsc{Expansion}(v_t, S)$
\Else
    \State $v_{t} \leftarrow \textsc{BestChild}(v_t)$
    %\State $\va_{t} \leftarrow \textsc{GetTaskProportion}(v_t)$
    \State $S$.append$(\va_{t})$, where $\va_{t} \leftarrow \textsc{GetTaskProportion}(v_t)$
\EndIf
\EndWhile
\State \Return $v_t, S$
\EndFunction

\Statex

\Function{Expansion}{$v_t, S$}
\State Sample $v_{t+1}$ uniformly among unvisited children of $v_t$
%\State $\gM \stackrel{M}{\sim} \gH$ using memory combination in $v_{t+1}$
%\State $\vtheta_{t+1} \leftarrow \textsc{TrainModel}(\gD_{t+1} \cup \gM, \vtheta_{t})$
%\State $\va_{t+1} \leftarrow \textsc{GetTaskProportion}(v_{t+1})$
\State $S$.append$(\va_{t+1})$, where $\va_{t+1} \leftarrow \textsc{GetTaskProportion}(v_{t+1})$
\State Add new child $v_{t+1}$ to node $v_{t}$%\State Add new child $v_{t+1}$ with model $\vtheta_{t+1}$ to node $v_{t}$
\State \Return $v_{t+1}, S$
\EndFunction

\Statex 

\Function{BestChild}{$v_t$}
    \State $v_{t+1} =   \argmax\limits_{v_{t+1} \in \text{ children of } v} \text{max}(Q(v_{t+1})) + C \sqrt{\frac{2 \log(N(v_{t}))}{N(v_{t+1})}}$
    %\State Get model $\vtheta_{t+1}$ from node $v_{t+1}$
    \State \Return $v_{t+1}$
\EndFunction

\Statex 

\Function{DefaultPolicy}{$v_t, S$}
\While{$v_t$ is non-terminal}  
    \State Sample $v_{t+1}$ uniformly among children of $v_t$
    %\State $\va_{t+1} \leftarrow \textsc{GetTaskProportion}(v_{t+1})$
    \State $S$.append$(\va_{t+1})$, where $\va_{t+1} \leftarrow \textsc{GetTaskProportion}(v_{t+1})$
    %\State $\gM \stackrel{M}{\sim} \gH$ using memory combination in $v_{t+1}$
    %\State $\vtheta_{t+1} \leftarrow \textsc{TrainModel}(\gD_{t+1} \cup \gM, \vtheta_{t})$
    \State Update $v_{t} \leftarrow v_{t+1}$ %, $\vtheta_{t} \leftarrow \vtheta_{t+1}$
\EndWhile
%\State $\textnormal{acc} \leftarrow \textsc{TestModel}(\gD_{1:T}, \vtheta_{t})$
\State \Return $v_t, S$
\EndFunction

\Statex

\Function{EvaluateReplaySchedule}{$\gD_{1:T}, S, M$}
\State Initialize neural network $f_{\vtheta}$
%\State $\gH \leftarrow \{\}$
\For{$t = 1, \dots, T$}  
    \State $\va \leftarrow S[t-1]$
    \State $\gM \leftarrow \textsc{GetReplayMemory}(M, \va)$
    \For{$ \gB \sim \gD_t^{(train)}$}  
        \State $\vtheta \leftarrow SGD(\gB \cup \gM, \vtheta, \eta)$ 
        %\State $Q(v_t) \leftarrow R$
        %\State $v_t \leftarrow$ parent of $v_t$
    \EndFor
    %\State $\gH \leftarrow \textsc{UpdateHistoricalData}(\gD_t^{(train)}, H)$
\EndFor
\State $A_{1:T}^{(val)} \leftarrow \textsc{EvaluateAccuracy}(f_{\vtheta}, \gD_{1:T}^{(val)})$
\State $\textnormal{ACC} \leftarrow \frac{1}{T} \sum_{i=1}^{T} A_{T,i}^{(val)}$
\State \Return ACC
\EndFunction

\Statex

\Function{Backpropagate}{$v_t, R$}
\While{$v_t$ is not root}  
    \State $N(v_t) \leftarrow N(v_t)+1$
    \State $Q(v_t) \leftarrow R$
    \State $v_t \leftarrow$ parent of $v_t$
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}
