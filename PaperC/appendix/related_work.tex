
\section{Related Work}\label{app:related_work}
We give an overview of continual learning methods, essentially memory-based methods, as well as spaced repetition techniques for human continual learning.

\vspace{-2mm}
\paragraph{Continual Learning:} Traditional continual learning can be divided into three main areas, namely regularization-based, architecture-based, and memory-based approaches. Regularization-based methods aim to mitigate catastrophic forgetting by protecting parameters influencing the predictive performance from wide changes and use the rest of the parameters for learning the new tasks~\cite{adel2019continual, chaudhry2018riemannian, kirkpatrick2017overcoming, li2017learning, nguyen2017variational, rannen2017encoder, schwarz2018progress, zenke2017continual}. Architecture-based methods isolate task-specific parameters by either increasing network capacity~\cite{rusu2016progressive, yoon2019scalable, yoon2017lifelong} or freezing parts of the network~\cite{mallya2018packnet, serra2018overcoming} to maintain good performance on previous tasks. Memory-based methods store examples from previous tasks in an external memory~\cite{chaudhry2019tiny, hayes2020remind, isele2018selective, lopez2017gradient}, or uses a generative model to generate pseudo-samples from a distribution over tasks~\cite{shin2017continual, van2018generative}, that are mixed with the current task dataset to mitigate catastrophic forgetting. Regularization-based approaches and dynamic architectures have been combined with memory-based approaches to methods to overcome their limitations~\cite{chaudhry2018riemannian, chaudhry2018efficient, douillard2020podnet, ebrahimi2020adversarial, joseph2020meta, mirzadeh2020linear, nguyen2017variational, pan2020continual, pellegrini2019latent, rolnick2018experience, von2019continual}. Our work relates most to memory-based methods with external memory which we spend more time on describing in the next paragraph. 



\vspace{-2mm}
\paragraph{Memory-based Continual Learning:} The simplest selection strategy is random selection of examples to store in the memory for replay. Much research effort has focused on selecting higher quality samples to store in memory~\cite{chaudhry2019tiny, chrysakis2020online, hayes2019memory, isele2018selective, lopez2017gradient, nguyen2017variational, rebuffi2017icarl}. Chaudhry \etal~\cite{chaudhry2019tiny} reviews several selection strategies, e.g., reservoir sampling~\cite{vitter1985random}, first-in first-out buffer~\cite{lopez2017gradient}, k-Means, and Mean-of-Features~\cite{rebuffi2017icarl}, in scenarios with tiny memory capacity. However, more elaborate selection strategies have been shown to give little benefit over random selection for image classification problems~\cite{chaudhry2018riemannian, hayes2020remind}. More recently, there has been work on compressing raw images to feature representations to increase the number of memory examples for replay~\cite{hayes2020remind, iscen2020memory}. Our approach differs from the above mentioned works since we focus on which memory examples to choose for training at the current task rather than which examples to store in the memory. Replay scheduling can however be combined with any selection strategy as well as storing feature representations. 

Another important aspect of memory-based methods is how to adjust the memory size when storing old examples from the most recent task. There exist two popular approaches for setting the memory size that maintain an equal distribution of past examples in the memory, which is important when the memory size is small. In the first strategy, a constant number of samples per class $M_{per}$ are stored, so that the total memory size grows with the number of classes~\cite{douillard2020podnet, hou2019learning}. The second strategy sets a fixed size capacity on the memory meaning that we are only allowed to store a total of $M_{total}$ number of samples from all previously seen classes/tasks~\cite{chaudhry2019tiny, lopez2017gradient}. For our task, we assume that we can easily access historical data and focus on which tasks to select for replay. In Section \ref{sec:replay_scheduling_for_continual_learning}, we describe our memory setting for learning policies to select replay schedules.


 
\vspace{-2mm}
\paragraph{Human Continual Learning:} Humans are continual learning systems in the sense of learning tasks and concepts sequentially. Furthermore, humans have an impressive ability to memorize experiences but can forget learned knowledge gradually rather than catastrophically~\cite{french1999catastrophic}. Different learning techniques have been suggested for humans to memorize better~\cite{dunlovsky2013improving, willis2007review}. An example is spaced repetition which gradually increases time-intervals between rehearsals for retaining long-term memory~\cite{dempster1989spacing} and has been known to be superior to rehearsing many times in short time, i.e., massed learning or massed training, to retain memory since the studies by Ebbinghaus~\cite{ebbinghaus2013memory}. Landauer and Bjork~\cite{landauer1978optimum} demonstrated that memory training schedules using adjusted spaced repetition were better at preserving memory than uniformly spaced training, while Hawley \etal~\cite{hawley2008comparison} compares its efficacy on adults with probable Alzheimer's disease for learning face-name association. More recently, computational models and neural representational analysis have been used to bring insights on the underlying mechanisms enabling spaced repetition to improve long-term memory~\cite{feng2019spaced, smolen2016right}. Several works in continual learning with neural networks are inspired by or have a connection to human learning techniques, including spaced repetition~\cite{feng2019spaced, smolen2016right}, mechanisms of sleep~\cite{ball2020study, schwarz2018progress}, and reactivation of memories~\cite{hayes2020remind, van2020brain}. Our replay scheduling method is inspired by spaced repetition; %where 
we learn schedules of which memory examples to use for replay at different time steps. 


