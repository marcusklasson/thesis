


%%% FIGURE ON MEMORY USAGE 5-TASK DATASETS
%\input{PaperC/sections/experiments/figures/memory_usage_comparison}

\subsection{Efficiency of Replay Scheduling}
\label{paperC:sec:efficiency_of_replay_scheduling}



%Here, we 
\begin{wrapfigure}[12]{r}{0.36\textwidth}
    \centering
	\setlength{\figwidth}{0.33\textwidth}
	\setlength{\figheight}{.14\textheight}
	\vspace{-3mm}
	\input{PaperC/sections/experiments/figures/tiny_memory/memory_usage_barplot}
	\vspace{-3mm}
	\captionsetup{width=.85\linewidth}
	\caption{
		Number of replayed samples per task for the 5-task datasets in the tiny memory setting. Ours use $M=2$ samples for replay, while the baselines increment their memory per task.
	}
	%\vspace{-4mm}
	\label{fig:tiny_memory_experiment_memory_usage}
\end{wrapfigure}
We illustrate the efficiency of replay scheduling with comparisons to several common replay-based CL baselines in an even more extreme memory setting.
Our goal is to investigate if scheduling over which tasks to replay can be more efficient in situations where the memory size is even smaller than the number of classes. % than replaying all available samples at every task.
To this end, we set the replay memory size for our method
to $M=2$ for the 5-task datasets, such that only 2 samples can be selected for replay at all times. For the 10- and 20-task datasets which have 100 classes, we set $M=50$. We then compare against the most memory efficient CL baselines, namely A-GEM~\citeC{C:chaudhry2018efficient}, ER-Ring~\citeC{C:chaudhry2019tiny} which show promising results with 1 sample per class for replay, % for replay after learning each task, 
and with uniform memory selection as reference. 
Additionally, we compare to using random replay schedules (Random) with the same memory setting as for RS-MCTS.
We visualize the memory usage for our method and the baselines when training on a 5-task dataset in Figure \ref{fig:tiny_memory_experiment_memory_usage}. 
Figure \ref{fig:memory_usage_10_and_20task_datasets} in Appendix \ref{paperC:app:additional_figures} shows the memory usage for the other datasets. 

Table \ref{tab:efficiency_of_replay_scheduling_all_datasets} shows the ACC for each method across all datasets. Despite using significantly fewer samples for replay, RS-MCTS performs better or on par with the best baselines on all datasets except Split CIFAR-100. 
These results indicate that replay scheduling is an important research direction in CL as storing 1 sample/class in the memory could be inefficient in settings with large number of tasks.

%%% TABLE
\input{PaperC/sections/experiments/tables/table_efficiency_replay_scheduling}

