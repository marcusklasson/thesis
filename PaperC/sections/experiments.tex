
%\input{PaperC/sections/experiments/figures/mcts_rewards_over_iterations}

\begin{figure}[t]
	\centering
	\setlength{\figwidth}{0.23\textwidth}
	\setlength{\figheight}{.13\textheight}
	\input{PaperC/figures/mcts_rewards_comparison/mcts_rewards_groupplot_single_row}
	\vspace{-3mm}
	\caption{ Average test accuracies over tasks after learning the final task (ACC) over the MCTS simulations for all datasets, where 'S' and 'P' are used as short for 'Split' and 'Permuted'. We compare performance for MCTS (Ours) against random replay schedules (Random), Equal Task Schedule (ETS), and Heuristic Scheduling (Heuristic) baselines. For the first three datasets, we show the ACC for for training on all seen task datasets jointly (Joint), as well as the best ACC found from a breadth-first search (BFS) as two upper bounds. All results have been averaged over 5 seeds. These results show that replay scheduling can improve over ETS and outperform or perform on par with Heuristic across different datasets and network architectures.
	}
	\label{fig:mcts_best_rewards}
	\vspace{-3mm}
\end{figure}


\section{Experiments}\label{paperC:sec:experiments}

We evaluated our replay scheduling method empirically on six common benchmark datasets for CL. 
We let MCTS select the result on the held-out test sets from the replay schedule that yielded the best reward on the validation set during the search.%We denote our method as Replay Scheduling MCTS (MCTS) and select the result on the held-out test sets from the replay schedule that yielded the best reward on the validation set during the search.
Full details on experimental settings are in Appendix \ref{paperC:app:experimental_settings} and additional results are in Appendix \ref{paperC:app:additional_experimental_results}. 
%Code will be available upon acceptance.

\vspace{-3mm}
\paragraph{Datasets.} We conduct experiments on six datasets commonly used as benchmarks in the CL literature: Split MNIST~\citeC{C:lecun1998gradient, C:zenke2017continual}, Fashion-MNIST~\citeC{C:xiao2017fashion}, Split notMNIST~\citeC{C:bulatov2011notMNIST}, Permuted MNIST~\citeC{C:goodfellow2013empirical}, Split CIFAR-100~\citeC{C:krizhevsky2009learning}, and Split miniImagenet~\citeC{C:vinyals2016matching}. We randomly sample 15\% of the training data from each task to use for validation when computing the reward for the MCTS simulations. 

\vspace{-3mm}
\paragraph{Baselines.} We compare MCTS to using 1) random replay schedules (Random), 2) equal task schedules (ETS), and 3) a heuristic scheduling method (Heuristic). The ETS baseline uses equal task proportions, such that $M/(t-1)$ samples per task are replayed during learning of task $t$, and use both training and validations sets for training such that ETS use the same amount of data as MCTS. %such that $M/(t-1)$ samples per task are replayed during learning of task $t$, and use both training and validations sets for training such that they use the same amount of data as MCTS. 
The Heuristic baseline replays the tasks which accuracy on the validation set is below a certain threshold proportional to the best achieved validation accuracy on the task. Here, the replay memory is filled with $M/k$ samples per task where $k$ is the number of selected tasks. If $k=0$, then we skip applying replay at the current task.  
See Appendix \ref{paperC:app:heuristic_scheduling_baseline} for more details on the Heuristic baseline. 



%\input{PaperC/sections/experiments/tables/table_memory_selection_methods}

\vspace{-3mm}
\paragraph{Architectures.} We use a multi-head output layer for all datasets except for Permuted MNIST where the network uses single-head output. We use a 2-layer MLP with 256 hidden units for Split MNIST, Split FashionMNIST, Split notMNIST, and Permuted MNIST. For Split CIFAR-100, we use the CNN architecture used in \citeC{C:vinyals2016matching,C:schwarz2018progress}. For Split miniImagenet, we apply the reduced ResNet-18 from \citeC{C:lopez2017gradient}. 

\vspace{-3mm}
\paragraph{Evaluation Metric.} 
We use ACC as the average test accuracy over all tasks after learning the final task and BWT for measuring forgetting~\citeC{C:lopez2017gradient}, i.e.,
\begin{align}
	\textnormal{ACC} = \frac{1}{T} \sum_{i=1}^{T} A_{T, i}, \quad \textnormal{BWT} = \frac{1}{T-1} \sum_{i=1}^{T-1} A_{T, i} - A_{i, i},
\end{align}
where $A_{t, i}$ is the test accuracy for task $i$ after learning task $t$. We report means and standard deviations of ACC and BWT using 5 different seeds on all datasets. 
%We use the average test accuracy over all tasks after learning the final task, i.e., $\text{ACC} = \frac{1}{T} \sum_{i=1}^{T} A_{T, i}^{(test)}$ where $A_{T, i}^{(test)}$ is the test accuracy of task $i$ after learning task $T$. We report means and standard deviations of ACC using 5 different seeds on all datasets. 



%%% MCTS Results
%\input{PaperC/sections/experiments/results_with_mcts}
\subsection{Performance Progress of MCTS}\label{paperC:sec:results_with_mcts}

In the first experiments, we show that the replay schedules from MCTS yield better performance than replaying an equal amount of samples per task. 
The replay memory size is fixed to $M=10$ for Split MNIST, FashionMNIST, and notMNIST, and $M=100$ for Permuted MNIST, Split CIFAR-100, and Split miniImagenet. Uniform sampling is used as the memory selection method for all methods in this experiment.
For the 5-task datasets, we provide the optimal replay schedule found from a breadth-first search (BFS) over all 1050 possible replay schedules in our action space (which corresponds to a tree with depth of 4) as an upper bound for MCTS. As the search space grows fast with the number of tasks, BFS becomes computationally infeasible when we have 10 or more tasks.


Figure \ref{fig:mcts_best_rewards} shows the progress of ACC over %the MCTS
iterations by MCTS for all datasets. We also show the best ACC metrics for Random, ETS, Heuristic, and BFS (where appropriate) as straight lines. 
Furthermore, we include the ACC achieved by training on all seen datasets jointly at every task (Joint) for the 5-task datasets.
We observe that MCTS outperforms Random and ETS successively with more iterations. Furthermore, MCTS approaches the upper limit of BFS on the 5-task datasets. For Permuted MNIST and Split CIFAR-100, the Heuristic baseline and MCTS perform on par after 50 iterations. This shows that Heuristic with careful tuning of the validation accuracy threshold can be a strong baseline when comparing replay scheduling methods. %Table \ref{tab:results_memory_selection_methods} at the rows for Uniform selection shows the ACC for each method in this experiment. We note that MCTS outperforms ETS significantly on most datasets and performs on par with Heuristic.


%%% Visualizations
%\input{PaperC/sections/experiments/replay_schedule_visualizations}
\subsection{Replay Schedule Visualizations}
\label{paperC:sec:replay_schedule_visualization}

%\input{PaperC/sections/experiments/figures/replay_schedule_visualization}
\begin{wrapfigure}{r}{0.52\textwidth}
	%\centering
	\setlength{\figwidth}{0.56\textwidth}
	\setlength{\figheight}{.42\textwidth}
	\vspace{-3mm}
	\input{PaperC/figures/replay_schedule_visualizations/cifar100/rs_bubble_plot_seed3}
	\vspace{-3mm}
	%\captionsetup{width=.85\linewidth}
	\caption{Replay schedule learned from Split CIFAR-100 visualized as a bubble plot. %Visualization using a bubble plot of a replay schedule learned from Split CIFAR-100. %The color of the circles corresponds to a historical task and its size represents the task proportion that is replayed. The memory size is $M=100$ and the results are from 1 seed. 
		The task proportions vary dynamically over time which would be hard to replace by a heuristic method. 
	}
	\vspace{-4mm}
	\label{fig:replay_schedule_vis_cifar100}
\end{wrapfigure}
We visualize a learned replay schedule from Split CIFAR-100 with memory size $M=100$ to gain insights into the behavior of the scheduling policy from MCTS. Figure \ref{fig:replay_schedule_vis_cifar100} shows a bubble plot of the task proportions that are used for filling the replay memory at every task. 
Each circle color corresponds to a historical task and the circle size represents its proportion of replay samples at the current task.
%Each color of the circles corresponds to a historical task and its size represents the proportion of examples that are replayed at the current task. 
The sum of all points from all circles at each column is fixed at the different time steps since the memory size $M$ is fixed. The task proportions vary dynamically over time in a sophisticated nonlinear way which would be hard to replace by a heuristic method. Moreover, we can observe spaced repetition-style scheduling on many tasks, e.g., task 1-3 are replayed with similar proportion at the initial tasks but eventually starts varying the time interval between replay. Also, task 4 and 6 need less replay in their early stages, which could potentially be that they are simpler or correlated with other tasks. We provide a similar visualization for Split MNIST in Figure \ref{fig:split_mnist_task_accuracies_and_bubble_plot} in Appendix \ref{paperC:app:replay_schedule_visualization_for_split_mnist} to bring more insights to the benefits of replay scheduling.


%\input{PaperC/sections/experiments/figures/replay_schedule_visualization}


%%% Results on memory selection methods
%%% Table 1, put it here for placement

\begin{table}[t]
	\footnotesize
	\centering
	\caption{
		Performance comparison with ACC between MCTS (Ours), Random scheduling (Random), Equal Task Schedule (ETS), and Heuristic Scheduling (Heuristic) with various memory selection methods evaluated across all datasets. We provide the metrics for training on all seen task datasets jointly (Joint) as an upper bound, as well as include the results from a breadth-first search (BFS) with Uniform memory selection for the 5-task datasets. 
		%We use 'S' and 'P' as short for 'Split' and 'Permuted' for the datasets. 
		Replay memory sizes are $M=10$ and $M=100$ for the 5-task and 10/20-task datasets respectively. We report the mean and standard deviation averaged over 5 seeds. Ours performs better or on par with the baselines on most datasets and selection methods, where MoF yields the best results %performance 
		in general.
	}
	\vspace{-3mm}
	%\setlength{\tabcolsep}{5pt}
	\resizebox{0.98\textwidth}{!}{ %\scalebox{0.87}{
			\input{PaperC/tables/memory_selection_methods}
		}
		\vspace{-3mm}
		\label{tab:results_memory_selection_methods}
	\end{table}

%\input{PaperC/sections/experiments/alternative_memory_selection_methods}
\subsection{Combine with Different Memory Selection Methods}
\label{paperC:sec:alternative_memory_selection_methods}

We show that our method can be combined with any memory selection method for storing replay samples. In addition to uniform sampling, we apply various memory selection methods commonly used in the CL literature, namely $k$-means clustering, $k$-center clustering~\citeC{C:nguyen2017variational}, and Mean-of-Features (MoF)~\citeC{C:rebuffi2017icarl}. We compare our method and ETS combined with these different selection methods. 
The replay memory sizes are $M=10$ for the 5-task datasets and $M=100$ for the 10- and 20-task datasets.  
Table \ref{tab:results_memory_selection_methods} shows the results across all datasets. 
We note that using the replay schedule from MCTS mostly outperforms the baselines when using the alternative selection methods, where MoF performs the best on most datasets. 


%%% Table 2, put it here for placement
%\input{PaperC/sections/experiments/tables/table_applying_scheduling_to_recent_replay_methods}
\begin{table}[t]
	%\footnotesize
	\centering
	\caption{
		Performance comparison with ACC and BWT between scheduling methods MCTS (Ours), Random, ETS, and Heuristic combined with replay-based methods HAL, MER, DER, and DER++. %such as Hindsight Anchor Learning (HAL), Meta Experience Replay (MER), Dark Experience Replay (DER), and DER++. 
		Replay memory sizes are $M=10$ and $M=100$ for the 5-task and 10/20-task datasets respectively. We report the mean and standard deviation averaged over 5 seeds. Results on Heuristic where some seed did not converge is denoted by $^{*}$. Applying MCTS to each method can enhance the performance compared to using the baseline schedules. 
	}
	\vspace{-3mm}
	%\setlength{\tabcolsep}{5pt}
	\resizebox{0.98\textwidth}{!}{ %\scalebox{0.87}{
			\input{PaperC/tables/applying_scheduling_to_recent_replay_methods}
		}
		\vspace{-3mm}
		\label{tab:results_applying_scheduling_to_recent_replay_methods}
	\end{table}
	
	%%% Results on applying replay scheduling to recent replay methods
	%\input{PaperC/sections/experiments/applying_scheduling_to_recent_replay_methods}
	\subsection{Applying Scheduling to Recent Replay Methods}\label{paperC:sec:applying_scheduling_to_recent_replay_methods}
	
	In this experiment, we show that replay scheduling can be combined with any replay method to enhance the CL performance. We combine MCTS with Hindsight Anchor Learning (HAL)~\citeC{C:chaudhry2021using}, Meta-Experience Replay (MER)~\citeC{C:riemer2018learning}, Dark Experience Replay (DER)~\citeC{C:buzzega2020dark}, and DER++. 
	We provide the hyperparameter settings in Appendix \ref{paperC:app:apply_scheduling_to_recent_replay_methods}. Table \ref{tab:results_applying_scheduling_to_recent_replay_methods} shows the performance comparison between our proposed replay scheduling against using Random, ETS, and Heuristic schedules for each method. The results confirm that replay scheduling is important for the final performance given the same memory constraints and it can benefit any existing CL framework. 


%%% Figure Acc over memory size
\begin{figure}[t]
	\centering
	\setlength{\figwidth}{0.35\textwidth}
	\setlength{\figheight}{.16\textheight}
	\begin{subfigure}[b]{0.9\textwidth}
		\centering
		\input{PaperC/figures/acc_over_memory_size_100iters_new/acc_over_memory_size_groupplot}
		\vspace{-1mm}
		\caption{Task/Domain-IL}
		\label{fig:acc_over_memory_size_task_il}
	\end{subfigure} \\
	\begin{subfigure}[b]{0.9\textwidth}
		\centering
		\input{PaperC/figures/class_il_scenario/acc_over_memory_size_groupplot}
		\vspace{-1mm}
		\caption{Class-IL}
		\label{fig:acc_over_memory_size_class_il}
	\end{subfigure}
	\vspace{-1mm}
	\caption{Performance comparison with ACC over various memory sizes for the methods, where (a) shows results in the Task- and Domain-Incremental Learning (IL) settings, and (b) in the Class-IL setting. All results have been averaged over 5 seeds. These results show that replay scheduling can outperform the baselines on both small and large datasets across different backbone choices in the different CL settings. 
	}
	%\vspace{-3mm}
	\label{fig:acc_over_replay_memory_size}
\end{figure}

%%%% Results of ACC over memory size
%\input{PaperC/sections/experiments/results_with_varying_memory_size}
\subsection{Varying Memory Size}
\label{paperC:sec:results_with_varying_replay_memory_size}

We show that our method can improve the CL performance across varying memory sizes in different CL scenarios, namely, Task-Incremental Learning (IL), Domain-IL, and Class-IL~\cite{van2019three}. In the experiment for Task- and Domain-IL, we set the replay memory size to ensure the ETS baseline replays an equal number of samples per class at the final task. The memory size is set to $M = n_{cpt} \cdot n_{spc} \cdot (T-1)$, where $n_{cpt}$ is the number of classes per task in the dataset and $n_{spc}$ are the number of samples per class we wish to replay at task $T$ for the ETS baseline. Figure \ref{fig:acc_over_memory_size_task_il} shows the results in the Task- and Domain-Incremental Learning (IL) scenarios, where we observe that MCTS generally obtains better task accuracies than ETS, especially for small memory sizes. Both MCTS and ETS perform better than Heur-GD as $M$ increases, which shows that Heur-GD requires careful tuning of the validation thresholds. 

In the Class-IL scenario, the task labels are absent at training and test time. Here, the replay memory is always filled with at least 1 sample/class to avoid fully forgetting non-replayed tasks. Each scheduling method then selects which tasks to replay out of the remaining samples $M_{rest} = M - t \cdot n_{cpt}$ samples. 
Figure \ref{fig:acc_over_memory_size_class_il} shows that ETS approaches MCTS when $M$ increases on the 5-task datasets. However, on the more challenging Split CIFAR-100 and Split miniImagenet, MCTS outperforms ETS clearly as $M$ increases. These results show that selecting the proper replay schedule is essential in various CL scenarios with both small and large datasets across different backbone choices.

%We show that our method can improve the performance across different choices of memory size. In this experiment, we set the replay memory size to ensure the ETS baseline replays an equal number of samples per class at the final task. The memory size is set to $M = n_{cpt} \cdot n_{spc} \cdot (T-1)$, where $n_{cpt}$ is the number of classes per task in the dataset and $n_{spc}$ are the number of samples per class we wish to replay at task $T$ for the ETS baseline. In Figure \ref{fig:acc_over_memory_size_task_il}, we observe that MCTS obtains better task accuracies than ETS, especially for small memory sizes. Both MCTS and ETS perform better than Heuristic as $M$ increases showing that Heuristic requires careful tuning of the validation accuracy threshold. These results show that replay scheduling can outperform the baselines, especially for small $M$, on both small and large datasets across different backbone choices.






%%% Efficiency of Replay Scheduling
%\input{PaperC/sections/experiments/efficiency_of_replay_scheduling}
\subsection{Efficiency of Replay Scheduling}
\label{paperC:sec:efficiency_of_replay_scheduling}

%Here, we 
\begin{wrapfigure}[12]{r}{0.35\textwidth}
	\centering
	\setlength{\figwidth}{0.33\textwidth}
	\setlength{\figheight}{.14\textheight}
	\vspace{-3mm}
	\input{PaperC/sections/experiments/figures/tiny_memory/memory_usage_barplot}
	\vspace{-3mm}
	\captionsetup{width=.98\linewidth}
	\caption{
		Number of replayed samples/task for the 5-task datasets in the tiny memory setting. Ours use $M=2$ samples for replay, while the baselines increment their memory per task.
	}
	%\vspace{-4mm}
	\label{fig:tiny_memory_experiment_memory_usage}
\end{wrapfigure}
We illustrate the efficiency of replay scheduling with comparisons to several common replay-based CL baselines in an even more extreme memory setting.
Our goal is to investigate if scheduling over which tasks to replay can be more efficient in situations where the memory size is even smaller than the number of classes. % than replaying all available samples at every task.
To this end, we set the replay memory size for our method
to $M=2$ for the 5-task datasets, such that only 2 samples can be selected for replay at all times. For the 10- and 20-task datasets which have 100 classes, we set $M=50$. We then compare against the most memory efficient CL baselines, namely A-GEM~\citeC{C:chaudhry2018efficient}, ER-Ring~\citeC{C:chaudhry2019tiny} which show promising results with 1 sample per class for replay, % for replay after learning each task, 
and with uniform memory selection as reference. 
Additionally, we compare to using random replay schedules (Random) with the same memory setting as for MCTS.
We visualize the memory usage for our method and the baselines when training on a 5-task dataset in Figure \ref{fig:tiny_memory_experiment_memory_usage}. 
Figure \ref{fig:memory_usage_10_and_20task_datasets} in Appendix \ref{paperC:app:additional_figures} shows the memory usage for the other datasets. 

Table \ref{tab:efficiency_of_replay_scheduling_all_datasets} shows the ACC for each method across all datasets. Despite using fewer samples for replay, MCTS performs on par with the best baselines and outperforms them on Permuted MNIST.  
These results indicate that replay scheduling is an important research direction in CL, since storing every seen class in the memory could be inefficient in settings with large number of tasks. 



\begin{table}[t]
	\centering
	\caption{
		Performance comparison with ACC and BWT averaged over 5 seeds in the 1 sample/class %example per class 
		memory setting evaluated across all datasets. 
		MCTS (Ours) has  memory size $M=2$ and $M=50$ for the 5-task and 10/20-task datasets respectively. The baselines replay all available memory samples. 
		MCTS performs on par with the best baselines and outperforms them on Permuted MNIST. 
	}
	\vspace{-3mm}
	\resizebox{0.98\textwidth}{!}{
		\input{PaperC/tables/efficiency_of_replay_scheduling}
	}
	\vspace{-3mm}
	\label{tab:efficiency_of_replay_scheduling_all_datasets}
\end{table}

%%% TABLE
%\input{PaperC/sections/experiments/tables/table_efficiency_replay_scheduling}

