
\section{Conclusions}\label{paperC:sec:conclusions}

We propose learning the time to learn, i.e.,~in a real-world CL context, learning schedules of which tasks to replay at different times. 
To the best of our knowledge, we are the first to consider the time to learn in CL inspired by human learning techniques. 
We demonstrated the benefits with replay scheduling by showing the performance improvements with schedules found using MCTS on several CL benchmarks when comparing against methods with fixed scheduling policies under the same memory budgets. Furthermore, the dynamic behavior of the replay schedules showed similarities to human learning techniques, such as spaced repetition, by replaying previous tasks with varying time intervals.  We also showed that replay scheduling can be combined with any replay-based method as well as utilize the memory efficiently even when the memory size is smaller than the number of classes. The proposed problem setting brings CL research closer to real-world needs, especially in scenarios where CL is applied under limited processing times but with rich amounts of historical data available for replay.

\vspace{-3mm}
\paragraph{Limitations and Future Work.}
In this work, we assumed that multiple episodes are allowed in the CL environments to use MCTS. However, this is prohibited in the CL setting since tasks can never be fully revisited.
Hence, it would be useful if we could learn replay scheduling policies that generalize to new CL scenarios for mitigating catastrophic forgetting. Using MCTS for searching after such policies can be inefficient as the algorithm needs to run for each dataset and application separately. In future work, we will incorporate reinforcement learning methods that generalize and extend to learning general policies that can be directly applied to any new application and domain. This requires defining the state using various task performance metrics such as accuracies and forgetting metrics, the action either in the same space as in this work or as in continuous space with fractions of different tasks in the memory, and the reward as final or accumulated CL performance. Finally, we would like to explore choosing memory samples on an instance level as the current work selects samples on task level. This would also require a policy learning method that scales to large action spaces which is a research challenge by itself. 


