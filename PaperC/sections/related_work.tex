
\section{Related Work}\label{sec:related_work}
In this section, we give a brief overview of CL methods, essentially replay-based methods, as well as spaced repetition techniques for human CL.

\vspace{-3mm}
\paragraph{Continual Learning.} Traditional CL can be divided into three main areas, namely regularization-based, architecture-based, and replay-based approaches. Regularization-based methods aim to mitigate catastrophic forgetting by protecting parameters influencing the predictive performance on known tasks from wide changes and use the rest of the parameters for learning new tasks~\citep{adel2019continual, chaudhry2018riemannian, kirkpatrick2017overcoming, li2017learning, nguyen2017variational, rannen2017encoder, schwarz2018progress, zenke2017continual}. Architecture-based methods isolate task-specific parameters by either increasing network capacity~\citep{rusu2016progressive, yoon2019scalable, yoon2017lifelong} or freezing parts of the network~\citep{mallya2018packnet, serra2018overcoming} to maintain good performance on previous tasks. 
Replay-based methods mix samples from old tasks with the current dataset to mitigate catastrophic forgetting, where the replay samples are either stored in an external memory~\citep{chaudhry2019tiny, hayes2020remind, isele2018selective, lopez2017gradient} or generated using a generative model~\citep{shin2017continual, van2018generative}. 
%Replay-based methods store examples from previous tasks in an external memory~\citep{chaudhry2019tiny, hayes2020remind, isele2018selective, lopez2017gradient}, or uses a generative model to generate pseudo-samples from a distribution over tasks~\citep{shin2017continual, van2018generative}, that are mixed with the current task dataset to mitigate catastrophic forgetting. 
Regularization-based approaches and dynamic architectures have been combined with replay-based approaches to methods to overcome their limitations~\citep{buzzega2020dark, chaudhry2018riemannian, chaudhry2018efficient, chaudhry2021using, douillard2020podnet, ebrahimi2020adversarial, joseph2020meta, mirzadeh2020linear, nguyen2017variational, pan2020continual, pellegrini2019latent, rolnick2018experience, von2019continual}. Our work relates most to replay-based methods with external memory which we spend more time on describing in the next paragraph.



\vspace{-3mm}
\paragraph{Replay-based Continual Learning.} A commonly used memory selection strategy of replay samples is random selection. 
%The simplest selection strategy is random selection of examples to store in the memory for replay. 
Much research effort has focused on selecting higher quality samples to store in memory~\citep{aljundi2019gradient, borsos2020coresets, chaudhry2019tiny, chrysakis2020online, hayes2019memory, isele2018selective, lopez2017gradient, nguyen2017variational, rebuffi2017icarl, yoon2021online}. \citet{chaudhry2019tiny} reviews several selection strategies in scenarios with tiny memory capacity, e.g., reservoir sampling~\citep{vitter1985random}, first-in first-out buffer~\citep{lopez2017gradient}, k-Means, and Mean-of-Features~\citep{rebuffi2017icarl}. However, more elaborate selection strategies have been shown to give little benefit over random selection for image classification problems~\citep{chaudhry2018riemannian, hayes2020remind}. More recently, there has been work on compressing raw images to feature representations to increase the number of memory examples for replay~\citep{hayes2020remind, iscen2020memory, pellegrini2019latent}. 
Our approach differs from the above mentioned works since we focus on learning to select which tasks to replay at the current task rather than improving memory selection or compression quality of the memory samples. %samples in the memory. 
%Our approach differs from the above mentioned works since we focus on which memory examples to choose for training at the current task rather than which examples to store in the memory. 
Replay scheduling can however be combined with any selection strategy and feature compression method. %as well as storing features. %feature representations. 

%\CZ{Then start with the tradtionion random memory (your last two sentence. And then say what research has been focused on. E.g. Selection higher quility samples. e.g. VCL, Chauhry etc. Memory representations to store more data etc..... Then in the end say, we are focusing the scheduling and focus on learning when to learn. } Research has focused on various selection strategies, e.g. reservoir sampling and online k-means, for which samples to store in memory~\citep{chaudhry2019tiny, hayes2019memory, rebuffi2017icarl}. Chaudhry \etal~\citep{chaudhry2019tiny} reviews several selection strategies \CZ{What are these trategies?} in scenarios with tiny memory capacity, and Hayes\etal~\citep{hayes2019memory} develops a strategy based on online k-means. However, most commonly the replay samples are selected uniformly at random depending on the experimental setting. Another line of work focuses on increasing the memory capacity by storing past examples a feature representations instead of raw format~\citep{hayes2020remind}. 

%Another important aspect of memory-based methods is how to adjust the memory size when storing old examples from the most recent task. There exist two popular approaches for setting the memory size that maintain an equal distribution of past examples in the memory, which is important when the memory size is small. In the first strategy, a constant number of samples per class $M_{per}$ are stored, so that the total memory size grows with the number of classes~\citep{douillard2020podnet, hou2019learning}. The second strategy sets a fixed size capacity on the memory meaning that we are only allowed to store a total of $M_{total}$ number of samples from all previously seen classes/tasks~\citep{chaudhry2019tiny, lopez2017gradient}. For our task, we assume that we can easily access historical data and focus on which tasks to select for replay. In Section \ref{sec:replay_scheduling_for_continual_learning}, we describe our memory setting for learning policies to select replay schedules.


\vspace{-3mm}
\paragraph{Human Continual Learning.} Humans are CL systems in the sense of learning tasks and concepts sequentially. Furthermore, humans have the ability to memorize experiences but forgets learned knowledge gradually rather than catastrophically~\citep{french1999catastrophic}. Different learning techniques have been suggested for humans to memorize better~\citep{dunlovsky2013improving, willis2007review}. 
An example is spaced repetition where time intervals between rehearsal are gradually increased to improve long-term memory retention~\citep{dempster1989spacing}, where the earliest documented works are from \citet{ebbinghaus2013memory}. %An example is spaced repetition which gradually increases time-intervals between rehearsals for retaining long-term memory~\citep{dempster1989spacing}, where the earliest documented works are from \citet{ebbinghaus2013memory}. 
Further studies have shown that memory training schedules with adjusted spaced repetition are better at preserving memory than using uniformly spaced rehearsal times~\cite{hawley2008comparison, landauer1978optimum}. 
%An example is spaced repetition which gradually increases time-intervals between rehearsals for retaining long-term memory~\citep{dempster1989spacing}. 
%This technique has been studied frequently and was inspired from the works of \citet{ebbinghaus2013memory} on memory retention. 
%For example, \citet{landauer1978optimum} demonstrated that memory training schedules using adjusted spaced repetition were better at preserving memory than uniformly spaced training. 
%\citet{hawley2008comparison} studies the efficacy of spaced repetition on adults with probable Alzheimer's disease for learning face-name association.  
Several works in CL with neural networks are inspired by %or have connections to 
human learning techniques, including spaced repetition~\citep{amiri2017repeat, feng2019spaced, smolen2016right}, %~\citep{amiri2017repeat, amiri2019neural, feng2019spaced, smolen2016right}, 
sleep mechanisms~\citep{ball2020study, mallya2018packnet, schwarz2018progress}, %mechanisms of sleep~\citep{ball2020study, mallya2018packnet, schwarz2018progress}, 
and memory reactivation~\citep{hayes2020remind, van2020brain}. % reactivation of memories~\citep{hayes2020remind, van2020brain}. 
Replay scheduling is also inspired by spaced repetition, where %Our replay scheduling method is inspired by spaced repetition; %where 
we learn schedules of which tasks to replay at different times. % steps. %memory samples to use for replay at different time steps. 

