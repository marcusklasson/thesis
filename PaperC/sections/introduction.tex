
\section{Introduction}\label{paperC:sec:introduction}

Many organizations deploying machine learning systems receive large volumes of data daily where these new data are often associated with new tasks. Although all historical data are stored in the cloud in practice, retraining machine learning systems on a daily basis is prohibitive both in time and cost. 
In this setting, the systems must continuously adapt to new tasks without forgetting the previously learned abilities. Continual learning (CL) methods~\citeC{C:de2019continual, C:parisi2019continual} 
address this challenge where, in particular, replay-based methods~\citeC{C:chaudhry2019tiny, C:hayes2020remind} have shown to be very effective in achieving great prediction performance and retaining knowledge of old tasks. 
Replay-based methods mitigate catastrophic forgetting by revisiting a small set of samples, which is feasible to process compared to the size of the historical data.
In the traditional CL literature, the replay memory is limited due to the assumption that historical data are not available. In the real-world setting where historical data are in fact always available, the requirement of small memory remains due to processing time and cost issues. % 


Most research on replay-based CL has been focused on the sample quality in the memory~\citeC{C:aljundi2019gradient, C:borsos2020coresets, C:chaudhry2019tiny, C:chrysakis2020online, C:nguyen2017variational, C:rebuffi2017icarl, C:yoon2021online} or data compression to increase the memory capacity~\citeC{C:hayes2020remind, C:iscen2020memory, C:pellegrini2019latent}. Common for these methods is that the memory allocates an equal amount of space for storing samples from old tasks. When learning new tasks, the whole memory is replayed to mitigate catastrophic forgetting.
However, in life-long learning settings, this simple strategy would be inefficient as the memory must store a large number of tasks. Furthermore, these methods ignore the time to learn old tasks again which is important in human learning. 
Humans are CL systems, and different methods have been developed to enhance memory retention, such as spaced repetition~\citeC{C:dempster1989spacing, C:ebbinghaus2013memory, C:landauer1978optimum} which is often used in education. These education methods focus on the scheduling of learning and rehearsal of previous learned knowledge.  

\begin{figure}[t]
\centering
\setlength{\figwidth}{.22\textwidth}
\setlength{\figheight}{.13\textheight}
\input{PaperC/figures/single_task_replay_experiment/single_task_replay_groupplot}
\vspace{-4mm}
\caption{Task accuracies on Split MNIST~\cite{C:zenke2017continual} when replaying only 10 samples of classes $0/1$ at a single time step. The black vertical line indicates when replay is used. ACC denotes the average accuracy over all tasks after learning Task 5. Results are averaged over 5 seeds. These results show that the time to replay the previous task is critical for the final performance.}
%This shows the time to learn the previous task again with memory is critical for the performance.
%}%\vspace{-4mm}
\label{fig:single_task_replay_with_M10}
\vspace{-1em}
\end{figure}

We 
%In this work, we 
argue that finding the proper schedule of which tasks to replay in the fixed memory setting is critical for CL. To demonstrate our claim, we perform a simple experiment on the Split MNIST~\citeC{C:zenke2017continual} dataset where each task consists of learning the digits 0/1, 2/3, etc.\ arriving in sequence.
The replay memory contains data from task 1 and can only be replayed at one point in time.
Figure \ref{fig:single_task_replay_with_M10} shows how the task performances progress over time when the memory is replayed at different time steps. In this example, the best final performance is achieved when the memory is used when learning task 5.
Note that choosing different time points to replay the same memory leads to noticeably different results in the final performance. 
These results indicate that scheduling the time when to apply replay can influence the final performance significantly of a CL system.  


To this end, we propose learning the time to learn, in which we learn replay schedules of which tasks to replay at different times inspired from human learning~\citeC{C:dempster1989spacing}. 
We illustrate the advantages with replay scheduling by using Monte Carlo tree search (MCTS)~\citeC{C:coulom2006efficient} to learn policies for replay. 
More specifically, we train a neural network on the current task dataset mixed with the scheduled replay samples and measure the final performance of the network to evaluate the replay schedules selected by MCTS. 
In summary, our contributions are:
\begin{itemize}[topsep=1pt,] %noitemsep,]
    \setlength\itemsep{0.1mm}
    \item We propose a new CL setting where historical data is available while the processing time is limited, in order to adjust current CL research closer to real-world needs (Section \ref{paperC:sec:problem_setting}). In this new setting, we introduce replay scheduling in CL where we learn the time to learn which tasks to replay (Section \ref{paperC:sec:replay_scheduling_in_continual_learning}).
    %\item \mk{In this new CL setting, we introduce replay scheduling in CL where we learn the time to learn which tasks to replay (Section \ref{sec:replay_scheduling_in_continual_learning}).%we demonstrate the importance of replay scheduling in continual learning (Figure \ref{fig:single_task_replay_with_M10}) and propose to learn the time to learn which tasks to replay (Section \ref{sec:replay_scheduling_in_continual_learning}).
    %}
    %\item We demonstrate the importance of replay scheduling in continual learning and propose to learn the time to learn which tasks to replay (Section \ref{sec:problem_setting} and \ref{sec:replay_scheduling_in_continual_learning}). 
    \item We use MCTS as an example method to illustrate how to learn replay scheduling policies by creating finite sets of memory compositions that can be replayed at every task (Section \ref{paperC:sec:mcts_for_replay_scheduling}).
    %\item We use MCTS as an example method to illustrate how replay schedules in CL can be learned by establishing a finite set of memory compositions that can be replayed at every task (Section \ref{sec:mcts_for_replay_scheduling}). 
    \item We demonstrate with six benchmark datasets that learned scheduling can improve the CL performance significantly in the fixed size memory setting (Section \ref{sec:results_with_mcts} and \ref{sec:results_with_varying_replay_memory_size}). Moreover, we show that our method can be combined with any memory selection technique and replay-based method (Section \ref{sec:alternative_memory_selection_methods} and \ref{sec:applying_scheduling_to_recent_replay_methods}), as well as being efficient in situations where the 
    memory size is even smaller than the number of classes (Section \ref{sec:efficiency_of_replay_scheduling}). 
    %Furthermore, we show that our method can be combined with any other memory selection methods (Section \ref{sec:alternative_memory_selection_methods}), as well as being efficient in situations where the memory size is even smaller than the number of classes (Section \ref{sec:efficiency_of_replay_scheduling}). 
\end{itemize}





