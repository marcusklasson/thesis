

\subsection{Replay Scheduling in Continual Learning}\label{paperC:sec:replay_scheduling_in_continual_learning}

In this section, we describe our setup for enabling the scheduling for selecting replay memories at different time steps. We define a replay schedule as a sequence $S = (\va_1, \dots, \va_{T-1})$, where the task proportions $\va_i = (a_1, \dots, a_{T-1})$ for $1 \leq i \leq T-1$ are used for determining how many samples from seen tasks with which to fill the replay memory at task $i$. We construct an action space with a discrete number of choices of task proportions that can be selected at each task: At task $t$, we have $t-1$ historical tasks that we can choose samples from. We create $t-1$ bins $\vb_t = [b_1, \dots, b_{t-1}]$ and sample a task index for each bin $b_i \in \{1, \dots, t-1\}$. The bins are treated as interchangeable and we only keep the unique choices. For example, at task 3, we have seen task 1 and 2, so the unique choices of vectors are $[1,1], [1,2], [2,2]$, where $[1,1]$ indicates that all memory samples are from task 1, $[1,2]$ indicates that half memory is from task 1 and the other half are from task etc. We count the number of occurrences of each task index in $\vb_t$ and divide by $t-1$ to obtain the task proportion, i.e., $\va_t = \texttt{bincount}(\vb_t) / (t-1)$. We round the number of replay samples from task $i$, i.e., $a_i \cdot M$, up or down accordingly to keep the memory size $M$ fixed when filling the memory. From this specification, we can build a tree of different replay schedules to evaluate with the network. We outline the steps for creating the action space in Algorithm \ref{paperC:alg:action_space_discretization} (see Appendix \ref{paperC:app:rs_mcts_algorithm}). 

Figure \ref{fig:replay_scheduling_mcts_tree_example} shows an example of a replay schedule tree with Split MNIST~\citeC{C:zenke2017continual} 
where the memory size is $M=8$. Each level corresponds to a task to learn and we show some examples of possible replay memories in the tree that can be evaluated at each task. A replay schedule is represented as a path traversal of different replay memory compositions from task 1 to task 5. At task 1, the memory $\gM_1 = \emptyset$ is empty, while $\gM_2$ is filled with samples from task 1 at task 2. The memory $\gM_3$ can be composed with samples from either task 1 or 2, or equally fill $\gM_3$ with samples from both tasks. All possible paths in the tree are valid replay schedules. We show three examples of possible schedules in Figure \ref{fig:replay_scheduling_mcts_tree_example} for illustration: the \textcolor{blue}{blue} path represents a replay schedule where only task 1 samples are replayed. The \textcolor{red}{red} path represents using memories with equally distributed tasks, and the \textcolor{purple}{purple} path represents a schedule where the memory is only filled with samples from the most previous task.



